{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documentation for the Geocodes Container Stack","text":"<pre><code>flowchart TB\nservices-- deployed by --&gt;portainer\ngeocodes-- deployed by  --&gt; portainer\ngleaner-- deployed by  --&gt; portainer\nfacetsearch-- routes --&gt; traefik\nfacetsearchservices-- routes--&gt;traefik\noss-- routes--&gt;traefik\ntriplestore-- routes --&gt; traefik\nsparqlgui-- routes --&gt; traefik\nsubgraph gleaner\nheadless\nend\nsubgraph geocodes\nfacetsearch--&gt;facetsearchservices\nend\nsubgraph services\noss[\"oss s3\"]\nsparqlgui\ntriplestore[\"graph -- triplestore\"]\nend\n\n        subgraph base\n           traefik&lt;-- routes --&gt;portainer\n        end\n</code></pre>"},{"location":"#overview","title":"Overview:","text":"<ol> <li>Configure a base server</li> <li>Setup services containers</li> <li>Setup Gleaner containers</li> <li>Intial setup of services and load data</li> <li>Setup Gepcodes UI containers,</li> </ol>"},{"location":"#what-to-learn-from-deploying-the-stack-and-the-indexing-application","title":"What to learn from deploying the stack, and the indexing application","text":"<ul> <li>how to deploy containers</li> <li>how to run indexing using gleaner<ul> <li>initial setup to use a test daatase</li> <li>production setup to use the sources google spreadsheet</li> <li>to learn to renenerate the config files when you edit using glcon.</li> </ul> </li> <li>how to setup the UI</li> <li>how to reconfigure the UI </li> </ul> <p>Clean Machine</p> <p>These are instructions for a clean machine. Your mileage will vary if you are trying to install this stack on a developers workstation. If you are experienced, then you can probably deploy the docker stacks on a server with docker running. The stack uses treafik labels to manage the roures between the web server and the containers. It is not a task for the faint of heart, but IMOHO, it is more automatic that nginx or caddy reverse proxy routing. and allows us to deploy 'tenant' client stacks using configured data steores for each client in the services stack. Probably can be done with helm charts.</p>"},{"location":"#requirements-prior-to-starting","title":"Requirements prior to starting","text":"<p>Three sections to know prior to installing * All * Production * Local Development</p>"},{"location":"#all","title":"ALL","text":"<p>You need to be able to run <code>docker compose version</code></p> <p>Should be &gt; v2.13 <code>docker compose  --help</code> needs to show the -p --project flag</p> <p>Known issue</p> <p>with (at least) Ubuntu default docker package. Install the official docker package</p> <pre><code>docker compose version\nDocker Compose version v2.13.0</code></pre>"},{"location":"#production","title":"Production","text":"<p>YOU NEED SETUP DNS.</p> <p>Setup DNS names for the aliases  so the treafik routing will work</p> Local(Tutorial)/Development <p>TODO: there are compose-local.yaml configurations. NOT WELL TESTED</p> <ul> <li>The run_local.sh works. There is no portainer for these. Managed by command line.</li> <li>minio (and maybe others) use ports. See \"-local path \" at this page</li> </ul>"},{"location":"#creating-and-managing-containers","title":"Creating and managing CONTAINERS","text":"<ol> <li> <p>Configure a base server</p> <ul> <li>docker</li> <li>setup directory and groups for installing geocodes</li> <li>git clone https://github.com/earthcube/geocodes.git</li> <li>cd geocodes/deployment</li> <li>setup domain names</li> <li>create .env file</li> <li>add  (traefik and portainer ) build-machine-compose.yaml</li> <li>(add headless with larger shared memory) ./run_gleaner.sh   </li> </ul> </li> <li> <p>Use portainer to setup geocodes services </p> <ul> <li>setup and configure services<ul> <li>create env variables file for the services</li> <li>add stack services-compose.yaml to portainer</li> </ul> </li> </ul> </li> <li>Setup Gleaner containers</li> <li>run shell script <code>run_gleaner.sh</code></li> <li> <p>Initial Setup of datastores and loading of sample data</p> <ul> <li>Setup datastores for s3 and graph</li> <li>Install software glcon</li> <li>create configuration gctest <code>./glcon config init --cfgName gctest</code></li> <li>copy file with repository information (sitemap location and name)</li> <li>edit file tell ingest what services to utilize </li> <li>generate a configuration updated with the source and configuration <code>./glcon config generate --cfgName gctest</code></li> <li>run ingest <code>./glcon gleaner batch --cfgName gctest</code><ul> <li>check minioadmin to see that bucket gctest was populated</li> </ul> </li> <li>convert to triples and upload: <code>./glcon nabu prefix --cfgName gctest</code><ul> <li>run sparql query at graph service to see that triples got converted and uploaded</li> </ul> </li> <li>create a materilized view of the data using summarize (TB DOCUMENTED BY MBCODE)</li> </ul> </li> <li> <p>Use portainer to setup geocodes user interface (and services )</p> <ul> <li>setup and configure user infertace and it's services<ul> <li>create a facets config</li> <li>upload facets config to portainer/docker</li> <li>add  stack gecodes-compose.yaml to portainer</li> </ul> </li> </ul> </li> <li>Creating a community instance (aka tennant)</li> </ol>"},{"location":"#data-loading","title":"Data Loading","text":"<ul> <li>Testing</li> <li>Production<ul> <li>Create a  'Production' config </li> <li>Production Fragments</li> </ul> </li> </ul>"},{"location":"#notes","title":"NOTES","text":"<ul> <li>Troubleshooting</li> </ul>"},{"location":"development_notes/","title":"Notes on issues","text":""},{"location":"development_notes/#creating-portainer-and-traefik","title":"Creating Portainer and Traefik","text":"<p>Initially thought to run the original stack in portainer.  Did not work well. </p>"},{"location":"development_notes/#lets-encrypt","title":"Lets encrypt","text":"<p>When testing use the let's enxrypt developement server in the traefi.yml</p> <p>Hit a limit during development where the acme.json was being created as a directory Creating a config directory and telling let's encrypt/traefik proxy to store in that  directory solved the issue. Still had to wait seven days for the limit to clear</p>"},{"location":"development_notes/#container-building","title":"Container Building","text":"<p>The docker container uses</p> <p><code>RUN npm build</code></p> <p>this calls</p> <p><code>vue-cli-service build</code></p> <p>Which run NODE_ENV production</p> <p>added</p> <p><code>\"geocodestest\": \"vue-cli-service build --mode geocodestest\"</code></p> <p>and changed Dockerfile to <code>RUN npm geocodestest</code></p> <p>This points out that the production container will need to just access a config.json  file from a remote url.. or have it published into the containers /public directory.</p> <p>Headless:</p> <p>Improving performance:  I tried to replicated the headless container, but that make headless queries fail.</p>"},{"location":"development_notes/#healthcheck","title":"healthcheck","text":"<p>https://medium.com/geekculture/how-to-successfully-implement-a-healthcheck-in-docker-compose-efced60bc08e</p>"},{"location":"development_notes/#notes-on-updating-a-config-from-the-command-line","title":"Notes on updating a config from the command line","text":"<pre><code>ubuntu@geocodes-dev:~$ docker stack services geocodes\nID             NAME                      MODE         REPLICAS   IMAGE                                      PORTS\nqb2dg6kb3nsw   geocodes_notebook-proxy   replicated   1/1        nsfearthcube/mknb:latest                   \nlo93ax3l4qp8   geocodes_vue-client       replicated   1/1        nsfearthcube/ec_facets_client:latest       \nd1g5vckmivnj   geocodes_vue-services     replicated   1/1        nsfearthcube/ec_facets_api_nodejs:latest   </code></pre> <pre><code>ubuntu@geocodes-dev:~$ docker stack services  --format=\"{{.ID}} {{.Name}}\"  geocodes\nqb2dg6kb3nsw geocodes_notebook-proxy\nlo93ax3l4qp8 geocodes_vue-client\nd1g5vckmivnj geocodes_vue-services</code></pre> <p>How to stop a service docker service scale [servicename]=0</p> <pre><code>buntu@geocodes-dev:~$ docker service scale geocodes_vue-client=0\ngeocodes_vue-client scaled to 0\noverall progress: 0 out of 0 tasks \nverify: Service converged \n</code></pre>"},{"location":"install_minio_client/","title":"minio client","text":"<p>Minio Client will help push and manage items in minio/s3 from the command line. basically, we can remove old datasets, empty buckets, etc.</p>"},{"location":"install_minio_client/#install-client","title":"install client","text":"<p>minio client</p> <pre><code>curl https://dl.min.io/client/mc/release/linux-amd64/mc \\\n--create-dirs \\\n-o $HOME/minio-binaries/mc\n\nchmod +x $HOME/minio-binaries/mc\nexport PATH=$PATH:$HOME/minio-binaries/\n\nmc --help</code></pre>"},{"location":"install_minio_client/#if-midnight-commander-is-installed","title":"if midnight commander is installed","text":"<ul> <li>add file .bash_aliases <pre><code>alias mc=' $HOME/minio-binaries/mc'</code></pre></li> </ul>"},{"location":"install_minio_client/#configure-client","title":"configure client","text":"<p>Note the single quotes around the password... some passwords are   not command line friendly <pre><code>set +o history\nmc config host add dev https://oss.geocodes.earthcube.org  {miniouser} '{miniopassword}'\n\nset -o history</code></pre></p>"},{"location":"install_minio_client/#some-commands","title":"Some commands:","text":"<p>Note these are using a dev alias Use mc alias set to set an alias</p>"},{"location":"install_minio_client/#what-buckets","title":"What Buckets","text":"<p><code>mc ls dev</code></p> <pre><code>[2022-07-28 09:52:51 PDT]     0B citesting/\n[2023-04-17 14:44:09 PDT]     0B deepoceans/\n[2023-01-11 09:03:29 PST]     0B dv-testing/\n[0000-12-31 16:07:02 LMT]     0B gleaner/\n[2022-10-18 05:30:41 PDT]     0B gleaner-wf/\n[2023-01-20 11:35:22 PST]     0B mb-testing/\n[2022-09-19 09:35:23 PDT]     0B mbci2/\n[2023-04-19 09:45:10 PDT]     0B opencore/\n[2023-03-11 16:13:11 PST]     0B public/</code></pre>"},{"location":"install_minio_client/#bucket-disk-usage","title":"Bucket Disk usage","text":"<p><code>mc du --depth 2 dev/citesting</code></p> <pre><code>107KiB  9 objects   citesting/milled\n1.2KiB  1 object    citesting/orgs\n68KiB   40 objects  citesting/prov\n30B 2 objects   citesting/reports\n107KiB  1 object    citesting/results\n58KiB   9 objects   citesting/summoned\n341KiB  62 objects  citesting</code></pre>"},{"location":"stack_machines/","title":"Stack Containers","text":""},{"location":"stack_machines/#containers-and-routes","title":"\"Containers and Routes\"","text":"<pre><code>flowchart TB\nservices-- deployed by --&gt;portainer\ngeocodes-- deployed by  --&gt; portainer\ngleaner-- deployed by  --&gt; portainer\nfacetsearch-- routes --&gt; traefik\nfacetsearchservices-- routes--&gt;traefik\noss-- routes--&gt;traefik\ntriplestore-- routes --&gt; traefik\nsparqlgui-- routes --&gt; traefik\nsubgraph gleaner\nheadless\nend\nsubgraph geocodes\nfacetsearch--&gt;facetsearchservices\nend\nsubgraph services\noss[\"oss s3\"]\nsparqlgui\ntriplestore[\"graph -- triplestore\"]\nend \n        subgraph base\n           traefik&lt;-- routes --&gt;portainer\n        end </code></pre>"},{"location":"stack_machines/#this-is-a-list-of-the-stack-containers","title":"This is a list of the stack containers.","text":"<p>NOTE, for production stacks, DNS Names listed need to be cnamed. REPEAT, so to setup a test machine for production, you need to request DNS. The local stacks do no use the traefik routing, and the followign ports need to be available: 3000, 3031,8080, 8888, 9000, 9001, 9999</p> container name stack -local path notes traekfik admin.{HOST} base n/a http router portainer portainer.{HOST} base n/a container management s3system oss.{HOST} services http://localhost:9000 s3 store s3system minioadmin.{HOST} services http://localhost:9001 s3 store triplestore graph.{HOST} services http://localhost:9999/blazegraph/ sparqlgui sparqlui.{HOST} services http://localhost:8888/sparqlgui sparql ui headless {none} gleaner_via_shell headless:9000 (internal route) start with ./run_gleaner.sh vue-client geocodes.{HOST} geocodes http://localhost:8080/ facetsearch ui vue-services geocodes.{HOST} geocodes http://localhost:3000/ec/api api ,at geocodes/ec/api notebook-proxy geocodes.{HOST} geocodes http://localhost:3031/notebook notebook proxy, at geocodes/notebook"},{"location":"stack_machines/#docker","title":"Docker","text":"<p>When you run local, these should be created</p> <p>Networks: traefik_proxy Volumes: * graph * s3 * logs configs: * configs will be locally mounted, See the docker files.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#troubleshooting","title":"Troubleshooting","text":"<p>Some topics (sorry disorganized notes)</p> <ul> <li>Containers<ul> <li>can't seem to connect</li> <li>Complaints about bad certificate</li> <li>cannot connect to (minioadmin,graph,sparqlui)</li> <li>Minoadmin/Graph/etc seem to be there, but do not connect</li> </ul> </li> <li>glcon/gleaner application</li> <li>Blazegraph<ul> <li>journal truncation</li> </ul> </li> <li>updating portainer</li> <li>OS issues<ul> <li>Ubuntu docker</li> <li>Ubuntu 16. glcon</li> </ul> </li> <li>issue with a repository</li> <li></li> </ul>"},{"location":"troubleshooting/#cant-seem-to-connect","title":"can't seem to connect;","text":"<p>are containers running <code>docker ps</code></p> <p>can you connect to portainer and traefik</p> <p>traefik:  <pre><code>https://admin.{HOST}</code></pre></p> <p>portainer:  <pre><code>https://portainer.{host}</code></pre></p> <p>In traefik, are  errors on the https://admin.{host}</p>"},{"location":"troubleshooting/#complaints-about-bad-certificate","title":"Complaints about bad certificate","text":"<p>Initially, we are using letsencypt dev services. THis message will show until you rebuild the base containers with the dev line commented out</p>"},{"location":"troubleshooting/#but-that-line-is-commented-out","title":"But that line is commented out.","text":"<p>need to delete the acme.json and restart the container.</p> <p>log onto console via portainer, use /bin/sh <pre><code>/ # ls\nacme.json      dev            etc            lib            mnt            proc           run            srv            tmp            var\nbin            entrypoint.sh  home           media          opt            root           sbin           sys            usr\n/ # cat acme.json \n{\n  \"httpresolver\": {\n    \"Account\": {\n      \"Email\": \"dwvalentine@ucsd.edu\",\n      \"Registration\": {\n        \"body\": {\n          \"status\": \"valid\",\n          \"contact\": [\n            \"mailto:dwvalentine@ucsd.edu\"\n          ]\n        },\n        \"uri\": \"https://acme-v02.api.letsencrypt.org/acme/acct/1030461777\"\n</code></pre></p> <pre><code>rm acme.json</code></pre> <p>some docker command also...</p>"},{"location":"troubleshooting/#cannot-connect-to-minioadmingraphsparqlui","title":"cannot connect to (minioadmin,graph,sparqlui)","text":"<ul> <li>is the traefik proxy a SCOPE swarm <code>docker network ls</code></li> <li>portainerui, networks</li> <li>is the DNS correct?</li> <li>need a dig example, since editing host and NS looku</li> <li>are they running?</li> <li><code>docker ps</code><ul> <li>do you see portainer stack </li> </ul> </li> </ul>"},{"location":"troubleshooting/#minoadmingraphetc-seem-to-be-there-but-do-not-connect","title":"Minoadmin/Graph/etc seem to be there, but do not connect","text":"<p>If minioadmin complains that it cannot connect with a 10.0.0.x message, then there  may be two</p> <ul> <li>open portainer</li> <li>select containers</li> <li>sort by name</li> <li>look and see if two of the s3 stack are running.</li> <li>if yes, delete both, and one should restart.</li> </ul> <p>have also seen something similar for graph.</p>"},{"location":"troubleshooting/#glcon","title":"glcon","text":""},{"location":"troubleshooting/#setup-failure","title":"setup failure","text":"<p><code>./glcon gleaner setup --cfgName {name}</code></p> <pre><code>{\"file\":\"/github/workspace/pkg/gleaner.go:63\",\"func\":\"github.com/gleanerio/gleaner/pkg.Setup\",\"level\":\"error\",\"msg\":\"Connection issue, make sure the minio server is running and accessible.The Access Key Id you provided does not exist in our records.\",\"time\":\"2022-07-22T19:08:01Z\"}</code></pre> <p>You probably have environment variables set. <pre><code>ubuntu@geocodes-dev:~/indexing$ env\n[snip]\nMINIO_SECRET_KEY=MySecretSecretKeyforMinio\nMINIO_ACCESS_KEY=MySecretAccessKey\n[snip]\nubuntu@geocodes-dev:~/indexing$ unset MINIO_SECRET_KEY\nubuntu@geocodes-dev:~/indexing$ unset MINIO_ACCESS_KEY</code></pre></p>"},{"location":"troubleshooting/#blazegraph-journal-truncation","title":"Blazegraph journal truncation:","text":""},{"location":"troubleshooting/#for-a-container","title":"for a container","text":"<p>in newer container, the command is available, but the service needs to be stopped. guess running an container with an exec command in a different container might work.</p> <pre><code>cd /var/lib/blazegraph ;java -jar /usr/bin/blazegraph.jar com.bigdata.journal.CompactJournalUtility blazegraph.jnl blazegraph.jnl.compact\n\n</code></pre>"},{"location":"troubleshooting/#count-quads","title":"count quads","text":"<pre><code>SELECT (COUNT(*) as ?Triples) WHERE {graph ?g {?s ?p ?o}}</code></pre>"},{"location":"troubleshooting/#updating-portainer-or-treafik","title":"updating Portainer, or treafik","text":"<p>the latest image needs to be pulled</p> <p><code>docker pull portainer/portainer-ce:latest</code></p> <p>then <code>./run_base.sh</code></p>"},{"location":"troubleshooting/#os-issues","title":"OS Issues","text":"<p>place where  os issues may be </p>"},{"location":"troubleshooting/#ubuntu-docker","title":"Ubuntu Docker","text":"<p>If you are running on Ubuntu, you need to remove the provided docker.com version. Official docker package We suggest that for others, confirm that you can run</p> <pre><code>```shell\ndocker compose version\nDocker Compose version v2.13.0\n```\n\nIf you cannot run `docker compose` then update to the docker.com version\nThis is the version we are presently running.\n\n```    \nClient: Docker Engine - Community\n     Version:           20.10.21\n     API version:       1.41\n```</code></pre>"},{"location":"troubleshooting/#ubuntu-18-and-glcon","title":"Ubuntu 18 and glcon","text":"<p>there appears to be issues with Ubuntu 18 and the latest versions of the golang library viper. If you run on Ubuntu 20.X it works. Solution is to 'do-realse-upgrade' and wait ;)</p> <p><code>shell ubuntu@geocodes-dev:~$ uname -a Linux geocodes-dev 4.15.0-194-generic #205-Ubuntu SMP Fri Sep 16 19:49:27 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux ubuntu@geocodes-dev:~$ cd indexing/ ubuntu@geocodes-dev:~/indexing$ ./glcon gleaner batch --cfgName ci version:  v3.0.8-ec {\"file\":\"/Users/valentin/development/dev_earthcube/gleanerio/gleaner/pkg/cli/gleaner.go:71\",\"func\":\"github.com/gleanerio/gleaner/pkg/cli.initGleanerConfig\",\"level\":\"fatal\",\"msg\":\"error reading config file While parsing config: yaml: unmarshal errors:\\n  line 1: cannot unmarshal !!str `\\u003c?xml v...` into map[string]interface {}\",\"time\":\"2023-02-14T15:58:00Z\"} ubuntu@geocodes-dev:~/indexing$</code></p>"},{"location":"data_loading/dataloading_troubleshooting/","title":"Data Loading Trouble shooting","text":""},{"location":"data_loading/dataloading_troubleshooting/#intial-places-to-look","title":"Intial places to look:","text":"<p>When you have issues there are multiple starting points</p> <ul> <li>console... </li> <li>logs</li> <li>minio</li> <li>run reports</li> </ul>"},{"location":"data_loading/dataloading_troubleshooting/#console-and-logs","title":"Console and Logs","text":"<p>There will be errors in the console and the logs. We have not correctly identified what is an actual data error, a network  error, or a code error.</p> <p>Need to add common errors with descriptions:</p> <p>Robots.txt</p> <p>These are normal. It just says hey cannot find the robots.txt. Being lowered to info or debug <code>{\"file\":\"/github/workspace/internal/summoner/acquire/utils.go:35\",\"func\":\"github .com/gleanerio/gleaner/internal/summoner/acquire.getRobotsTxt\",\"level\":\"error\",\" msg\":\"error parsing robots.txt at https://ecl.earthchem.org/home.php/robots.txtP arse error(s): \\nAllow before User-agent at token #31.\\n\",\"time\":\"2023-05-03T14: 01:19-05:00\"}</code></p> <p>** Headless **</p> <p>There are normal. If gleaner cannot find a jsonld it calls headless. And then if headless cannot find one,  it presently throws an error.</p> <p>{\"file\":\"/github/workspace/internal/summoner/acquire/headlessNG.go:340\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.PageRender\",\"issue\":\"Headless Evaluate\",\"level\":\"error\",\"msg\":\"cdp.Runtime: Evaluate: context deadline exceeded\",\"time\":\"2023-05-04T14:00:16-05:00\",\"url\":\"https://earthref.org/MagIC/13413\"}</p> <p>headless timeput</p> <p>Normal, but you may need to check it out</p> <p>{\"file\":\"/github/workspace/internal/summoner/acquire/headlessNG.go:340\",\"func\":\" github.com/gleanerio/gleaner/internal/summoner/acquire.PageRender\",\"issue\":\"Head less Evaluate\",\"level\":\"error\",\"msg\":\"cdp.Runtime: Evaluate: rpc error: Executio n context was destroyed. (code = -32000)\",\"time\":\"2023-05-03T14:01:24-05:00\",\"ur l\":\"https://ssdb.iodp.org/dataset\"}</p> <p>** error **</p> <p>This may mean that a page does not have JSON, or that a page that gleaner thinks should be json, is not json. You may need to look in postman, and send a Accept header to see what gleaner is getting from the website</p> <p><code>{\"contentType\":\"script[type='application/ld+json']\",\"file\":\"/github/workspace/in ternal/summoner/acquire/acquire.go:228\",\"func\":\"github.com/gleanerio/gleaner/int ernal/summoner/acquire.FindJSONInResponse.func1\",\"level\":\"error\",\"msg\":\"Error pr ocessing script tag in http://get.iedadata.org/doi/100505error checking for vali d json: Error in unmarshaling json: invalid character '}' looking for beginning of object key string\",\"time\":\"2023-05-03T14:01:23-05:00\",\"url\":\"http://get.iedad ata.org/doi/100505\"}</code></p>"},{"location":"data_loading/dataloading_troubleshooting/#data-loading-stops-when-my-terminal-disconnects","title":"Data Loading stops when my terminal disconnects","text":"<p>Yes, the process dies when you are disconnected... if you do not nohup the process,  or use tmux or screen to allow the process  to be detaached.</p> <p>There are insructions for screen, since that is what the write of this documentation uses:</p> <p>If source is large,  use screen e.g. <code>screen -S gleaner</code></p> <p>Did we suggest running in a screen</p>"},{"location":"data_loading/dataloading_troubleshooting/#missing-jsonld","title":"Missing JSONLD","text":"<p>from open topo one id</p>"},{"location":"data_loading/dataloading_troubleshooting/#duplicate-id","title":"duplicate id","text":""},{"location":"data_loading/dataloading_troubleshooting/#r2r","title":"r2r","text":"<p>@id is set to doi:null</p> <p>same id generated, only one file will be maintained. <pre><code>level=debug issue=\"Uploaded JSONLD to object store\" sha=00a6b5eec951ac51065f0f2485c3406a4c260fb0 url=\"https://dev.rvdata.us/search/fileset/149018\"\nlevel=debug issue=\"Uploaded JSONLD to object store\" sha=d26f68908c3d75d7705f78518beb19c325d32ac9 url=\"https://dev.rvdata.us/search/fileset/149018\"\nlevel=debug msg=\"&lt;nil&gt;\" issue=\"Multiple JSON\" url=\"https://dev.rvdata.us/search/fileset/149019\"\nlevel=debug issue=\"Multiple JSON\" url=\"https://dev.rvdata.us/search/fileset/149019\"\nlevel=debug issue=\"Uploaded JSONLD to object store\" sha=00a6b5eec951ac51065f0f2485c3406a4c260fb0 url=\"https://dev.rvdata.us/search/fileset/149019\"\nlevel=debug issue=\"Uploaded JSONLD to object store\" sha=d26f68908c3d75d7705f78518beb19c325d32ac9 url=\"https://dev.rvdata.us/search/fileset/149019</code></pre></p>"},{"location":"data_loading/dataloading_troubleshooting/#data-catalog","title":"Data Catalog","text":"<p>we need to deal with these</p>"},{"location":"data_loading/dataloading_troubleshooting/#no-datasets","title":"No datasets","text":"<p>run the graph and no datasets</p>"},{"location":"data_loading/indexing_with_gleanerio_for_testing/","title":"Loading Data for Testing and Validation","text":"<p>Goal: Load data from GeocodesMetadata Repository for testing</p> <p>The stories should be on the Geocodes Documentation Wiki. data validation and loading story</p> <p>This will load data into  buckets that is for testing. Aka not in gleaner</p> <p>let's use:</p> <ul> <li>ci</li> <li>ci2 </li> </ul> <p>Testing Matrix</p> Tests config s3 Bucket graph namespace notes ReporitoryMeta gctest gctest gctest samples of actual datasets TestingMeta ci citesting citesting Good Dataset multiple ci2 citesting2 citesting2 two repositories DoubleLoad ci citesting citesting Run Nabu a second time to try to load duplicates <p>gctest</p> <p>gctest configuration and setup is described in Setup</p>"},{"location":"data_loading/indexing_with_gleanerio_for_testing/#install-glcon","title":"Install glcon","text":"<p><code>glcon</code> is a console application that combines the functionality of Gleaner and Nabu into a single application. It also has features to create and manage configurations for gleaner and nabu.</p> <p>Install glcon</p>"},{"location":"data_loading/indexing_with_gleanerio_for_testing/#create-a-configuration-for-continuous-integration","title":"Create a configuration for Continuous Integration","text":"<code>./glcon config init --cfgName ci</code> <pre><code>   ubuntu@geocodes-dev:~/indexing$ ./glcon config init --cfgName ci\n    2022/07/21 23:27:31 EarthCube Gleaner\n    init called\n    make a config template is there isn't already one\n    ubuntu@geocodes-dev:~/indexing$ ls configs/ci\n    README_Configure_Template.md  localConfig.yaml  sources.csv\n    gleaner_base.yaml             nabu_base.yaml\n    ubuntu@geocodes-dev:~/indexing$ </code></pre>"},{"location":"data_loading/indexing_with_gleanerio_for_testing/#edit-files","title":"edit files:","text":"<p>You will need to change the localConfig.yaml</p> <code>nano configs/ci/localConfig.yaml</code> <pre><code>---\nminio:\n  address: oss.{HOST}\n  port: 433\n  accessKey: worldsbestaccesskey\n  secretKey: worldsbestaccesskey\n  ssl: true\n  bucket: citesting\n  # can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/sparql\ns3:\n  bucket: citesting\n  # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n#  location: sources.csv \n# this can be a remote csv\n#  type: csv\n  location: https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=TestDatasetSources\n# TBD -- Just use the sources in the gleaner file.\n#  type: yaml\n#  location: gleaner.yaml</code></pre>"},{"location":"data_loading/indexing_with_gleanerio_for_testing/#generate-configs-glcon-config-generate-cfgname-ci","title":"Generate configs <code>./glcon config generate --cfgName ci</code>","text":"<code>.</code>/glcon config generate --cfgName ci` <pre><code>./glcon config generate --cfgName ci\n2022/07/21 23:37:46 EarthCube Gleaner\ngenerate called\n{SourceType:sitemap Name:geocodes_demo_datasets Logo:https://github.com/earthcube/GeoCODES-Metadata/metadata/OtherResources URL:https://raw.githubusercontent.com/earthcube/GeoCODES-Metadata/gh-pages/metadata/Dataset/sitemap.xml Headless:false PID:https://www.earthcube.org/datasets/ ProperName:Geocodes Demo Datasets Domain:0 Active:true CredentialsFile: Other:map[] HeadlessWait:0}\nmake copy of servers.yaml\nRegnerate gleaner\nRegnerate nabu</code></pre>"},{"location":"data_loading/indexing_with_gleanerio_for_testing/#flightest","title":"flightest","text":"<code>./glcon gleaner setup --cfgName ci</code> <pre><code>   ubuntu@geocodes-dev:~/indexing$ ./glcon gleaner setup --cfgName ci\n   2022/07/21 23:42:54 EarthCube Gleaner\n   Using gleaner config file: /home/ubuntu/indexing/configs/ci/gleaner\n   Using nabu config file: /home/ubuntu/indexing/configs/ci/nabu\n   setup called\n   2022/07/21 23:42:54 Validating access to object store\n   2022/07/21 23:42:54 Connection issue, make sure the minio server is running and accessible. The specified bucket does not exist.\n   ubuntu@geocodes-dev:~/indexing$ </code></pre>"},{"location":"data_loading/indexing_with_gleanerio_for_testing/#run-batch","title":"run batch","text":"<p>Robots.txt</p> <p>OK TO IGNORE. you will need to ignore errors about robot.txt and sitemap.xml not being an index <pre><code>{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:204\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"error\",\"msg\":\"error getting robots.txt for https://www.earthcube.org/datasets/allgood:Robots.txt unavailable at https://www.earthcube.org/datasets/allgood/robots.txt\",\"time\":\"2023-01-30T20:45:53-06:00\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:66\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResourceURLs\",\"level\":\"error\",\"msg\":\"Error getting robots.txt for geocodes_demo_datasets, continuing without it.\",\"time\":\"2023-01-30T20:45:53-06:00\"}    </code></pre></p> <p>Access issues</p> <pre><code>{\u201cfile\u201d:\u201c/github/workspace/internal/organizations/org.go:87\",\u201cfunc\u201d:\u201cgithub.com/gleanerio/gleaner/internal/organizations.BuildGraph\u201d,\u201clevel\u201d:\u201cerror\u201d,\u201cmsg\u201d:\u201corgs/geocodes_demo_datasets.nqThe Access Key Id you provided does not exist in our records.\u201c,\u201dtime\u201d:\u201c2023-01-31T15:27:39-06:00\u201d}</code></pre> <ul> <li>Access Key password could be incorrect</li> <li>address may be incorrect. It is a hostname or TC/IP, and not a URL</li> <li>ssl may need to be true</li> <li>See setup issues</li> </ul> \"./glcon gleaner batch --cfgName ci <pre><code>    ubuntu@geocodes-dev:~/indexing$ ./glcon gleaner batch --cfgName ci\n    INFO[0000] EarthCube Gleaner                            \n    Using gleaner config file: /home/ubuntu/indexing/configs/ci/gleaner\n    Using nabu config file: /home/ubuntu/indexing/configs/ci/nabu\n    batch called\n    {\"file\":\"/github/workspace/internal/organizations/org.go:55\",\"func\":\"github.com/gleanerio/gleaner/internal/organizations.BuildGraph\",\"level\":\"info\",\"msg\":\"Building organization graph.\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/pkg/gleaner.go:35\",\"func\":\"github.com/gleanerio/gleaner/pkg.Cli\",\"level\":\"info\",\"msg\":\"Sitegraph(s) processed\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/summoner.go:17\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner.Summoner\",\"level\":\"info\",\"msg\":\"Summoner start time:2022-07-22 19:16:53.451745139 +0000 UTC m=+0.182100234\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:189\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"info\",\"msg\":\"Getting robots.txt from 0/robots.txt\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/utils.go:23\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsTxt\",\"level\":\"error\",\"msg\":\"error fetching robots.txt at 0/robots.txtGet \\\"0/robots.txt\\\": unsupported protocol scheme \\\"\\\"\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:192\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"error\",\"msg\":\"error getting robots.txt for 0:Get \\\"0/robots.txt\\\": unsupported protocol scheme \\\"\\\"\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:63\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResourceURLs\",\"level\":\"error\",\"msg\":\"Error getting robots.txt for geocodes_demo_datasetscontinuing without it.\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:127\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getSitemapURLList\",\"level\":\"info\",\"msg\":\"https://raw.githubusercontent.com/earthcube/GeoCODES-Metadata/gh-pages/metadata/Dataset/sitemap.xml is not a sitemap index, checking to see if it is a sitemap\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/acquire.go:32\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResRetrieve\",\"level\":\"info\",\"msg\":\"Queuing URLs for geocodes_demo_datasets\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/acquire.go:74\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getConfig\",\"level\":\"info\",\"msg\":\"Thread count 5 delay 0\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n    12% |\u2588\u2588\u2588\u2588\u2588\u2588                                                 | (2/16, 2 it/s) [0s:7s]{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n    43% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                | (7/16, 6 it/s) [1s:1s]{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n    68% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                  | (11/16, 6 it/s) [1s:0s]{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n    75% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588              | (12/16, 6 it/s) [1s:0s]{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (16/16, 9 it/s)        \n    {\"file\":\"/github/workspace/internal/summoner/summoner.go:37\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner.Summoner\",\"level\":\"info\",\"msg\":\"Summoner end time:2022-07-22 19:16:55.660367672 +0000 UTC m=+2.390721648\",\"time\":\"2022-07-22T19:16:55Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/summoner.go:38\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner.Summoner\",\"level\":\"info\",\"msg\":\"Summoner run time:0.0368103569\",\"time\":\"2022-07-22T19:16:55Z\"}\n    {\"file\":\"/github/workspace/internal/millers/millers.go:27\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Miller start time2022-07-22 19:16:55.661434567 +0000 UTC m=+2.391819553\",\"time\":\"2022-07-22T19:16:55Z\"}\n    {\"file\":\"/github/workspace/internal/millers/millers.go:44\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Adding bucket to milling list:summoned/geocodes_demo_datasets\",\"time\":\"2022-07-22T19:16:55Z\"}\n    {\"file\":\"/github/workspace/internal/millers/millers.go:55\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Adding bucket to prov building list:prov/geocodes_demo_datasets\",\"time\":\"2022-07-22T19:16:55Z\"}\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (15/15, 51 it/s)        \n    {\"file\":\"/github/workspace/internal/millers/graph/graphng.go:82\",\"func\":\"github.com/gleanerio/gleaner/internal/millers/graph.GraphNG\",\"level\":\"info\",\"msg\":\"Assembling result graph for prefix:summoned/geocodes_demo_datasetsto:milled/geocodes_demo_datasets\",\"time\":\"2022-07-22T19:16:56Z\"}\n    {\"file\":\"/github/workspace/internal/millers/graph/graphng.go:83\",\"func\":\"github.com/gleanerio/gleaner/internal/millers/graph.GraphNG\",\"level\":\"info\",\"msg\":\"Result graph will be at:results/runX/geocodes_demo_datasets_graph.nq\",\"time\":\"2022-07-22T19:16:56Z\"}\n    {\"file\":\"/github/workspace/internal/millers/graph/graphng.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/millers/graph.GraphNG\",\"level\":\"info\",\"msg\":\"Pipe copy for graph done\",\"time\":\"2022-07-22T19:16:56Z\"}\n    {\"file\":\"/github/workspace/internal/millers/millers.go:84\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Miller end time:2022-07-22 19:16:56.387639969 +0000 UTC m=+3.117994225\",\"time\":\"2022-07-22T19:16:56Z\"}\n    {\"file\":\"/github/workspace/internal/millers/millers.go:85\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Miller run time:0.0121029112\",\"time\":\"2022-07-22T19:16:56Z\"}</code></pre>"},{"location":"data_loading/indexing_with_gleanerio_for_testing/#push-to-graph","title":"push to graph","text":"<code>./glcon nabu prefix --cfgName ci</code> <p>```json lines ./glcon nabu prefix --cfgName ci INFO[0000] EarthCube Gleaner                           Using gleaner config file: /home/ubuntu/indexing/configs/ci/gleaner Using nabu config file: /home/ubuntu/indexing/configs/ci/nabu check called 2022/07/22 19:23:16 Load graphs from prefix to triplestore {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:41\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.ObjectAssembly\",\"level\":\"info\",\"msg\":\"[milled/geocodes_demo_datasets prov/geocodes_demo_datasets org]\",\"time\":\"2022-07-22T19:23:16Z\"} {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:61\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.ObjectAssembly\",\"level\":\"info\",\"msg\":\"gleaner:milled/geocodes_demo_datasets object count: 15\\n\",\"time\":\"2022-07-22T19:23:16Z\"} {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:79\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.PipeLoad\",\"level\":\"info\",\"msg\":\"Loading milled/geocodes_demo_datasets/11316929f925029101493e8a05d043b0ae829559.rdf \\n\",\"time\":\"2022-07-22T19:23:16Z\"} [snip] {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:197\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.Insert\",\"level\":\"info\",\"msg\":\"response Status: 200 OK\",\"time\":\"2022-07-22T19:23:21Z\"} {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:198\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.Insert\",\"level\":\"info\",\"msg\":\"response Headers: map[Access-Control-Allow-Credentials:[true] Access-Control-Allow-Headers:[Authorization,Origin,Content-Type,Accept] Access-Control-Allow-Origin:[*] Content-Length:[449] Content-Type:[text/html;charset=utf-8] Date:[Fri, 22 Jul 2022 19:23:21 GMT] Server:[Jetty(9.4.z-SNAPSHOT)] Vary:[Origin] X-Frame-Options:[SAMEORIGIN]]\",\"time\":\"2022-07-22T19:23:21Z\"} 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (1/1, 15 it/s)</p> <p>```</p>"},{"location":"data_loading/indexing_with_gleanerio_for_testing/#test-in-graph","title":"Test in Graph","text":"<p><code>https://graph.geocodes-dev.earthcube.org/blazegraph/#query</code></p>"},{"location":"data_loading/indexing_with_gleanerio_for_testing/#returns-all-triples","title":"returns all triples","text":"<pre><code>select * \nwhere {\n  ?s ?p ?o\n     }\nlimit 1000</code></pre>"},{"location":"data_loading/indexing_with_gleanerio_for_testing/#query-for-amgeo","title":"query for amgeo","text":"<pre><code>PREFIX bds: &lt;http://www.bigdata.com/rdf/search#&gt;\nPREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nprefix schema: &lt;http://schema.org/&gt;\nprefix sschema: &lt;https://schema.org/&gt;\nSELECT distinct ?subj ?g ?resourceType ?name ?description  ?pubname\n(GROUP_CONCAT(DISTINCT ?placename; SEPARATOR=\", \") AS ?placenames)\n(GROUP_CONCAT(DISTINCT ?kwu; SEPARATOR=\", \") AS ?kw)\n?datep  (GROUP_CONCAT(DISTINCT ?url; SEPARATOR=\", \") AS ?disurl) (MAX(?score1) as ?score)\n(MAX(?lat) as ?maxlat) (Min(?lat) as ?minlat) (MAX(?lon) as ?maxlon) (Min(?lon) as ?minlon)\nWHERE {\n?lit bds:search \"amgeo\" .\n?lit bds:matchAllTerms false .\n?lit bds:relevance ?score1 .\n?lit bds:minRelevance 0.14 .\n?subj ?p ?lit .\n#filter( ?score1 &gt; 0.14).\ngraph ?g {\n?subj schema:name|sschema:name ?name .\n?subj schema:description|sschema:description ?description .\n#Minus {?subj a sschema:ResearchProject } .\n# Minus {?subj a schema:ResearchProject } .\n# Minus {?subj a schema:Person } .\n# Minus {?subj a sschema:Person } .\n}\n#BIND (IF (exists {?subj a schema:Dataset .} ||exists{?subj a sschema:Dataset .} , \"data\", \"tool\" ) AS ?resourceType).\nvalues (?type ?resourceType) {\n(schema:Dataset \"data\")\n(sschema:Dataset \"data\")\n(schema:ResearchProject \"Research Project\") #BCODMO- project\n(sschema:ResearchProject  \"Research Project\")\n(schema:SoftwareApplication  \"tool\")\n(sschema:SoftwareApplication  \"tool\")\n(schema:Person  \"Person\") #BCODMO- Person\n(sschema:Person  \"Person\")\n(schema:Event  \"Event\") #BCODMO- deployment\n(sschema:Event  \"Event\")\n(schema:Award  \"Award\") #BCODMO- Award\n(sschema:Award  \"Award\")\n(schema:DataCatalog  \"DataCatalog\")\n(sschema:DataCatalog  \"DataCatalog\")\n#(UNDEF \"other\")  # assume it's data. At least we should get  name.\n} ?subj a ?type .\noptional {?subj schema:distribution/schema:url|sschema:subjectOf/sschema:url ?url .}\nOPTIONAL {?subj schema:datePublished|sschema:datePublished ?datep .}\nOPTIONAL {?subj schema:publisher/schema:name|sschema:publisher/sschema:name|sschema:sdPublisher|sschema:provider/schema:name ?pubname .}\nOPTIONAL {?subj schema:spatialCoverage/schema:name|sschema:spatialCoverage/sschema:name ?placename .}\n\nOPTIONAL {?subj schema:keywords|sschema:keywords ?kwu .}\n\n}\nGROUP BY ?subj ?pubname ?placenames ?kw ?datep ?disurl ?score ?name ?description  ?resourceType ?g ?minlat ?maxlat ?minlon ?maxlon\nORDER BY DESC(?score)\nLIMIT 100\nOFFSET 0\n</code></pre>"},{"location":"data_loading/indexing_with_gleanerio_for_testing/#test-in-client","title":"test in client","text":"<p><code>https://geocodes.geocodes-dev.earthcube.org</code></p> <ul> <li>terms</li> <li>amgeo</li> <li>bcodmo</li> </ul>"},{"location":"data_loading/indexing_with_gleanerio_for_testing/#detailed-testing","title":"Detailed Testing","text":"<p>TODO Need to ses the datavalidaton story:</p>"},{"location":"data_loading/install_glcon/","title":"Install Glcon","text":""},{"location":"data_loading/install_glcon/#install-glcon","title":"Install glcon","text":"<p><code>glcon</code> is a console application that combines the functionality of Gleaner and Nabu into a single application. It also has features to create and manage configurations for gleaner and nabu.</p> <ul> <li>create a directory <code>cd ~ ; mkdir indexing</code></li> <li>download and install: We will try to keep this updated, but for the latest release. <code>wget https://github.com/gleanerio/gleaner/releases/download/{{RELASE}}}}</code></li> </ul> <p><code>tar xf glcon-v3.0.8-linux-amd64.tar.gz</code></p> OS download linux intel glcon-{{VERSION}}-linux-amd64.tar.gz linux arm glcon-{{VERSION}}-linux-arm64.tar.gz mac glcon-{{VERSION}}-darwin-arm64.tar.gz windows  intel glcon-{{VERSION}}-windows-amd64.zip downloading glcon <pre><code>    3.0.4-dev/glcon-v3.0.4-dev-linux-amd64.tar.gz\n    --2022-07-21 23:04:55--  https://github.com/gleanerio/gleaner/releases/download/v3.0.4-dev/glcon-v3.0.4-dev-linux-amd64.tar.gz\n    Resolving github.com (github.com)... 140.82.113.4\n    Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n    HTTP request sent, awaiting response... 302 Found\n    Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/127204495/28707eb9-9cd2-4d4e-8b94-5e27db26a08f?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220721%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20220721T230428Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=668c44362081f0506f138cc52483f54d73fbd48fa906365ac80909b3b5e2b787&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;key_id=0&amp;repo_id=127204495&amp;response-content-disposition=attachment%3B%20filename%3Dglcon-v3.0.4-dev-linux-amd64.tar.gz&amp;response-content-type=application%2Foctet-stream [following]\n    --2022-07-21 23:04:56--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/127204495/28707eb9-9cd2-4d4e-8b94-5e27db26a08f?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220721%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20220721T230428Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=668c44362081f0506f138cc52483f54d73fbd48fa906365ac80909b3b5e2b787&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;key_id=0&amp;repo_id=127204495&amp;response-content-disposition=attachment%3B%20filename%3Dglcon-v3.0.4-dev-linux-amd64.tar.gz&amp;response-content-type=application%2Foctet-stream\n    Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n    Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n    HTTP request sent, awaiting response... 200 OK\n    Length: 13826668 (13M) [application/octet-stream]\n    Saving to: \u2018glcon-v3.0.4-dev-linux-amd64.tar.gz\u2019\n\nglcon-v3.0.4-dev-linux- 100%[=============================&gt;]  13.19M  12.6MB/s    in 1.0s</code></pre> <p>See that it is installed <pre><code>ubuntu@geocodes-dev:~/indexing$ tar xf glcon-v3.0.4-dev-linux-amd64.tar.gz\nubuntu@geocodes-dev:~/indexing$ ls\nREADME.md  docs   glcon-v3.0.4-dev-linux-amd64.tar.gz  scripts\nconfigs    glcon  schemaorg-current-https.jsonld</code></pre></p> <ul> <li>test</li> </ul> <code>./glcon --help</code> <pre><code>ubuntu@geocodes-dev:~/indexing$ ./glcon --help\nINFO[0000] EarthCube Gleaner                            \nThe gleaner.io stack harvests JSON-LD from webpages using sitemaps and other tools\nstore files in S3 (we use minio), uploads triples to be processed by nabu (the next step in the process)\nconfiguration is now focused on a directory (default: configs/local) with will contain the\nprocess to configure and run is:\n* glcon config init --cfgName {default:local}\n  edit files, servers.yaml, sources.csv\n* glcon config generate --cfgName  {default:local}\n* glcon gleaner Setup --cfgName  {default:local}\n* glcon gleaner batch  --cfgName  {default:local}\n* run nabu (better description)\n\nUsage:\n  glcon [command]\n\nAvailable Commands:\n  completion  generate the autocompletion script for the specified shell\n  config      commands to intialize, and generate tools: gleaner and nabu\n  gleaner     command to execute gleaner processes\n  help        Help about any command\n  nabu        command to execute nabu processes\n\nFlags:\n      --access string        Access Key ID (default \"MySecretAccessKey\")\n      --address string       FQDN for server (default \"localhost\")\n      --bucket string        The configuration bucket (default \"gleaner\")\n      --cfg string           compatibility/overload: full path to config file (default location gleaner in configs/local)\n      --cfgName string       config file (default is local so configs/local) (default \"local\")\n      --cfgPath string       base location for config files (default is configs/) (default \"configs\")\n      --gleanerName string   config file (default is local so configs/local) (default \"gleaner\")\n  -h, --help                 help for glcon\n      --nabuName string      config file (default is local so configs/local) (default \"nabu\")\n      --port string          Port for minio server, default 9000 (default \"9000\")\n      --secret string        Secret access key (default \"MySecretSecretKeyforMinio\")\n      --ssl                  Use SSL boolean\n\nUse \"glcon [command] --help\" for more information about a command.</code></pre>"},{"location":"data_loading/onboarding_or_testing_a_datasource/","title":"Oboarding a Dataseource/Repository","text":"<p>If we have an issue with a repository, let's test it, independently. If we have a new datasource/community, let's test it, independently.</p> <p>We can do this because:</p> <ul> <li>s3 paths are repository based, so we can load to an s3 bucket</li> <li>blazegraph, we can create two namespaces in our openstack blazegraph stores</li> <li>We can setup 'tenant' instances of the UI that connect to  (s3 bucket, and project namespaces) services</li> <li>we have some source reports to help evaluate the data loading.</li> </ul> <p>So, basically, This assumes we have some basic information about a data source, aka sitemap, and something we want to  name this repository.</p> <p>Note</p> <p>This is a high level overview that assumes you have loaded data, and do not need any deep details. Over time put the troubleshooting an gotchas below the steps.</p> <p>Note</p> <p>if source is large, run in a screen In fact, it is suggested to always run in a screen</p> <p>Please put any issues/notes in the production/repos google docs</p>"},{"location":"data_loading/onboarding_or_testing_a_datasource/#some-steps","title":"Some steps.","text":"<ol> <li>Grab some urls from the sitemap, evaluate in validator.schema.org</li> <li>run check_sitemap to see it url's are good</li> <li>setup datastores<ul> <li>any s3</li> <li>independent project and project_summary namespaces</li> </ul> </li> <li>create gleaner config for repo</li> <li>if source is large,  use screen e.g. <code>screen -S gleaner</code></li> <li><code>glcon gleaner batch</code> Summon to an s3 location. Repos are independent at this point.<ul> <li>did we suggest runing in a screen</li> </ul> </li> <li>evaluate summon. Look at jsonld. Do they seem like they got loaded?<ul> <li>thought: do we need a tool to pull a specific url from s3? could filter the listSummonedUrls, we do have getOringalUrl</li> <li>run missing stats... may need an option to just check the sitemap&gt;summon portion</li> </ul> </li> <li><code>glcon nabu prefix</code> if these look good, then <ul> <li>did we suggest runing in a screen</li> <li>there needs to be a note about using <code>glcon nabu release --cfgName CONFIG</code>, and how to upload the quads</li> </ul> </li> <li>run <code>graph_stats</code> and <code>missing...</code><ul> <li>graph stats report needs to be updated to include [all/repo]_count_types_top_level.sparql, </li> </ul> </li> <li>check reports</li> <li>feel free to run repo_urn_w_types_toplevel.sparql</li> <li>run summarize_* to populate summary</li> <li>create a facets configuration for the project, upload to portainer, and create stack of tenant containers to run against the project and project_summary namespaces</li> <li>via UI run queries to see that it works.<ul> <li>humm, add a simple query tester to scripts</li> </ul> </li> <li>Review with team</li> <li>review with datasource</li> <li>add to production sources</li> </ol>"},{"location":"data_loading/onboarding_or_testing_a_datasource/#evaluating-with-validator","title":"Evaluating with validator","text":"<p>what to look for</p>"},{"location":"data_loading/onboarding_or_testing_a_datasource/#evaluating-summon","title":"evaluating summon","text":"<p>What to look for</p>"},{"location":"data_loading/onboarding_or_testing_a_datasource/#evaluating-graph-and-missing-reports","title":"Evaluating graph and missing reports","text":"<p>What to look for</p>"},{"location":"data_loading/onboarding_or_testing_a_datasource/#how-to-test-the-ui","title":"How to test the UI","text":"<p>keywords: * data * repository keywords</p>"},{"location":"data_loading/setup_indexing_with_gleanerio/","title":"Loading Data for The Initial Installation","text":"<p>This is step 4 of 5 major steps:</p> <ol> <li>Install base containers on a server</li> <li>Setup services containers</li> <li>Setup Gleaner containers</li> <li>Initial setup of services and loading of data</li> <li>Setup Geocodes UI using datastores defined in Initial Setup</li> </ol> <p>Step Overview:</p> <ol> <li>create data stores in minioadmin and graph</li> <li>install glcon, if not installed</li> <li>create a configuration file to install a small set of data<ol> <li><code>./glcon config init --cfgName gctest</code></li> <li>edit</li> <li><code>./glcon config generate --cfgName gctest</code></li> </ol> </li> <li>setup and summon data using 'gleaner' <ol> <li><code>./glcon gleaner setup --cfgName gctest</code></li> <li><code>./glcon gleaner batch --cfgName gctest</code></li> </ol> </li> <li>load data to graph using 'nabu' <ol> <li><code>./glcon nabu prefix --cfgName gctest</code></li> <li><code>./glcon nabu prune --cfgName gctest</code></li> </ol> </li> <li>Test data in Graph </li> <li>Example of how to edit the source</li> <li>edit gctest.csv</li> <li>regenerate configs</li> <li>rerun batch</li> <li>Run Summarize task. This is performance related.</li> </ol> <p>regenerate</p> <p>if you edit localConfig.yaml, you need to regenerate the configs using <code>./glcon config generate --cfgName gctest</code></p>"},{"location":"data_loading/setup_indexing_with_gleanerio/#step-details","title":"Step Details","text":""},{"location":"data_loading/setup_indexing_with_gleanerio/#setup-datastores","title":"Setup Datastores","text":"<p>There are several datastores required to enable data summoning(harvesting), converting to a graph. While the production presently uses the earthcube repository convention, we suggest that  tutorial and communities setting up an instance to use the geocodes repository pattern. Earthcube/Decoder staff should use the A Community pattern when setting up an instance for a community.</p> Repository config s3 Bucket graph namespaces notes GeocodesTest gctest gctest gctest, gctest_summary samples of actual datasets geocodes geocodes geocodes geocodes, geocodes_summary suggested standalone repository earthcube geocodes gleaner earthcube, summary DEFAULT PRODUCTION NAME A COMMUNITY eg {acomm} {acomm} {acomm}, {acomm}_summary A communities tenant repository <p>Initial Setup</p> <p>we will be setting up both the gctest and gecodes repositories.</p>"},{"location":"data_loading/setup_indexing_with_gleanerio/#setup-minio-buckets","title":"Setup Minio buckets","text":"<p>Gleaner extracts JSONLD from a web apge, and stores it in an s3 system (Minio) in </p> <p>go to https://minioadmin.{youhost}/</p> <p>create buckets gctest, and geocodes</p> <p>go to settings for the bucket and make  public.</p>"},{"location":"data_loading/setup_indexing_with_gleanerio/#setup-graph-stores","title":"Setup Graph stores.","text":"<p>Nabu pulls from the s3 system, converts to RDF quads, and uploads to a graph store.</p> <p>go to https://graph.{your host}</p> <p>namespace tab, create  a mode 'quads' namespace with full text index,  \"gctest\", and \"geocodes\"</p> <p>namespace tab, create mode 'triples' namespace with full text index, \"gctest_summary\", and \"geocodes_summary\"</p>"},{"location":"data_loading/setup_indexing_with_gleanerio/#install-indexing-software","title":"Install Indexing Software","text":"<p><code>glcon</code> is a console application that combines the functionality of Gleaner and Nabu into a single application. It also has features to create and manage configurations for gleaner and nabu.</p> <p>Install glcon</p>"},{"location":"data_loading/setup_indexing_with_gleanerio/#harvest-and-load-data","title":"Harvest and load data","text":"<p>Goal is to create a configuration file to load gctest data. The sitemap is here:</p>"},{"location":"data_loading/setup_indexing_with_gleanerio/#create-a-configuration-and-load-sample-data","title":"Create a configuration and load sample data","text":""},{"location":"data_loading/setup_indexing_with_gleanerio/#create-a-configuration-for-continuous-integration","title":"Create a configuration for Continuous Integration","text":"<code>./glcon config init --cfgName gctest</code> <pre><code>   ubuntu@geocodes-dev:~/indexing$ ./glcon config init --cfgName gctest\n    2022/07/21 23:27:31 EarthCube Gleaner\n    init called\n    make a config template is there isn't already one\n    ubuntu@geocodes-dev:~/indexing$ ls configs/gctest\n    README_Configure_Template.md  localConfig.yaml  sources.csv\n    gleaner_base.yaml             nabu_base.yaml\n    ubuntu@geocodes-dev:~/indexing$ </code></pre>"},{"location":"data_loading/setup_indexing_with_gleanerio/#copy-sources-list-to-configsgctest","title":"Copy sources list to configs/gctest","text":"<p>Note</p> <p>assumes you are in indexing, and have put the geocodes at ~/geocodes aka your home directory</p> <p><code>cp ~/geocodes/deployment/ingestconfig/gctest.csv configs/gctest/</code></p>"},{"location":"data_loading/setup_indexing_with_gleanerio/#edit-files","title":"edit files:","text":"<p>You will need to change the localConfig.yaml</p> <code>nano configs/gctest/localConfig.yaml</code> <pre><code>---\nminio:\n  address: oss.{YOU HOST}\n  port: 433\n  accessKey: worldsbestaccesskey\n  secretKey: worldsbestaccesskey\n  ssl: true\n  bucket: gctest # can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: https://graph.{YOU HOST}/blazegraph/namespace/gctest/sparql\ns3:\n  bucket: gctest # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1 \n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n  location: gctest.csv \n# this can be a remote csv\n#  type: csv\n#  location: https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=TestDatasetSources</code></pre> <p>regenerate</p> <p>if you edit localConfig.yaml, you need to regenerate the configs using <code>./glcon config generate --cfgName gctest</code></p>"},{"location":"data_loading/setup_indexing_with_gleanerio/#generate-configs","title":"Generate configs","text":"<code>./glcon config generate --cfgName gctest</code> <pre><code>./glcon config generate --cfgName gctest\n2022/07/21 23:37:46 EarthCube Gleaner\ngenerate called\n{SourceType:sitemap Name:geocodes_demo_datasets Logo:https://github.com/earthcube/GeoCODES-Metadata/metadata/OtherResources URL:https://raw.githubusercontent.com/earthcube/GeoCODES-Metadata/gh-pages/metadata/Dataset/sitemap.xml Headless:false PID:https://www.earthcube.org/datasets/ ProperName:Geocodes Demo Datasets Domain:0 Active:true CredentialsFile: Other:map[] HeadlessWait:0}\nmake copy of servers.yaml\nRegnerate gleaner\nRegnerate nabu</code></pre>"},{"location":"data_loading/setup_indexing_with_gleanerio/#flightest","title":"flightest","text":"<p>Run setup to see if you can connect to the minio store</p> `./glcon gleaner setup --cfgName gctest <pre><code>   ubuntu@geocodes-dev:~/indexing$ ./glcon gleaner setup --cfgName gctest\n   2022/07/21 23:42:54 EarthCube Gleaner\n   Using gleaner config file: /home/ubuntu/indexing/configs/gctest/gleaner\n   Using nabu config file: /home/ubuntu/indexing/configs/gctest/nabu\n   setup called\n   2022/07/21 23:42:54 Validating access to object store\n   2022/07/21 23:42:54 Connection issue, make sure the minio server is running and accessible. The specified bucket does not exist.\n   ubuntu@geocodes-dev:~/indexing$ </code></pre> <p>Access issues</p> <pre><code>{\u201cfile\u201d:\u201c/github/workspace/internal/organizations/org.go:87\",\u201cfunc\u201d:\u201cgithub.com/gleanerio/gleaner/internal/organizations.BuildGraph\u201d,\u201clevel\u201d:\u201cerror\u201d,\u201cmsg\u201d:\u201corgs/geocodes_demo_datasets.nqThe Access Key Id you provided does not exist in our records.\u201c,\u201dtime\u201d:\u201c2023-01-31T15:27:39-06:00\u201d}</code></pre> <ul> <li>Access Key password could be incorrect</li> <li>address may be incorrect. It is a hostname or TC/IP, and not a URL</li> <li>ssl may need to be true</li> <li>See setup issues</li> </ul>"},{"location":"data_loading/setup_indexing_with_gleanerio/#load-data","title":"Load Data","text":"<p>Gleaner will harvest jsonld from the URL's listed in the sitemap.</p> <p>Robots.txt</p> <p>OK TO IGNORE. you will need to ignore errors about robot.txt and sitemap.xml not being an index <pre><code>{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:204\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"error\",\"msg\":\"error getting robots.txt for https://www.earthcube.org/datasets/allgood:Robots.txt unavailable at https://www.earthcube.org/datasets/allgood/robots.txt\",\"time\":\"2023-01-30T20:45:53-06:00\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:66\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResourceURLs\",\"level\":\"error\",\"msg\":\"Error getting robots.txt for geocodes_demo_datasets, continuing without it.\",\"time\":\"2023-01-30T20:45:53-06:00\"}    </code></pre></p> <p>Access issues</p> <pre><code>{\u201cfile\u201d:\u201c/github/workspace/internal/organizations/org.go:87\",\u201cfunc\u201d:\u201cgithub.com/gleanerio/gleaner/internal/organizations.BuildGraph\u201d,\u201clevel\u201d:\u201cerror\u201d,\u201cmsg\u201d:\u201corgs/geocodes_demo_datasets.nqThe Access Key Id you provided does not exist in our records.\u201c,\u201dtime\u201d:\u201c2023-01-31T15:27:39-06:00\u201d}</code></pre> <ul> <li>Access Key password could be incorrect </li> <li>address may be incorrect. It is a hostname or TC/IP, and not a URL</li> <li>ssl may need to be true</li> <li>See setup issues</li> </ul> <code>./glcon gleaner batch --cfgName gctest</code> <pre><code>ubuntu@geocodes-dev:~/indexing$ ./glcon gleaner batch --cfgName gctest\nINFO[0000] EarthCube Gleaner                            \nUsing gleaner config file: /home/ubuntu/indexing/configs/gctest/gleaner\nUsing nabu config file: /home/ubuntu/indexing/configs/gctest/nabu\nbatch called\n{\"file\":\"/github/workspace/internal/organizations/org.go:55\",\"func\":\"github.com/gleanerio/gleaner/internal/organizations.BuildGraph\",\"level\":\"info\",\"msg\":\"Building organization graph.\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/pkg/gleaner.go:35\",\"func\":\"github.com/gleanerio/gleaner/pkg.Cli\",\"level\":\"info\",\"msg\":\"Sitegraph(s) processed\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/summoner.go:17\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner.Summoner\",\"level\":\"info\",\"msg\":\"Summoner start time:2022-07-22 19:16:53.451745139 +0000 UTC m=+0.182100234\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:189\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"info\",\"msg\":\"Getting robots.txt from 0/robots.txt\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/utils.go:23\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsTxt\",\"level\":\"error\",\"msg\":\"error fetching robots.txt at 0/robots.txtGet \\\"0/robots.txt\\\": unsupported protocol scheme \\\"\\\"\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:192\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"error\",\"msg\":\"error getting robots.txt for 0:Get \\\"0/robots.txt\\\": unsupported protocol scheme \\\"\\\"\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:63\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResourceURLs\",\"level\":\"error\",\"msg\":\"Error getting robots.txt for geocodes_demo_datasetscontinuing without it.\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:127\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getSitemapURLList\",\"level\":\"info\",\"msg\":\"https://raw.githubusercontent.com/earthcube/GeoCODES-Metadata/gh-pages/metadata/Dataset/sitemap.xml is not a sitemap index, checking to see if it is a sitemap\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/acquire.go:32\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResRetrieve\",\"level\":\"info\",\"msg\":\"Queuing URLs for geocodes_demo_datasets\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/acquire.go:74\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getConfig\",\"level\":\"info\",\"msg\":\"Thread count 5 delay 0\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n12% |\u2588\u2588\u2588\u2588\u2588\u2588                                                 | (2/16, 2 it/s) [0s:7s]{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n43% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                | (7/16, 6 it/s) [1s:1s]{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n68% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                  | (11/16, 6 it/s) [1s:0s]{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n75% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588              | (12/16, 6 it/s) [1s:0s]{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (16/16, 9 it/s)        \n{\"file\":\"/github/workspace/internal/summoner/summoner.go:37\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner.Summoner\",\"level\":\"info\",\"msg\":\"Summoner end time:2022-07-22 19:16:55.660367672 +0000 UTC m=+2.390721648\",\"time\":\"2022-07-22T19:16:55Z\"}\n{\"file\":\"/github/workspace/internal/summoner/summoner.go:38\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner.Summoner\",\"level\":\"info\",\"msg\":\"Summoner run time:0.0368103569\",\"time\":\"2022-07-22T19:16:55Z\"}\n{\"file\":\"/github/workspace/internal/millers/millers.go:27\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Miller start time2022-07-22 19:16:55.661434567 +0000 UTC m=+2.391819553\",\"time\":\"2022-07-22T19:16:55Z\"}\n{\"file\":\"/github/workspace/internal/millers/millers.go:44\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Adding bucket to milling list:summoned/geocodes_demo_datasets\",\"time\":\"2022-07-22T19:16:55Z\"}\n{\"file\":\"/github/workspace/internal/millers/millers.go:55\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Adding bucket to prov building list:prov/geocodes_demo_datasets\",\"time\":\"2022-07-22T19:16:55Z\"}\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (15/15, 51 it/s)        \n{\"file\":\"/github/workspace/internal/millers/graph/graphng.go:82\",\"func\":\"github.com/gleanerio/gleaner/internal/millers/graph.GraphNG\",\"level\":\"info\",\"msg\":\"Assembling result graph for prefix:summoned/geocodes_demo_datasetsto:milled/geocodes_demo_datasets\",\"time\":\"2022-07-22T19:16:56Z\"}\n{\"file\":\"/github/workspace/internal/millers/graph/graphng.go:83\",\"func\":\"github.com/gleanerio/gleaner/internal/millers/graph.GraphNG\",\"level\":\"info\",\"msg\":\"Result graph will be at:results/runX/geocodes_demo_datasets_graph.nq\",\"time\":\"2022-07-22T19:16:56Z\"}\n{\"file\":\"/github/workspace/internal/millers/graph/graphng.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/millers/graph.GraphNG\",\"level\":\"info\",\"msg\":\"Pipe copy for graph done\",\"time\":\"2022-07-22T19:16:56Z\"}\n{\"file\":\"/github/workspace/internal/millers/millers.go:84\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Miller end time:2022-07-22 19:16:56.387639969 +0000 UTC m=+3.117994225\",\"time\":\"2022-07-22T19:16:56Z\"}\n{\"file\":\"/github/workspace/internal/millers/millers.go:85\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Miller run time:0.0121029112\",\"time\":\"2022-07-22T19:16:56Z\"}</code></pre> <p>See files in Minio</p> <p>You can open the minioadmin console (https://minioadmin.{your host}/) and look to see that file are uploaded into the bucket, in this case gctest.. summon/gecodes_demo_data</p> <p>(NEED IMAGE HERE)</p>"},{"location":"data_loading/setup_indexing_with_gleanerio/#push-to-graph","title":"Push to graph","text":"<p>Nabu will read files from the bucket, and push them to the graph store.</p> <code>./glcon nabu prefix --cfgName gctest</code> <p>```json lines ./glcon nabu prefix --cfgName gctest INFO[0000] EarthCube Gleaner                           Using gleaner config file: /home/ubuntu/indexing/configs/gctest/gleaner Using nabu config file: /home/ubuntu/indexing/configs/gctest/nabu check called 2022/07/22 19:23:16 Load graphs from prefix to triplestore {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:41\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.ObjectAssembly\",\"level\":\"info\",\"msg\":\"[milled/geocodes_demo_datasets prov/geocodes_demo_datasets org]\",\"time\":\"2022-07-22T19:23:16Z\"} {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:61\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.ObjectAssembly\",\"level\":\"info\",\"msg\":\"gleaner:milled/geocodes_demo_datasets object count: 15\\n\",\"time\":\"2022-07-22T19:23:16Z\"} {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:79\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.PipeLoad\",\"level\":\"info\",\"msg\":\"Loading milled/geocodes_demo_datasets/11316929f925029101493e8a05d043b0ae829559.rdf \\n\",\"time\":\"2022-07-22T19:23:16Z\"} [snip] {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:197\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.Insert\",\"level\":\"info\",\"msg\":\"response Status: 200 OK\",\"time\":\"2022-07-22T19:23:21Z\"} {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:198\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.Insert\",\"level\":\"info\",\"msg\":\"response Headers: map[Access-Control-Allow-Credentials:[true] Access-Control-Allow-Headers:[Authorization,Origin,Content-Type,Accept] Access-Control-Allow-Origin:[*] Content-Length:[449] Content-Type:[text/html;charset=utf-8] Date:[Fri, 22 Jul 2022 19:23:21 GMT] Server:[Jetty(9.4.z-SNAPSHOT)] Vary:[Origin] X-Frame-Options:[SAMEORIGIN]]\",\"time\":\"2022-07-22T19:23:21Z\"} 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (1/1, 15 it/s)</p> <p>```</p>"},{"location":"data_loading/setup_indexing_with_gleanerio/#test-in-graph","title":"Test in Graph","text":"<p>One the data is loaded into the graph store <code>https://graph.{your host}/blazegraph/#query</code></p> <ol> <li>go to namespace tab, select gctest, </li> <li>go to query tab, input the </li> </ol> returns all triples <pre><code>select * \nwhere {\n?s ?p ?o\n }\nlimit 1000</code></pre> <p>A more complex query can be ran:</p> what types are in the system <pre><code>prefix schema: &lt;https://schema.org/&gt;\nSELECT  ?type  (count(distinct ?s ) as ?scount)\nWHERE {\n{\n       ?s a ?type .\n       }\n} \nGROUP By ?type\nORDER By DESC(?scount)</code></pre> <p>A more complex query can be ran:</p> Show me just datasets <pre><code>SELECT (count(?g ) as ?count) \nWHERE     {     GRAPH ?g {?s a &lt;https://schema.org/Dataset&gt;}}</code></pre> <p>More SPARQL Examples</p>"},{"location":"data_loading/setup_indexing_with_gleanerio/#example-of-how-to-edit-the-source","title":"Example of how to edit the source","text":"<p>This demonstrates a feature where if you have duplicate identifiers, then you can ensure all data get loaded. It's a bad idea to have the same ID, but it happens.</p> <p>There are two lines in gctest csv.  The second dataset is [actual data] (https://github.com/earthcube/GeoCODES-Metadata/tree/main/metadata/Dataset/actualdata).  There are three files, the two earthchem files have the same @id, 1 2 The identifierType is set to 'filesha' which generates a sha based on the entire file.</p> gctest cs <pre><code>hack,SourceType,Active,Name,ProperName,URL,Headless,HeadlessWait,IdentifierType,IdentifierPath,Domain,PID,Logo,validator link,NOTE\n58,sitemap,TRUE,geocodes_demo_datasets,Geocodes Demo Datasets,https://earthcube.github.io/GeoCODES-Metadata/metadata/Dataset/allgood/sitemap.xml,FALSE,0,identifiersha,,https://www.earthcube.org/datasets/allgood,https://github.com/earthcube/GeoCODES-Metadata/metadata/OtherResources,,,\n59,sitemap,FALSE,geocodes_actual_datasets,Geocodes Actual Datasets,https://earthcube.github.io/GeoCODES-Metadata/metadata/Dataset/actualdata/sitemap.xml,FALSE,0,filesha,,https://www.earthcube.org/datasets/actual,https://github.com/earthcube/GeoCODES-Metadata/metadata/,,,</code></pre>"},{"location":"data_loading/setup_indexing_with_gleanerio/#edit-gctestcsv","title":"edit gctest.csv","text":"<p>Set the second line active to TRUE</p> edited gctest cs <pre><code>hack,SourceType,Active,Name,ProperName,URL,Headless,HeadlessWait,IdentifierType,IdentifierPath,Domain,PID,Logo,validator link,NOTE\n58,sitemap,TRUE,geocodes_demo_datasets,Geocodes Demo Datasets,https://earthcube.github.io/GeoCODES-Metadata/metadata/Dataset/allgood/sitemap.xml,FALSE,0,identifiersha,,https://www.earthcube.org/datasets/allgood,https://github.com/earthcube/GeoCODES-Metadata/metadata/OtherResources,,,\n59,sitemap,TRUE,geocodes_actual_datasets,Geocodes Actual Datasets,https://earthcube.github.io/GeoCODES-Metadata/metadata/Dataset/actualdata/sitemap.xml,FALSE,0,filesha,,https://www.earthcube.org/datasets/actual,https://github.com/earthcube/GeoCODES-Metadata/metadata/,,,</code></pre>"},{"location":"data_loading/setup_indexing_with_gleanerio/#regenerate-configs","title":"regenerate configs","text":"<p><code>./glcon config generate --cfgName gctest</code></p>"},{"location":"data_loading/setup_indexing_with_gleanerio/#rerun-batch","title":"rerun batch","text":"<code>./glcon gleaner batch --cfgName gctest</code> <pre><code>ubuntu@geocodes:~/indexing$ ./glcon gleaner batch --cfgName gctest\nversion:  v3.0.8-fix129\nbatch called\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:204\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"error\",\"msg\":\"error getting robots.txt for https://www.earthcube.org/datasets/allgood:Robots.txt unavailable at https://www.earthcube.org/datasets/allgood/robots.txt\",\"time\":\"2023-01-30T21:09:49-06:00\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:66\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResourceURLs\",\"level\":\"error\",\"msg\":\"Error getting robots.txt for geocodes_demo_datasets, continuing without it.\",\"time\":\"2023-01-30T21:09:49-06:00\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:204\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"error\",\"msg\":\"error getting robots.txt for https://www.earthcube.org/datasets/actual:Robots.txt unavailable at https://www.earthcube.org/datasets/actual/robots.txt\",\"time\":\"2023-01-30T21:09:49-06:00\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:66\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResourceURLs\",\"level\":\"error\",\"msg\":\"Error getting robots.txt for geocodes_actual_datasets, continuing without it.\",\"time\":\"2023-01-30T21:09:49-06:00\"}\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (3/3, 10 it/s)        \n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (9/9, 25 it/s)        \nRunStats:\n  Start: 2023-01-30 21:09:49.120833598 -0600 CST m=+0.105789938\n  Repositories:\n    - name: geocodes_demo_datasets\n      SitemapCount: 9 \n      SitemapHttpError: 0 \n      SitemapIssues: 0 \n      SitemapSummoned: 9 \n      SitemapStored: 9 \n    - name: geocodes_actual_datasets\n      SitemapSummoned: 3 \n      SitemapStored: 3 \n      SitemapCount: 3 \n      SitemapHttpError: 0 \n      SitemapIssues: 0 \n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (9/9, 168 it/s)\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (2/2, 123 it/s)</code></pre>"},{"location":"data_loading/setup_indexing_with_gleanerio/#create-a-materialized-view-of-the-data-using-summarize-to-the-repo_summary-namespace","title":"Create  a materialized view of the data using summarize to the  repo_summary namespace","text":"<p>DOCUMENTATION NEEDED </p> <p>(TBD assigned to Mike Bobak)</p>"},{"location":"data_loading/setup_indexing_with_gleanerio/#go-to-step-5","title":"Go to step 5.","text":"<ol> <li>Install base containers on a server</li> <li>Setup services containers</li> <li>Setup Gleaner containers</li> <li>Initial setup of services and loading of data</li> <li>Setup Geocodes UI using datastores defined in Initial Setup</li> </ol>"},{"location":"data_loading/using_screen_for_manual_loading/","title":"Manual Loading with <code>screen</code>","text":"<p>If a source has many URLs/records, it suggest that one runs the  <code>glcon</code> \\ <code>gleaner</code> command in a <code>screen</code></p>"},{"location":"data_loading/using_screen_for_manual_loading/#start-a-screen","title":"start a screen","text":"<p>Since this is a long running process, it is suggested that this be done in <code>screen</code> There is also a possibility of using tmux, if a user has experience with it.</p> <p>But someonee else needs to write that up.</p>"},{"location":"data_loading/using_screen_for_manual_loading/#notes","title":"Notes:","text":"<p>To create a 'named' screen named gleaner</p> <p><code>screen -S gleaner</code></p> <p>to detach from screen, use: <code>ctl-a-d</code></p> <p>to find running screens: <code>screen -ls</code></p> <pre><code>There are screens on:\n    7187.gleaner    (07/28/22 17:43:48) (Detached)\n    6879.pts-3.geocodes-dev (07/28/22 17:33:25) (Detached)\n2 Sockets in /run/screen/S-ubuntu.</code></pre> <p>to attach to a screen  in this case you use the name</p> <p><code>screen -r gleaner</code></p> <p>or</p> <p><code>screen -r pts-3</code></p> <pre><code>screen -S gleaner\nubuntu@geocodes-dev:~/indexing$ screen -ls\nThere is a screen on:\n    7187.gleaner    (07/28/22 17:43:48)   (Attached)\n1 Socket in /run/screen/S-ubuntu.</code></pre>"},{"location":"data_loading/watching_manual_gleanerio_data_loading/","title":"Data Loading - Watching CLI/Manual Loading","text":""},{"location":"data_loading/watching_manual_gleanerio_data_loading/#when-you-are-doing-a-manual-load-things-you-can-observe","title":"When you are doing a manual load, things you can observe","text":"<ul> <li>console</li> <li>logs</li> <li>Minio/s3</li> </ul> <p>Note</p> <p>Remember to use <code>screen</code> for long data loads. This will make sure the process  does not die when a terminal disconnccts. <code>screen -S {SOME_NAME}</code></p>"},{"location":"data_loading/watching_manual_gleanerio_data_loading/#console","title":"console","text":"<ul> <li><code>screen -ls</code></li> <li><code>screen r {SOME_NAME}</code> You should see a set of json records being reported.</li> </ul> <p>Note</p> <p>Nabu will provide a progress bar, but not anything into the main gleaner log.</p>"},{"location":"data_loading/watching_manual_gleanerio_data_loading/#logs","title":"Logs","text":"<ul> <li><code>cd indexing</code> or whereever you ran glcon from</li> <li><code>ls -l logs</code> You will see a set of logs. initally, this will be there:</li> </ul> <pre><code>gleaner-2023-04-27-21-56-46.log</code></pre> <p>Then when sitemaps are loaded, reposiories will appear:</p> <pre><code>repo-magic-issues-2022-12-20-18-54-58.log\nrepo-magic-loaded-2022-10-06-15-40-07.log\nrepo-opentopography-issues-2022-10-05-22-04-03.log\nrepo-opentopography-loaded-2022-10-05-22-04-03.log</code></pre> <p>Note</p> <p>Headless repositories run after the not headless repositories. They also run serially, so it can take a long time to run headless. You can just run the headless as separate runs using the --source {SOURCE/REPO}</p> <pre><code>\ngleaner-runstats-2023-04-27-05-38-59.log</code></pre> <p>You can <code>tail -f logs/{file}</code></p>"},{"location":"data_loading/watching_manual_gleanerio_data_loading/#minioadmins","title":"Minioadmin/s","text":"<p>In minioadmin you can see a bucket loading. Go to a minioadmin https://minioadmin.geocodes.ncsa.illinois.edu/</p> <p>Go to the bucket you are loading, summoned path select a repo, Sort by date to see what is the latest loaded (click twice)</p>"},{"location":"data_loading/watching_manual_gleanerio_data_loading/#run-a-missing-report","title":"Run a missing report","text":"<p>If you run the missing_report you can do a quick idea of what did not make it in...  but there is still alot to go the there will be a lot of missing.</p>"},{"location":"data_loading/configuration/","title":"Configuration Examples","text":""},{"location":"data_loading/configuration/#basics","title":"Basics","text":"<p>At present, there are two similar but different configuration files that are used by the two core applications: <code>gleaner</code> and <code>nabu</code> These can be generated using a command line tool: <code>glcon</code> when generating using<code>glcon</code>, a file called <code>localConfig.yaml</code> is edited, and a command generate generates  the two configuraiton files.</p> <p>Plans for the future are to refactor into two files, core services and sources</p>"},{"location":"data_loading/configuration/#services-and-sources","title":"Services and Sources","text":"<p>To load data you need to know the services and the sources. The services can be a remote cloud based, or  local usually running in a container (warn local is not always easy.)</p>"},{"location":"data_loading/configuration/#using-glcon-to-generate-configurations","title":"Using glcon to generate configurations","text":"<p>Step overview:</p> <ul> <li><code>./glcon config init --cfgName {projectname}</code></li> <li>edit configs/projectname/localConfig.yaml</li> <li><code>./glcon config generate --cfgName {projectname}</code></li> </ul> <pre><code># NOTE: while you can, it's not always a good pattern to put a comment after a property: value\n#     property: value # comment\n# sometimes things do not go well\n---\nminio:\n  address: 0.0.0.0\n# aws need to include the region in the bucket. eg: s3.us-west-2.amazonaws.com\n  port: 9000\n  accessKey: worldsbestaccesskey\n  secretKey: worldsbestsecretkey\n  ssl: false\n  bucket: gleaner\n  # can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: http://localhost/blazegraph/namespace/earthcube/sparql\ns3:\n  bucket: gleaner\n  # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n  location: sources.csv\n# this can be a remote csv\n#  type: csv\n#  location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&amp;sheet={sheet_name}\n# TBD -- Just use the sources in the gleaner file.\n#  type: yaml\n#  location: gleaner.yaml\n</code></pre>"},{"location":"data_loading/configuration/#examples","title":"Examples","text":""},{"location":"data_loading/configuration/#demo","title":"Demo","text":"<p>This is configured as a local</p> <pre><code>---\nminio:\n  address: 0.0.0.0\n  port: 9000\n  accessKey: worldsbestaccesskey\n  secretKey: worldsbestsecretkey\n  ssl: false\n  bucket: gleaner # can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: http://localhost/blazegraph/namespace/earthcube/sparql\ns3:\n  bucket: gleaner # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n  location: sources.csv</code></pre>"},{"location":"data_loading/configuration/#flight-test","title":"Flight Test","text":"<p>This </p>"},{"location":"data_loading/configuration/demo/readme/","title":"Readme","text":"<p>Demonstration to run against samples.earth</p>"},{"location":"data_loading/configuration/demo/readme/#demoyaml-is-a-hand-generated-file","title":"demo.yaml is a hand generated file","text":"<p>run demo using gleaner <pre><code>gleaner -cfg configs/demo/demo</code></pre> Note: no .yaml extension  at the end of the file</p> <p>run demo using glcon <pre><code>glcon gleaner batch -cfgFile  configs/demo/demo</code></pre></p>"},{"location":"data_loading/configuration/demo/readme/#gleaner-configuration-directory-mode","title":"gleaner configuration directory mode","text":"<p>After running glcon config</p> <pre><code>glcon config generate  -cfgName  myDemo</code></pre> <p>gleaner: <pre><code>gleaner -cfg configs/demo/gleaner</code></pre> glcon: <pre><code>glcon gleaner batch -cfgName  demo</code></pre> Note: No filename needed. assumed to be gleaner</p>"},{"location":"data_loading/configuration/template/README_Configure_Template/","title":"Configure Using glcon and Templates","text":"<p>You do not need to have a container stack running to run    <code>glcon config</code> But run <code>glcon gleaner</code> and <code>glcon nabu</code>, you will need to. </p>"},{"location":"data_loading/configuration/template/README_Configure_Template/#overview-glcon-configuration-generation","title":"OVERVIEW glcon Configuration generation","text":"<p><code>glcon config</code> is used to create configuration files for gleaner and nabu The pattern is to intiialize a configuration directory, edit files, and generate new  configuration files for gleaner and nabu. Inside a configuration, you will need to edit a localConfiguration file Edit/add sources in a csv listing, and generate the configurations.</p>"},{"location":"data_loading/configuration/template/README_Configure_Template/#initialize-a-configuration-directory","title":"initialize a configuration directory","text":"<p>use  glcon command can intialize a configuration directory, and allow for the generation of gleaner and nabu configurations</p> <pre><code>glcon config init -cfgName test</code></pre> <p>initializes a configuration in configs with name of 'test' Inside you will find <pre><code>test % ls\ngleaner_base.yaml   readme.txt      sources.csv\nnabu_base.yaml      localConfig.yaml \nREADME_Configure_Template.md</code></pre></p>"},{"location":"data_loading/configuration/template/README_Configure_Template/#edit-the-files","title":"EDIT the files","text":"<p>Usually, you will only need to edit the localConfig.yaml and sources.csv The localConfig.yaml</p>"},{"location":"data_loading/configuration/template/README_Configure_Template/#localconfigyaml","title":"localConfig.yaml","text":"<pre><code>---\nminio:\n  address: 0.0.0.0 # can be overridden with MINIO_ADDRESS\n  port: 9000 # can be overridden with MINIO_PORT\n  accessKey: worldsbestaccesskey # can be overridden with MINIO_ACCESS_KEY\n  secretKey: worldsbestsecretkey # can be overridden with MINIO_SECRET_KEY\n  ssl: false # can be overridden with MINIO_SSL\n  bucket: gleaner # can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: http://localhost/blazegraph/namespace/earthcube/sparql\ns3:\n  bucket: gleaner # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n   type: csv\n   location: sources.csv</code></pre> <p>First, in the \"mino:\" section make sure the accessKey and secretKey here match the access keys for your minio. These can be overridden with the environent variables: * \"MINIO_ACCESS_KEY\" * \"MINIO_SECRET_KEY\"</p>"},{"location":"data_loading/configuration/template/README_Configure_Template/#sourcessource","title":"sourcesSource","text":"<p>sources.csv is utilized to generate the information needed to retrieve (summon), process (mill), upload (prefix), and cull old records (prune) This is done in order to facilitate the managing a list of sources, in a spreadsheet, rather than a yamil file. a csv file with the fields below</p> <p>This is designed to be edited in a spreadsheet, or exported by url as csv from a google spreadsheet</p> <pre><code>hack,SourceType,Active,Name,ProperName,URL,Headless,Domain,PID,Logo\n1,sitegraph,FALSE,aquadocs,AquaDocs,https://oih.aquadocs.org/aquadocs.json ,FALSE,https://aquadocs.org,http://hdl.handle.net/1834/41372,\n3,sitemap,TRUE,opentopography,OpenTopography,https://opentopography.org/sitemap.xml,FALSE,http://www.opentopography.org/,https://www.re3data.org/repository/r3d100010655,https://opentopography.org/sites/opentopography.org/files/ot_transp_logo_2.png\n,sitemap,TRUE,iris,IRIS,http://ds.iris.edu/files/sitemap.xml,FALSE,http://iris.edu,https://www.re3data.org/repository/r3d100010268,http://ds.iris.edu/static/img/layout/logos/iris_logo_shadow.png</code></pre> <p>Fields:  1. hack:a hack to make the fields are properly read. 2. SourceType : [sitemap, sitegraph, googledrive, api] type of source 3. Active: [TRUE,FALSE] is source active.  4. Name: short name of source. It should be one word (no space) and be lower case. 5. ProperName: Long name of source that will be added to organization record for provenance 6. URL: URL of sitemap or sitegraph. 7. Headless: [FALSE,TRUE] should be set to false unless you know this site uses JavaScript to place the JSON-LD into the page.  This is true of some sites and it is supported but not currently auto-detected.  So you will need to know this and set it.  For most place, this will be false.    if the json-ld is generated in a page dynamically, then use , TRUE 8. Domain:  9. PID: a unique identifier for the source. Perfered that is is a research id. 10. Logo: while no longer used, logo of the source 11. googleapikeyenv: (ONLY NEEDED FOR type:googledrive) environment variable pointing to a google api key. 12. any additional feilds you wish. This might be used to generate information about sources for a website.</p>"},{"location":"data_loading/configuration/template/README_Configure_Template/#configuration-of-your-source","title":"Configuration of your source","text":"<p>You configure the source in the localConfig.yaml (or override via the command line) <pre><code># looks for Mysources.csv in configuration directory\nsourcesSource:\n   type: csv\n   location: Mysources.csv</code></pre> This can also be a remote url starting with http:// or https:// <pre><code># pulls from a google sheet\nsourcesSource:\n   type: csv\n   location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&amp;sheet={sheet_name}</code></pre></p> <p>If you start the name with a '/' a full path to the file is assumed. <pre><code># file outside of local configuration directory\nsourcesSource:\n   type: csv\n   location: /home/user/ourSources.csv</code></pre></p>"},{"location":"data_loading/configuration/template/README_Configure_Template/#override-sources-via-the-cli","title":"Override Sources via the CLI","text":"<p>pass --sourcemaps to generate:</p> <p><code>glcon config generate --cfgName test --sourcemaps \"My Sources.csv\"</code></p>"},{"location":"data_loading/configuration/template/README_Configure_Template/#generate-the-configuration-files","title":"GENERATE the configuration files","text":"<p><pre><code>glcon generate -cfgName test</code></pre> This will generate files  'gleaner' and  'nabu' and make copies of the existing configuration files</p>"},{"location":"data_loading/configuration/template/README_Configure_Template/#some-gleaner-configuration-details","title":"Some Gleaner Configuration details","text":"<p>This is a summary of a few portions of the configuration files generated. More details are at: GleanerConfiguration.md </p> <p>Open the file 'gleaner', and you will see  is actually quite a bit information this file, but for this starting demo only a few things we need to worry about.  The default file will look like:</p> <pre><code>---\nminio:\n  address: 0.0.0.0\n  port: 9000\n  accessKey: worldsbestaccesskey\n  secretKey: worldsbestsecretkey\n  ssl: false\n  bucket: gleaner\ngleaner:\n  runid: runX # this will be the bucket the output is placed in...\n  summon: true # do we want to visit the web sites and pull down the files\n  mill: true\ncontext:\n  cache: true\ncontextmaps:\n  - prefix: \"https://schema.org/\"\n    file: \"./configs/schemaorg-current-https.jsonld\"\n  - prefix: \"http://schema.org/\"\n    file: \"./configs/schemaorg-current-https.jsonld\"\nsummoner:\n  after: \"\"      # \"21 May 20 10:00 UTC\"   \n  mode: full  # full || diff:  If diff compare what we have currently in gleaner to sitemap, get only new, delete missing\n  threads: 5\n  delay:  # milliseconds (1000 = 1 second) to delay between calls (will FORCE threads to 1) \n  headless: http://127.0.0.1:9222  # URL for headless see docs/headless\nmillers:\n  graph: true\n# will be built from sources.csv\nsources:\n  - sourcetype: sitegraph\n    name: aquadocs\n    logo: \"\"\n    url: https://oih.aquadocs.org/aquadocs.json\n    headless: false\n    pid: http://hdl.handle.net/1834/41372\n    propername: AquaDocs\n    domain: https://aquadocs.org\n    active: false\n  - sourcetype: sitemap\n    name: opentopography\n    logo: https://opentopography.org/sites/opentopography.org/files/ot_transp_logo_2.png\n    url: https://opentopography.org/sitemap.xml\n    headless: false\n    pid: https://www.re3data.org/repository/r3d100010655\n    propername: OpenTopography\n    domain: http://www.opentopography.org/\n    active: false</code></pre> <p>A few things we need to look at.</p> <p>First, in the \"mino:\" section make sure the accessKey and secretKey here are the ones you utlize. Note: blank these out, and used environment variables (TODO:Need to describe them)</p> <p>Next, lets look at the \"gleaner:\" section.  We can set the runid to something.  This is the ID for a run and it allows you to later make different runs and keep the resulting graphs organized.  It can be set to any lower case string with no spaces. </p> <p>The miller and summon sections are true and we will leave them that way.  It means we want Gleaner to both fetch the resources and process (mill) them.  </p> <p>Now look at the \"miller:\"  section when lets of pick what milling to do.   Currently it is set with only graph set to true.  Let's leave it that way for now.  This means Gleaner will only attempt to make graph and not also run validation or generate prov reports for the process.  </p> <p>The final section we need to look at is the \"sources:\" section.  Here is where the fun is.  While there are two types, sitegraph and sitemaps we will normally use sitemap type.  There is a third type that involves configuring and pulling from a</p> <p>A standard sitemap is below: <pre><code>sources:\n  - sourcetype: sitemap\n      name: opentopography\n      logo: https://opentopography.org/sites/opentopography.org/files/ot_transp_logo_2.png\n      url: https://opentopography.org/sitemap.xml\n      headless: false\n      pid: https://www.re3data.org/repository/r3d100010655\n      propername: OpenTopography\n      domain: http://www.opentopography.org/\n      active: true</code></pre></p> <p>A sitegraph  <pre><code>sources:\n  - sourcetype: sitegraph\n    name: aquadocs\n    logo: \"\"\n    url: https://oih.aquadocs.org/aquadocs.json\n    headless: false\n    pid: http://hdl.handle.net/1834/41372\n    propername: AquaDocs\n    domain: https://aquadocs.org\n    active: false</code></pre> A google drive <pre><code>sources:\n   - sourcetype: googledrive\n     name: ecrr_submitted\n     logo: https://www.earthcube.org/sites/default/files/doc-repository/logo_earthcube_full_horizontal.png\n     url: https://drive.google.com/drive/u/0/folders/1TacUQqjpBbGsPQ8JPps47lBXMQsNBRnd\n     headless: false\n     pid: \"\"\n     propername: Earthcube Resource Registry\n     domain: http://www.earthcube.org/resourceregistry/\n     active: true\n     GoogleServiceJsonEnv: GOOGLEAPIAUTH\n     # see below. Enviroment variable name that contains filename of service_account.json authentication</code></pre> These are the sources we wish to pull and process.  Each source has a type, and 8 entries though at this time we no longer use the \"logo\" value.  It was used in the past to provide a page showing all the sources and  a logo for them.  However, that's really just out of scope for what we want to do.  You can leave it blank or set it to any value, it wont make a difference.  </p> <p>The name is what you want to call this source.  It should be one word (no space) and be lower case. </p> <p>The url value needs to point to the URL for the site map XML file.  This will be created and served by the data provider. </p> <p>The headless value should be set to false unless you know this site uses JavaScript to place the JSON-LD into the page.  This is true of some sites and it is supported but not currently auto-detected.  So you will need to know this and set it.  For most place, this will be false. </p> <p>You can have as many sources as you wish.  For an example look the configure file for the CDF Semantic Network at: https://github.com/gleanerio/CDFSemanticNetwork/blob/master/configs/cdf.yaml</p>"},{"location":"data_loading/configuration/template/README_Configure_Template/#google-drive","title":"Google Drive","text":"<p>The key GoogleServiceJson Env points to a env variable name that contains the path to service_account.json file <code>setenv GOOGLEAPIAUTH \"configs/credentials/gleaner-331805-030e15e1d9c4.json\"</code></p> <p>Create service credentials https://developers.google.com/workspace/guides/create-credentials</p> <p>save in credentials/{filename}</p> <p>The URL will be the folder you see in google drive.</p> <p>A google drive <pre><code>sources:\n   - sourcetype: googledrive\n     name: ecrr_submitted\n     logo: https://www.earthcube.org/sites/default/files/doc-repository/logo_earthcube_full_horizontal.png\n     url: https://drive.google.com/drive/u/0/folders/1TacUQqjpBbGsPQ8JPps47lBXMQsNBRnd\n     headless: false\n     pid: \"\"\n     propername: Earthcube Resource Registry\n     domain: http://www.earthcube.org/resourceregistry/\n     active: true\n     GoogleServiceJsonEnv: GOOGLEAPIAUTH\n     # see below. Enviroment variable name that contains filename of service_account.json authentication</code></pre></p>"},{"location":"development/","title":"Developing local UI","text":""},{"location":"development/#user-interface","title":"User Interface","text":"<p>Presently the local development is used for the UI</p> <p>grab the  facetsearch repository</p> <p>run the server</p> <p>run the client</p> <p>We do development in Jetbrains Webstorm</p>"},{"location":"development/#full-stack","title":"full stack","text":"<p>see Local Stack</p>"},{"location":"development/local_stacks/","title":"Local or Developer Stacks","text":"<p>TODO GRAPH OF STACK... or say see index ;)</p> <p>Container Stacks: * services * geocodes</p>"},{"location":"development/local_stacks/#starting","title":"starting.","text":"<ul> <li>copy env.local.example to .env  <code>./run_local.sh</code></li> <li>or copy env.local.example to yourconfig.env</li> <li><code>./run_local.sh -e yourconfig.env</code></li> </ul> <p>Ports/Services see Stack Machines</p> <p>====</p>"},{"location":"development/local_stacks/#is-it-running","title":"is it Running?","text":"<p>traefik admin</p> <p>Graph: http://localhost:9999/blazegraph</p>"},{"location":"development/localdeveloper_configs/","title":"Production configuration settings.","text":"<p>This is just a list of the customized sections of the 'production' configurations You should be able to 'glean' information needed about what servers and sources are being utilized.</p> <p>Note</p> <p>You will need to customize these for each server.</p> service config servers source production geocodes geocodes-1 production from  sources geocodes-dev geocodes_all geocodes-dev sources from sources sheet beta geocodes_all geocodes-dev sources from sources sheet alpha geocodes_all geocodes-1 sources from sources sheet wifire wifire geocodes-dev wifire from sources sheet ** BETA AND ALPHA NEED TO BE UPDATED  to the latest tenant with updated configs and config/facet_search_{project} ** service servers notes production geocodes-1 Runs vetted Data geocodes-dev geocodes-dev All sources beta geocodes-dev config/facets_serarch_beta point at geocodes-dev services? alpha geocodes-1 config/facets_serarch_alpha pointed at geocodes-1 services wifire gecodes-dev tenant <p>** Alpha and Beta ** are user interface testing clients, so while tenants, they are using  the data sources for production and gecodes-dev (all sources). These can be changed as needed.</p> <p>Production service naming logic</p> <p>In order to better handle the ability to point the 'client' at different endpoints and services The new pattern is that the main server has a basename  that is not only 'geocodes' 'geocodes-1' is the production service and it's services are affixed with geocodes-1.earthcube.org</p>"},{"location":"development/localdeveloper_configs/#production-geocodesearthcubeorg","title":"Production geocodes.earthcube.org","text":"<p>docker config: geocodes-1 configs/onfigs/facet_search</p> <p>Production is a subset of the sources that have been vetted.</p> <p>In order to better handle the ability to point the 'client' at different endpoints and services The new pattern is that the main server has a basename  that is not only 'geocodes' 'geocodes-1' is the production service and it's services are affixed with geocodes-1.earthcube.org</p> <p>environment <pre><code>HOST=geocodes-1.earthcube.org\nGLEANER_PORTAINER_DOMAIN=portainer.geocodes-1.earthcube.org\nGLEANER_ADMIN_DOMAIN=admin.geocodes-1.earthcube.org\nGLEANER_OSS_DOMAIN=oss.geocodes-1.earthcube.org\nGLEANER_OSS_CONSOLE_DOMAIN=minioadmin.geocodes-1.earthcube.org\nGLEANER_GRAPH_DOMAIN=graph.geocodes-1.earthcube.org\nGLEANER_WEB_DOMAIN=web.geocodes-1.earthcube.org\nGLEANER_SPARQLGUI_DOMAIN=sparqlui.geocodes.earthcube.org\nGLEANER_GRAPH2_DOMAIN=graph2.geocodes-1.earthcube.org\nGC_CLIENT_DOMAIN=geocodes.earthcube.org\nGEODEX_BASE_DOMAIN=geodex.org\nMINIO_ROOT_ACCESS_KEY={snip}\nMINIO_ROOT_SECRET_KEY={snip}\nMINIO_SERVICE_ACCESS_KEY=gleaner\nMINIO_SERVICE_SECRET_KEY=addtoearthcube\nFUSEKI_ADMIN_PASSWORD=earthcubeAdmin1!\nGLEANER_TRAEFIK_YML=traefik_data\nGLEANER_TRAEFIK=traefik_data\nGLEANER_OBJECTS=minio\nGLEANER_GRAPH=graph\nSPARQL_DEFAULT_SPARQL_ENDPOINT_PATH=/blazegraph/namespace/earthcube/sparql\nTRAEFIK_AUTH={snip}\nGC_GITHUB_SECRET={snip}\nGC_GITHUB_CLIENTID={snip}\nGC_NB_AUTH_MODE=service\nS3ADDRESS=oss.geocodes-1.earthcube.org\nS3KEY=gleaner\nS3SECRET=adtoearthcube\nS3SSL=true\nS3PORT=443\nBUCKET=gleaner\nBUCKETPATH=summoned\nPATHTEMPLATE='${bucketpath}/${reponame}/${sha}.jsonld'\nTOOLTEMPLATE='${bucketpath}/${reponame}/${ref}.json'\nTOOLBUCKET=ecrr\nTOOLPATH=summoned</code></pre></p> <p>localConfig.yaml <pre><code>---\nminio:\n  address: oss.geocodes-1.earthcube.org\n  port: 443\n  accessKey: {snip}\n  secretKey: {snip}\n  ssl: true\n  bucket: gleaner # can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: https://graph.geocodes-1.earthcube.org/blazegraph/namespace/earthcube/sparql\ns3:\n  bucket: gleaner # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n#  location: sources.csv\n# this can be a remote csv\n#  type: csv\n  location: https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=TestSources202210\n#  location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&amp;sheet={sheet_name}\n# TBD -- Just use the sources in the gleaner file.\n#  type: yaml\n#  location: gleaner.yaml</code></pre></p> <p>config/facets_search <pre><code>---\n#API_URL: http://localhost:3000\nAPI_URL: https://geocodes.earthcube.org/ec/api\nTRIPLESTORE_URL: https://graph.geocodes-1.earthcube.org/blazegraph/namespace/earthcube/sparql\nSUMMARYSTORE_URL: https://graph.geocodes-1.earthcube.org/blazegraph/namespace/summary2/sparql\n#SUMMARYSTORE_URL: https://graph.geodex.org/blazegraph/namespace/summary/sparql\nECRR_TRIPLESTORE_URL: http://132.249.238.169:8080/fuseki/ecrr/query\nECRR_GRAPH: http://earthcube.org/gleaner-summoned\nTHROUGHPUTDB_URL: https://throughputdb.com/api/ccdrs/annotations\nSPARQL_QUERY: queries/sparql_query.txt\nSPARQL_HASTOOLS: queries/sparql_hastools.txt\nSPARQL_TOOLS_WEBSERVICE: queries/sparql_gettools_webservice.txt\nSPARQL_TOOLS_DOWNLOAD: queries/sparql_gettools_download.txt\nJSONLD_PROXY: https://geocodes.geocodes-1.earthcube.org/ec/api/${o}\nSPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\nSPARQL_YASGUI: https://sparqlui.geocodes.earthcube.org/?</code></pre></p>"},{"location":"development/localdeveloper_configs/#-","title":"----------","text":""},{"location":"development/localdeveloper_configs/#geocodes-devstaging-geocodesgeocodes-devearthcubeorg","title":"Geocodes-dev/staging  geocodes.geocodes-dev.earthcube.org","text":"<p>docker config: geocodes-dev configs/facet_search</p> <p>This would be a list of all sources and sitemaps.</p> <p>environment <pre><code>HOST=geocodes-1.earthcube.org\nFACET_SERVICES_FILE=./config/services.js\nGC_CLIENT_DOMAIN=alpha.geocodes.earthcube.org\nMINIO_ROOT_ACCESS_KEY={snip}\nMINIO_ROOT_SECRET_KEY={snip}\nMINIO_SERVICE_ACCESS_KEY={snip}\nMINIO_SERVICE_SECRET_KEY={snip}\nSPARQL_DEFAULT_SPARQL_ENDPOINT_PATH=/blazegraph/namespace/earthcube/sparql\nS3ADDRESS=oss.geocodes-1.earthcube.org\nS3KEY=worldsbestaccesskey\nS3SECRET=worldsbestsecretkey\nS3SSL=true\nS3PORT=443\nBUCKET=gleaner\nBUCKETPATH=summoned\nPATHTEMPLATE={{bucketpath}}/{{reponame}}/{{sha}}.jsonld\nTOOLTEMPLATE={{bucketpath}}/{{reponame}}/{{ref}}.json\nTOOLBUCKET=ecrr\nTOOLPATH=summoned\nGC_GITHUB_SECRET={snip}\nGC_GITHUB_CLIENTID={snip}\nGC_NB_AUTH_MODE=service\nGC_BASE=gcalpha</code></pre></p> <p>localConfig.yaml <pre><code>---\nminio:\n  address: oss.geocodes-dev.earthcube.org\n  port: 443\n  accessKey: {snip}\n  secretKey: {snip}\n  ssl: true\n  bucket: gleaner # can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/sparql\ns3:\n  bucket: gleaner # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n  #  location: sources.csv\n  location: https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=sources\n# this can be a remote csv\n#  type: csv\n#  location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&amp;sheet={sheet_name}\n# TBD -- Just use the sources in the gleaner file.\n#  type: yaml\n#  location: gleaner.yaml\n</code></pre></p> <p>facets_search.yaml <pre><code>---\n#API_URL: http://localhost:3000\nAPI_URL: https://geocodes.geocodes-dev.earthcube.org/ec/api\nTRIPLESTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/sparql\nSUMMARYSTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/summary/sparql\nECRR_TRIPLESTORE_URL: http://132.249.238.169:8080/fuseki/ecrr/query\nECRR_GRAPH: http://earthcube.org/gleaner-summoned\nTHROUGHPUTDB_URL: https://throughputdb.com/api/ccdrs/annotations\nSPARQL_QUERY: queries/sparql_query.txt\nSPARQL_HASTOOLS: queries/sparql_hastools.txt\nSPARQL_TOOLS_WEBSERVICE: queries/sparql_gettools_webservice.txt\nSPARQL_TOOLS_DOWNLOAD: queries/sparql_gettools_download.txt\nJSONLD_PROXY: https://geocodes.geocodes-dev.earthcube.org/ec/api/${o}\n# oauth issues. need to add another auth app for additional 'proxies'\nSPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\nSPARQL_YASGUI: https://sparqlui.geocodes-dev.earthcube.org/?</code></pre></p>"},{"location":"development/localdeveloper_configs/#wifire","title":"wifire","text":"<p>docker config: geocodes-dev   configs/wifire</p> <p>environment <pre><code>HOST=geocodes-dev.earthcube,org\nFACET_SERVICES_FILE=./config/services.js\nGC_CLIENT_DOMAIN=geocodes.wifire-data.sdsc.edu\nMINIO_ROOT_ACCESS_KEY={snip}\nMINIO_ROOT_SECRET_KEY={snip}\nMINIO_SERVICE_ACCESS_KEY={snip}\nMINIO_SERVICE_SECRET_KEY={snip}\nSPARQL_DEFAULT_SPARQL_ENDPOINT_PATH=/blazegraph/namespace/wifire/sparql\nS3ADDRESS=oss.geocodes-dev.earthcube.org\nS3KEY={snip}\nS3SECRET={snip}\nS3SSL=true\nS3PORT=443\nBUCKET=wifire\nBUCKETPATH=summoned\nPATHTEMPLATE={{bucketpath}}/{{reponame}}/{{sha}}.jsonld\nTOOLTEMPLATE={{bucketpath}}/{reponame}}/{{ref}}.json\nTOOLBUCKET=ecrr\nTOOLPATH=summoned\nGC_GITHUB_SECRET=OAUTH SECRET\nGC_GITHUB_CLIENTID=OAUTH APP ID\nGC_NB_AUTH_MODE=service\nGC_BASE=wifire</code></pre></p> <p>localConfig.yaml <pre><code>---\nminio:\n  address: oss.geocodes-dev.earthcube.org\n  port: 443\n  accessKey: {snip}\n  secretKey: {snip}\n  ssl: true\n  bucket: wifire # can be overridden with MINIO_BUCKET\nsparql:\n#  endpoint: http://localhost/blazegraph/namespace/wifire/sparql\n  endpoint: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/wifire/sparql\ns3:\n  bucket: wifire # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n  location:  https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=wifire\n# this can be a remote csv\n#  type: csv\n#  location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&amp;sheet={sheet_name}\n# TBD -- Just use the sources in the gleaner file.\n#  type: yaml\n#  location: gleaner.yaml\n</code></pre></p> <p>config/facets_config_wifire  <pre><code>---\n#API_URL: http://localhost:3000\nAPI_URL: https://geocodes.wifire-data.sdsc.edu/ec/api\n#TRIPLESTORE_URL: https://graph.geodex.org/blazegraph/namespace/earthcube/sparql\nTRIPLESTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/wifire/sparql\nSUMMARYSTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/wifire_summary/sparql\n#SUMMARYSTORE_URL: https://graph.geodex.org/blazegraph/namespace/summary/sparql\nECRR_TRIPLESTORE_URL: http://132.249.238.169:8080/fuseki/ecrr/query\nECRR_GRAPH: http://earthcube.org/gleaner-summoned\nTHROUGHPUTDB_URL: https://throughputdb.com/api/ccdrs/annotations\nSPARQL_QUERY: queries/sparql_query.txt\nSPARQL_HASTOOLS: queries/sparql_hastools.txt\nSPARQL_TOOLS_WEBSERVICE: queries/sparql_gettools_webservice.txt\nSPARQL_TOOLS_DOWNLOAD: queries/sparql_gettools_download.txt\nJSONLD_PROXY: \"${window.location.origin}/ec/api/${o}\"\n# oauth issues. need to add another auth app for additional 'proxies'\n# This is the one that will work: SPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\nSPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\n####\nSPARQL_YASGUI: https://sparqlui.geocodes-dev.earthcube.org/?</code></pre></p>"},{"location":"development/localdeveloper_configs/#alpha-needs-to-be-updated-alphageocodesearthcubeorg","title":"Alpha needs to be updated - alpha.geocodes.earthcube.org","text":"<p>This would be a list of all sources and sitemaps.</p> <p>environment <pre><code>HOST=geocodes-1.earthcube.org\nFACET_SERVICES_FILE=./config/services.js\nGC_CLIENT_DOMAIN=alpha.geocodes.earthcube.org\nMINIO_ROOT_ACCESS_KEY={snip}\nMINIO_ROOT_SECRET_KEY={snip}\nMINIO_SERVICE_ACCESS_KEY={snip}\nMINIO_SERVICE_SECRET_KEY={snip}\nSPARQL_DEFAULT_SPARQL_ENDPOINT_PATH=/blazegraph/namespace/earthcube/sparql\nS3ADDRESS=oss.geocodes-1.earthcube.org\nS3KEY=worldsbestaccesskey\nS3SECRET=worldsbestsecretkey\nS3SSL=true\nS3PORT=443\nBUCKET=gleaner\nBUCKETPATH=summoned\nPATHTEMPLATE={{bucketpath}}/{{reponame}}/{{sha}}.jsonld\nTOOLTEMPLATE={{bucketpath}}/{{reponame}}/{{ref}}.json\nTOOLBUCKET=ecrr\nTOOLPATH=summoned\nGC_GITHUB_SECRET={snip}\nGC_GITHUB_CLIENTID={snip}\nGC_NB_AUTH_MODE=service\nGC_BASE=gcalpha</code></pre></p> <p>localConfig.yaml <pre><code>---\nminio:\n  address: oss.geocodes-1.earthcube.org\n  port: 443\n  accessKey: {snip}\n  secretKey: {snip}\n  ssl: true\n  bucket: gleaner # can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: https://graph.geocodes-1.earthcube.org/blazegraph/namespace/earthcube/sparql\ns3:\n  bucket: gleaner # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n#  location: sources.csv\n# this can be a remote csv\n#  type: csv\n  location: https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=sources\n#  location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&amp;sheet={sheet_name}\n# TBD -- Just use the sources in the gleaner file.\n#  type: yaml\n#  location: gleaner.yaml\n</code></pre></p> <p>facets_search.yaml <pre><code></code></pre></p>"},{"location":"ecrr/","title":"notes on ECRR","text":"<p>Original ECCR Google Drive  * Resource Registry Project  * ECRR Files</p>"},{"location":"ecrr/#rlcone-to-copy-from-google-drive-to-s3","title":"Rlcone to copy from google drive to s3.","text":"<ul> <li>install rclone</li> </ul> <p><code>rclone config</code></p> <p>add a ecrr_gdrive</p> <p>make a link to the ECRR in your google drive.</p> <p>configure advanced to all for access to shared</p> <p>rclone ls ecrr_gdrive:RegistryProject2019/ECRR_Resources --max-depth 1</p>"},{"location":"production/","title":"Production","text":"Layout of Geocodes Stacks and Containers <pre><code>flowchart TB\n    subgraph Base Machine Stacks\n      subgraph base\n         traefik[traefik routing]\n         portainer[portainer container admin]\n      end\n      subgraph services\n         oss[\"oss s3\"]\n         sparqlgui\n         triplestore[\"graph -- triplestore\"]\n      end\n      subgraph geocodes\n         facetsearch--&gt;facetsearchservices\n      end\n      subgraph gleaner\n         headless\n      end\n  end</code></pre>"},{"location":"production/#pages","title":"Pages:","text":"<ul> <li>Prodcution UI DNS/Server changes</li> <li>Creating Configurations for Production<ul> <li>Production Configuration Fragments</li> </ul> </li> <li>Managing Services</li> <li>Testing/Onboarding a Datasource</li> <li>Maanging Geocodes UI Containers</li> <li>Some Sparql queries</li> <li>Notebook Proxy Container</li> </ul>"},{"location":"production/#machines","title":"Machines:","text":"<ul> <li>geodex.org</li> <li>geocodes.earthcube.org</li> </ul>"},{"location":"production/#builds","title":"Builds:","text":"<ul> <li>Containers<ul> <li>dockerhub nsfearthcube</li> </ul> </li> <li>actions</li> <li>github</li> </ul>"},{"location":"production/#troubleshooting","title":"Troubleshooting","text":""},{"location":"production/#i-refreshed-and-i-get-a-404","title":"I refreshed and i get a 404:","text":"<ol> <li>See if container is running in portainer</li> <li>Look at the portainer container log</li> <li>See if the treafik is mangled.</li> </ol> <p><code>https://admin.geodex.org/dashboard/#/</code>    (admin:password)</p>"},{"location":"production/creatingAndLoadingProduction/","title":"Production Configuration","text":"<p>setting up the 'production' config files This example is using geocodes-dev.earthcube.org Should this be an actual production configuration, the names need to be changed to protect the innocent</p> <p>Fragments from production configs for cribbing are in production_configs.md:</p>"},{"location":"production/creatingAndLoadingProduction/#overview","title":"Overview","text":"<ol> <li>Setup Datastores</li> <li>install glcon</li> <li>create a new configuration directory</li> <li>edit the local config for the configuration</li> <li>Generate the configuration files for gleaner and nabu</li> <li>setup minio using glcon gleaner setup</li> <li>start a screen (adds ability to run long running processes)</li> <li>run <code>gleaner batch</code> ||  <code>glcon gleaner batch</code></li> <li>run <code>nabu prefix</code> ||  <code>glcon nabu prefix</code></li> <li>there needs to be a note above can be replaced with  <code>glcon nabu release --cfgName CONFIG</code>, and how to upload the quads</li> </ol>"},{"location":"production/creatingAndLoadingProduction/#reconfiguration","title":"Reconfiguration","text":"<ol> <li>Changes to the gleaner/nabu configuration or the sources spreadsheet, other other items<ol> <li>make a change</li> <li>regenerate configs  <code>glcon config generate</code></li> <li>run <code>gleaner batch</code> ||  <code>glcon gleaner batch</code></li> </ol> </li> <li>run <code>nabu prefix</code> ||  <code>glcon nabu prefix</code></li> <li>run <code>nabu prune</code> ||  <code>glcon nabu prune</code><ol> <li>if files were removed from a repo, then this should prune them.</li> </ol> </li> </ol> <p>regenerate</p> <p>if you edit localConfig.yaml, you need to regenerate the configs using <code>./glcon config generate --cfgName geocodes</code></p>"},{"location":"production/creatingAndLoadingProduction/#setup-datastores","title":"Setup Datastores","text":"<p>If you followed the (setup indexing)[../setup_indexing_with_gleanerio.md] steps, you should have the datastores needed already created.  The production presently uses the earthcube repository convention, and that is what this document will use</p> Repository config s3 Bucket graph namespaces notes GeocodesTest gctest gctest gctest, gctest_summary samples of actual datasets geocodes geocodes geocodes geocodes, geocodes_summary suggested standalone repository earthcube geocodes gleaner earthcube, summary DEFAULT PRODUCTION NAME A COMMUNITY eg {acomm} {acomm} {acomm}, {acomm}_summary A communities tenant repository"},{"location":"production/creatingAndLoadingProduction/#install-glcon","title":"Install glcon","text":"<pre><code>cd \nls indexing</code></pre> <p>If glcon does not exist Install glcon</p>"},{"location":"production/creatingAndLoadingProduction/#create-a-new-configuration-directory","title":"create a new configuration directory","text":"<pre><code>cd indexing\n./glcon config init --cfgName geocodes</code></pre> <p>See that it is created.</p> <p><code>ls configs/</code></p> <p><code>ls configs/geocodes</code></p> <p>note there a only a few files.</p>"},{"location":"production/creatingAndLoadingProduction/#edit-the-local-config-for-the-configuration","title":"Edit the local config for the configuration","text":"<p>You will need to change the localConfig.yaml</p> <code>nano configs/ci/localConfig.yaml</code> <pre><code>---\nminio:\n  address: oss.{HOST}\n  port: 433\n  accessKey: worldsbestaccesskey\n  secretKey: worldsbestaccesskey\n  ssl: true\n  bucket: gleaner\n  # bucket: can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: https://graph.{HOST}/blazegraph/namespace/earthcube/sparql\ns3:\n  bucket: gleaner\n  #  sync with above... bucket: can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n#  location: sources.csv \n# this is a remote csv, of the Vetted sources (though it's called Test)\n  location:  https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=TestSources202210\n# this has all sources\n#      location: https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=sources\n</code></pre> <p>regenerate</p> <p>if you edit localConfig.yaml, you need to regenerate the configs using <code>./glcon config generate --cfgName gctest</code></p> <p>values need to match your {myhost}.env file</p>"},{"location":"production/creatingAndLoadingProduction/#generate-the-configuration-files-for-gleaner-and-nabu","title":"Generate the configuration files for gleaner and nabu","text":"<code>./glcon config generate --cfgName geocodes</code> <pre><code>./glcon config generate --cfgName geocodes\nINFO[0000] EarthCube Gleaner                            \ngenerate called\n{SourceType:sitemap Name:balto Logo:http://balto.opendap.org/opendap/docs/images/logo.png URL:http://balto.opendap.org/opendap/site_map.txt  Headless:false PID:http://balto.opendap.org ProperName:Balto Domain:http://balto.opendap.org Active:false CredentialsFile: Other:map[] HeadlessWait:0 Delay:0}\n{SourceType:sitemap Name:neotomadb Logo:https://www.neotomadb.org/images/site_graphics/Packrat.png URL:http://data.neotomadb.org/sitemap.xml Headless:true PID:http://www.re3data.org/repository/r3d100011761 ProperName:Neotoma Domain:http://www.neotomadb.org/ Active:false CredentialsFile: Other:map[] HeadlessWait:0 Delay:0}\n#{SNIP]\n{SourceType:sitemap Name:usap-dc Logo:https://www.usap-dc.org/ URL:https://www.usap-dc.org/view/dataset/sitemap.xml Headless:true PID:https://www.re3data.org/repository/r3d100010660 ProperName:U.S. Antarctic Program Data Center Domain:https://www.usap-dc.org/ Active:true CredentialsFile: Other:map[] HeadlessWait:0 Delay:0}\n{SourceType:sitemap Name:cchdo Logo:https://cchdo.ucsd.edu/static/svg/logo_cchdo.svg URL:https://cchdo.ucsd.edu/sitemap.xml Headless:false PID:https://www.re3data.org/repository/r3d100010831 ProperName:CLIVAR and Carbon Hydrographic Data Office Domain:https://cchdo.ucsd.edu/ Active:true CredentialsFile: Other:map[] HeadlessWait:0 Delay:0}\n{SourceType:sitemap Name:amgeo Logo:https://amgeo.colorado.edu/static/img/amgeosmall.svg URL:https://amgeo-dev.colorado.edu/sitemap.xml Headless:false PID: ProperName:Assimilative Mapping of Geospace Observations Domain:https://amgeo.colorado.edu/ Active:true CredentialsFile: Other:map[] HeadlessWait:0 Delay:0}\nmake copy of servers.yaml\nRegnerate gleaner\nRegnerate nabu</code></pre> <p>Check: <code>ls configs/geocodes</code></p> <p>Now there will be at least a 'gleaner', a 'nabu' and a 'nabu_prov' files.</p>"},{"location":"production/creatingAndLoadingProduction/#setup-minio","title":"setup minio","text":"<code>./glcon gleaner setup --cfgName geocodes</code> <p>This only needs to be done once <pre><code>ubuntu@geocodes-dev:~/indexing$ ./glcon gleaner setup --cfgName geocodes\nINFO[0000] EarthCube Gleaner                            \nUsing gleaner config file: /home/ubuntu/indexing/configs/geocodes/gleaner\nUsing nabu config file: /home/ubuntu/indexing/configs/geocodes/nabu\nsetup called\n{\"file\":\"/github/workspace/pkg/gleaner.go:60\",\"func\":\"github.com/gleanerio/gleaner/pkg.Setup\",\"level\":\"info\",\"msg\":\"Validating access to object store\",\"time\":\"2022-07-28T17:32:51Z\"}\n{\"file\":\"/github/workspace/pkg/gleaner.go:67\",\"func\":\"github.com/gleanerio/gleaner/pkg.Setup\",\"level\":\"info\",\"msg\":\"Setting up buckets\",\"time\":\"2022-07-28T17:32:51Z\"}\n{\"file\":\"/github/workspace/pkg/gleaner.go:78\",\"func\":\"github.com/gleanerio/gleaner/pkg.Setup\",\"level\":\"info\",\"msg\":\"Buckets generated.  Object store should be ready for runs\",\"time\":\"2022-07-28T17:32:51Z\"}</code></pre></p>"},{"location":"production/creatingAndLoadingProduction/#start-a-screen","title":"Start a screen","text":"<p>Since this is a long running process, it is suggested that this be done in <code>screen</code> There is also a possibility of using tmux, if a user has experience with it.</p>"},{"location":"production/creatingAndLoadingProduction/#notes","title":"Notes:","text":"<p>To create a 'named' screen named gleaner </p> <p><code>screen -S gleaner</code></p> <p>to detach from screen, use: <code>ctl-a-d</code> </p> <p>to find running screens: <code>screen -ls</code></p> <pre><code>There are screens on:\n    7187.gleaner    (07/28/22 17:43:48) (Detached)\n    6879.pts-3.geocodes-dev (07/28/22 17:33:25) (Detached)\n2 Sockets in /run/screen/S-ubuntu.</code></pre> <p>to attach to a screen  in this case you use the name </p> <p><code>screen -r gleaner</code></p> <p>or</p> <p><code>screen -r pts-3</code></p> <pre><code>screen -S gleaner\nubuntu@geocodes-dev:~/indexing$ screen -ls\nThere is a screen on:\n    7187.gleaner    (07/28/22 17:43:48)   (Attached)\n1 Socket in /run/screen/S-ubuntu.</code></pre>"},{"location":"production/creatingAndLoadingProduction/#run-gleaner-to-pull-jsonld-from-sitemaps","title":"Run Gleaner to pull JSONLD from sitemaps","text":"<p>Robots.txt</p> <p>OK TO IGNORE. you will need to ignore errors about robot.txt and sitemap.xml not being an index <pre><code>{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:204\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"error\",\"msg\":\"error getting robots.txt for https://www.earthcube.org/datasets/allgood:Robots.txt unavailable at https://www.earthcube.org/datasets/allgood/robots.txt\",\"time\":\"2023-01-30T20:45:53-06:00\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:66\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResourceURLs\",\"level\":\"error\",\"msg\":\"Error getting robots.txt for geocodes_demo_datasets, continuing without it.\",\"time\":\"2023-01-30T20:45:53-06:00\"}    </code></pre></p> <p>Access issues</p> <pre><code>{\u201cfile\u201d:\u201c/github/workspace/internal/organizations/org.go:87\",\u201cfunc\u201d:\u201cgithub.com/gleanerio/gleaner/internal/organizations.BuildGraph\u201d,\u201clevel\u201d:\u201cerror\u201d,\u201cmsg\u201d:\u201corgs/geocodes_demo_datasets.nqThe Access Key Id you provided does not exist in our records.\u201c,\u201dtime\u201d:\u201c2023-01-31T15:27:39-06:00\u201d}</code></pre> <ul> <li>Access Key password could be incorrect</li> <li>address may be incorrect. It is a hostname or TC/IP, and not a URL</li> <li>ssl may need to be true</li> <li>See setup issues</li> </ul> <p>** Are you running, like suggested in a screen**</p> <code>./glcon gleaner batch --cfgName geocodes</code> <pre><code>ubuntu@geocodes-dev:~/indexing$ ./glcon gleaner batch --cfgName geocodes\nINFO[0000] EarthCube Gleaner                            \nUsing gleaner config file: /home/ubuntu/indexing/configs/geocodes/gleaner\nUsing nabu config file: /home/ubuntu/indexing/configs/geocodes/nabu\nbatch called\n{\"file\":\"/github/workspace/internal/organizations/org.go:55\",\"func\":\"github.com/gleanerio/gleaner/internal/organizations.BuildGraph\",\"level\":\"info\",\"msg\":\"Building organization graph.\",\"time\":\"2022-07-28T17:34:18Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/sitegraph.go:40\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.GetGraph\",\"level\":\"info\",\"msg\":\"Processing sitegraph file (this can be slow with little feedback):https://oih.aquadocs.org/aquadocs.json\",\"time\":\"2022-07-28T17:34:18Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/sitegraph.go:41\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.GetGraph\",\"level\":\"info\",\"msg\":\"Downloading sitegraph file:https://oih.aquadocs.org/aquadocs.json\",\"time\":\"2022-07-28T17:34:18Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/sitegraph.go:53\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.GetGraph\",\"level\":\"info\",\"msg\":\"Sitegraph file downloaded. Uploading togleanerhttps://oih.aquadocs.org/aquadocs.json\",\"time\":\"2022-07-28T17:34:57Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/sitegraph.go:60\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.GetGraph\",\"level\":\"info\",\"msg\":\"Sitegraph file uploaded togleanerUploaded :https://oih.aquadocs.org/aquadocs.json\",\"time\":\"2022-07-28T17:34:59Z\"}\n\n[SNIP]</code></pre> <p>You can now detach,</p> <p><code>ctl-a-d</code></p> <p>watch the logs <code>tail -f  logs/gleaner{somedate pattern}log</code></p> <p>to attach to a screen  in this case you use the name</p> <p><code>screen -r gleaner</code></p>"},{"location":"production/creatingAndLoadingProduction/#run-nabu-prefix-to-upload-data-to-graph-store","title":"run nabu prefix  to upload data to graph store","text":"<p>when gleaner is complete</p> <p>IF detached,  attach to a screen  in this case you use the name <code>screen -r gleaner</code></p> <p><code>./glcon nabu prefix --cfgName geocodes</code></p>"},{"location":"production/creatingAndLoadingProduction/#run-nabu-prefix-to-prov-to-graph-store","title":"Run nabu prefix to prov to graph store","text":"<p>This uses a separate config, for now.</p> <p>IF detached,  attach to a screen  in this case you use the name <code>screen -r gleaner</code></p> <p><code>./glcon nabu prefix --cfg configs/geocodes/nabuprov</code></p> <p>Note</p> <p>The above can be replaced with  <code>glcon nabu release --cfgName CONFIG</code>,   Need Note on how to do this and how to upload the quads</p>"},{"location":"production/creatingAndLoadingProduction/#run-nabu-prune-to-cullremove-data-to-graph-store","title":"run nabu prune  to cull/remove data to graph store","text":"<p>when gleaner is complete</p> <p>IF detached,  attach to a screen  in this case you use the name <code>screen -r gleaner</code></p> <p><code>./glcon nabu prune --cfgName geocodes</code></p>"},{"location":"production/creatingAndLoadingProduction/#changes-that-will-be-needed-for-the-client-configuration","title":"Changes that will be needed for the client configuration","text":"<p>production model for post step 4</p> <p>Portions of deployment/facets/config.yaml that might be changed. This is for production. IF you completed the initial data load using gctest, then you can modify and rebuild the geecodes stack using Updating a GEOCODES CLIENT Configuration production configuration in Manging Geocodes UI containers</p> production section of deployment/facets/config.yaml <pre><code>API_URL: https://geocodes.{your host}/ec/api/\nSPARQL_NB: https:/geocodes.{your host}/notebook/mkQ?q=${q}\nSPARQL_YASGUI: https://geocodes.{your host}/sparqlgui?\n#API_URL: \"${window_location_origin}/ec/api\"\n#TRIPLESTORE_URL: https://graph.geocodes-1.earthcube.org/blazegraph/namespace/earthcube/sparql\nTRIPLESTORE_URL: https://graph.{your host}/blazegraph/namespace/earthcube/sparql\nBLAZEGRAPH_TIMEOUT: 20\n## ECRR need to use fuseki source, for now.\nECRR_TRIPLESTORE_URL: http://132.249.238.169:8080/fuseki/ecrr/query \n# ECRR_TRIPLESTORE_URL:   http://{your host}/blazegraph/namespace/ecrr/sparql \nECRR_GRAPH: http://earthcube.org/gleaner-summoned\nTHROUGHPUTDB_URL: https://throughputdb.com/api/ccdrs/annotations\nSPARQL_QUERY: queries/sparql_query.txt\nSPARQL_HASTOOLS: queries/sparql_hastools.txt\nSPARQL_TOOLS_WEBSERVICE: queries/sparql_gettools_webservice.txt\nSPARQL_TOOLS_DOWNLOAD: queries/sparql_gettools_download.txt\n# JSONLD_PROXY needs qoutes... since it has a $\nJSONLD_PROXY: \"https://geocodes.{your host}/ec/api/${o}\"\n\nSPARQL_YASGUI: https://sparqlui.{your host}/?</code></pre>"},{"location":"production/creatingAndLoadingProduction/#reconfiguration_1","title":"Reconfiguration","text":""},{"location":"production/creatingAndLoadingProduction/#changes-to-the-gleanernabu-configuration-or-the-sources-spreadsheet","title":"Changes to the gleaner/nabu configuration or the sources' spreadsheet","text":"<p>If you change the {config}/localConfig.yaml or you update the source google sheet, then you need to regenerate the config files</p> <p>regenerate</p> <p>if you edit localConfig.yaml, you need to regenerate the configs using <code>./glcon config generate --cfgName gctest</code></p>"},{"location":"production/creatingAndLoadingProduction/#regenerate-configs","title":"regenerate configs","text":"<p><code>./glcon config generate --cfgName geocodes</code></p>"},{"location":"production/creatingAndLoadingProduction/#run-batch","title":"run batch","text":"<p><code>./glcon gleaner batch --cfgName geocodes</code></p>"},{"location":"production/creatingAndLoadingProduction/#run-nabu-prefix","title":"run nabu prefix","text":"<p><code>./glcon nabu prefix --cfgName geocodes</code></p>"},{"location":"production/creatingAndLoadingProduction/#run-nabu-prune","title":"run nabu prune.","text":"<p><code>./glcon nabu prune --cfgName geocodes</code></p> <p>if files were removed from a repo, then this should prune them.</p>"},{"location":"production/fuseki/","title":"Fuseki","text":"<p>Fuseki service uses a CONFIG in portainer. * upload the earthcube.ttl as a config file * open the graph2 seervice, add the config to: /fuseki-base/configuration/earthcube.ttl</p> <p>https://graph2.geocodes-dev.earthcube.org/$/stats</p> <p>https://graph2.geocodes-dev.earthcube.org/$/metrics</p>"},{"location":"production/geocodes_notebook_proxy_notes/","title":"notebook Proxy","text":"<p>Env variables</p> <pre><code>AUTH_MODE=server\nGITHUB_SECRET={KEY}\nGITHUB_CLIENTID=b5a5494cf21096a99e37</code></pre> <p>Note: In portainer, you may need to go into the service to update the ENV keys, there.</p>"},{"location":"production/geocodes_notebook_proxy_notes/#github-authorization","title":"Github authorization","text":"<p>The app is setup to run in a service.</p> <p>The app is here: https://github.com/organizations/earthcube/settings/applications/1768280</p> <p>Authorization callback URL: https://geocodes.earthcube.org/notebook/auth</p>"},{"location":"production/geodex.org/","title":"Geodex.org","text":"<p>log into geodex.org</p> <p>notes: update the version on the on facetsearch for it to know you got the correct one. <code>cd code/geodex</code></p> <p><code>./refresh_containers.sh</code></p> <p><code>./restart_all.sh</code></p> <p>to find an issue</p> <p><code>cat restart_all.sh</code></p> <p>grab the command line</p> <p>change up -d to</p> <p>log {service2view}</p> <p>eg</p> <p><code>docker-compose --env-file env.beta -f headless-only.yml -f geocodes-compose.yaml  -f geodex-compose.yml   logs vue-services</code></p> <p>You canfigure out the running endpoints from the /config route.</p> <p>http://localhost:3000/config https://geocodes.earthcube.org/ec/api/config</p> <p>NOTES: Things borked... match to staging or dev config... be careful, dev config points to localhost, so do not blindly copy</p> <p>Inside the geocodes.yaml there is a machine path.. <pre><code>vue-services:\nimage: nsfearthcube/ec_facets_api_nodejs:latest\n#build: ./server\nrestart: unless-stopped\nenvironment:\n- NODE_ENV=production\n#- S3ADDRESS=s3system:9000\n- S3ADDRESS=oss.geocodes.earthcube.org\n- S3BUCKET=sites\n- S3PREFIX=alpha\n- DOMAIN=https://${HOST:?HOST environment varaible is required}/\n- S3KEY=${S3KEY}\n- S3SECRET=${S3SECRET}</code></pre></p>"},{"location":"production/managing_geocodes_ui_containers/","title":"Manging Geocodes UI containers","text":""},{"location":"production/managing_geocodes_ui_containers/#updating-a-geocodes-client-configuration","title":"Updating a GEOCODES CLIENT Configuration","text":"<p>You can modify the facets_config config, in order to do this, stop the stack, delete the config and recreate the config.</p> <ol> <li>go to portainer,</li> <li>select geocodes_geocodes, stop</li> <li>select config, facets_config, copy content, select delete</li> <li>create a new config with name 'facets_config', paste in content</li> <li>modify content, save</li> <li>restart stack</li> <li>update the service<ol> <li>services, geocodes_vue-client or geocodes_xxx_vue-client</li> <li>udate the service    *** NOTE: TRY A SECOND BROWSER... and/or Clear browser cache ****</li> <li>If that does not work, check to see in services if the correct container image is being pulled.</li> </ol> </li> <li>Then go to containers, geocodes_vue-client or geocodes_xxx_vue-client<ol> <li>remove container. It will rebuild if it is not stopped</li> </ol> </li> </ol>"},{"location":"production/managing_geocodes_ui_containers/#developers-testing-a-ui-branch-in-portainerdocker","title":"DEVELOPERS: Testing a UI Branch in Portainer/Docker","text":"<p>:memo: An Update May be needed. You can now deploy a tennant configuration, which many mean that geocodes repo changes may not be needed</p> <p>:memo: you should do local development before deployment testing</p> <p>To do this we will need to do two branches, one on the facet search, and one on the services stack geocodes. Or, you can disconnect your development services</p>"},{"location":"production/managing_geocodes_ui_containers/#facetsearch-repository-changes","title":"Facetsearch repository changes","text":"<ul> <li>create a branch<ul> <li>on that branch edit the github workflows/docker_xxx add your branch</li> </ul> </li> </ul> <pre><code>on:\n  push:\n    branches:\n    - master\n    - feat_summary</code></pre> <ul> <li>make changes and push</li> </ul>"},{"location":"production/managing_geocodes_ui_containers/#geocodes-repository-changes","title":"geocodes repository changes","text":"<ul> <li>create a branch</li> <li>modify   deployment/geocodes-compose.yaml</li> </ul> <pre><code>vue-services:\n  image: nsfearthcube/ec_facets_api_nodejs:{{BRANCH NAME}}</code></pre> <pre><code>vue-client:\n  image: nsfearthcube/ec_facets_client:{{BRANCH NAME}}</code></pre>"},{"location":"production/managing_geocodes_ui_containers/#deployment-in-in-portainer","title":"Deployment in in portainer","text":"<ul> <li>create a new stack</li> <li>under advanced configuration   ??? example \"stack deploy from a branch\"   </li> <li>save</li> <li>pull and deploy</li> </ul>"},{"location":"production/managing_geocodes_ui_containers/#troubleshooting","title":"Troubleshooting","text":""},{"location":"production/managing_geocodes_ui_containers/#seems-like-the-container-is-not-getting-updated","title":"seems like  the container is not getting updated","text":"<p>occasionally, a branch is being used for a stack. This will  be true of alpha/beta/tennant containers.</p> <ul> <li>open stack</li> <li>user Redeploy from Git: select advanced configuration</li> <li>change the branch information</li> </ul> stack deploy from a branch <p></p> <p>Occasionally, the latest will not be pulled, Seen  when I  change a branch,</p> <ul> <li>open services,</li> <li>select a service,</li> <li>go down to Change container image</li> <li>set to the appropriate container path.</li> </ul> stack container path  <p></p>"},{"location":"production/managing_services/","title":"notes on managing Services","text":"<ul> <li>minio</li> <li>blazegraph</li> </ul>"},{"location":"production/managing_services/#minio","title":"Minio","text":""},{"location":"production/managing_services/#what-servers-exist-in-the-minio-configuration","title":"what servers exist in the minio configuration","text":"<p><code>mc alias ls</code></p>"},{"location":"production/managing_services/#minio-add-configuration-for-a-server","title":"minio add configuration for a server","text":"<p>Note the single quotes around the password... some passwords are   not command line friendly <pre><code>set +o history\nmc config host add dev https://oss.geocodes.earthcube.org  {miniouser} '{miniopassword}'\n\nset -o history</code></pre></p>"},{"location":"production/managing_services/#test","title":"test","text":"<pre><code>mc ls dev</code></pre>"},{"location":"production/managing_services/#mino-sync-between-servers","title":"Mino sync between servers","text":"<pre><code>mc cp --recursive dev/ecrr/ gc1/ecrr/</code></pre>"},{"location":"production/managing_services/#copy-log-files-to-minio","title":"copy log files to minio","text":"<pre><code>cd indexing/logs\nmc cp  --recursive . dev/gleaner/logs/{date}\nmc share download --recursive dev/gleaner/logs/{date}\n</code></pre>"},{"location":"production/managing_services/#links-for-a-bucket","title":"links for a bucket:","text":"<p>(IGNORE: only works for top level of bucket... may need a better policy)</p> <pre><code>mc anonymous links --recursive dev/gleaner/summoned/amgeo</code></pre>"},{"location":"production/managing_services/#statistics-for-a-bucket","title":"statistics for a bucket","text":"<pre><code>mc ls dev/gleaner/summoned/{repo} --summarize --recursive\n\nmc ls dev/gleaner/summoned/amgeo --summarize --recursiv\n\n\nmc stat --recursive  dev/gleaner/summoned/{repo}</code></pre> <p>eg</p> <p>mc stat --recursive  dev/gleaner/summoned/amgeo</p>"},{"location":"production/managing_services/#remove-old-records","title":"Remove old records:","text":"<pre><code>mc rm dev/gleaner/results --recursive --older-than 365d00h00m00s\nmc rm dev/gleaner/summoned --recursive --older-than 365d00h00m00s\nmc rm dev/gleaner/milled --recursive --older-than 365d00h00m00s</code></pre>"},{"location":"production/managing_services/#blazegraph","title":"BLAZEGRAPH","text":""},{"location":"production/managing_services/#cleaning-up-the-journal","title":"CLEANING UP THE JOURNAL","text":"<p>Blazegraph (and fuseki) will grow as data is added, its a journaled file system so it's not  cleaned up.</p> <p>The steps  are originally from (Medium.com)[https://medium.com/@nvbach91/how-to-reclaim-disk-space-in-blazegraph-95a47575f8a8]</p> <p>(MIKE WILL INSERT INSTRUCTIONS HERE)</p>"},{"location":"production/managing_services/#deleting","title":"deleting:","text":"<p>https://www.w3.org/TR/sparql11-update/#clear <pre><code>`CLEAR ALL`\n## Clear a graph\n`CLEAR GRAPH earthcube:{iri}`</code></pre></p>"},{"location":"production/portainer_docker_swarm/","title":"Notes on setting up Portainer to connect to a docker swarm.","text":""},{"location":"production/portainer_docker_swarm/#notes","title":"Notes:","text":"<p>Docker Swarm wants port 9001. Not truely needed since it's the minioadminport, that can be routed via trafik</p> <p>Created an edge agent https://docs.portainer.io/start/agent/edge</p>"},{"location":"production/portainer_docker_swarm/#openstack","title":"Openstack","text":"<pre><code>&gt;docker info\nubuntu@geocodes-1:~$ docker info\nClient:\n Context:    default\n Debug Mode: false\n Plugins:\n  app: Docker App (Docker Inc., v0.9.1-beta3)\n  buildx: Docker Buildx (Docker Inc., v0.9.1-docker)\n  compose: Docker Compose (Docker Inc., v2.12.2)\n  scan: Docker Scan (Docker Inc., v0.21.0)\n\nServer:\n Containers: 38\n  Running: 12\n  Paused: 0\n  Stopped: 26\n Images: 47\n Server Version: 20.10.21\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\n Swarm: active\n  NodeID: ct9p7o7wrf5m1aq4njzh74ekh\n  Is Manager: true\n  ClusterID: j45b7si0xn5o1kmegv3zhza8l\n  Managers: 1\n  Nodes: 1\n  Default Address Pool: 10.0.0.0/8  \n  SubnetSize: 24\n  Data Path Port: 4789\n  Orchestration:\n   Task History Retention Limit: 5\n  Raft:\n   Snapshot Interval: 10000\n   Number of Old Snapshots to Retain: 0\n   Heartbeat Tick: 1\n   Election Tick: 10\n  Dispatcher:\n   Heartbeat Period: 5 seconds\n  CA Configuration:\n   Expiry Duration: 3 months\n   Force Rotate: 0\n  Autolock Managers: false\n  Root Rotation In Progress: false\n  Node Address: 10.128.57.9\n  Manager Addresses:\n   10.128.57.9:2377\n Runtimes: runc io.containerd.runc.v2 io.containerd.runtime.v1.linux\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 1c90a442489720eec95342e1789ee8a5e1b9536f\n runc version: v1.1.4-0-g5fd4c4d\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: default\n  cgroupns\n Kernel Version: 5.15.0-52-generic\n Operating System: Ubuntu 22.04.1 LTS\n OSType: linux\n Architecture: x86_64\n CPUs: 8\n Total Memory: 31.35GiB\n Name: geocodes-1\n ID: YNXV:NTF4:5APE:UF2U:DTAN:CLW3:QQII:4NWH:FAFP:WRCA:N5GG:I5Q3\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Registry: https://index.docker.io/v1/\n Labels:\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n Live Restore Enabled: false\n\n</code></pre>"},{"location":"production/production_configs/","title":"Production configuration settings.","text":"<p>This is just a list of the customized sections of the 'production' configurations You should be able to 'glean' information needed about what servers and sources are being utilized.</p> <p>Note</p> <p>You will need to customize these for each server.</p> service config servers source production geocodes geocodes-1 production from  sources geocodes-dev geocodes_all geocodes-dev sources from sources sheet beta geocodes_all geocodes-dev sources from sources sheet alpha geocodes_all geocodes-1 sources from sources sheet wifire wifire geocodes-dev wifire from sources sheet ** BETA AND ALPHA NEED TO BE UPDATED  to the latest tenant with updated configs and config/facet_search_{project} ** service servers notes production geocodes-1 Runs vetted Data geocodes-dev geocodes-dev All sources beta geocodes-dev config/facets_serarch_beta point at geocodes-dev services? alpha geocodes-1 config/facets_serarch_alpha pointed at geocodes-1 services wifire gecodes-dev tenant <p>** Alpha and Beta ** are user interface testing clients, so while tenants, they are using  the data sources for production and gecodes-dev (all sources). These can be changed as needed.</p> <p>Production service naming logic</p> <p>In order to better handle the ability to point the 'client' at different endpoints and services The new pattern is that the main server has a basename  that is not only 'geocodes' 'geocodes-1' is the production service and it's services are affixed with geocodes-1.earthcube.org</p>"},{"location":"production/production_configs/#production-geocodesearthcubeorg","title":"Production geocodes.earthcube.org","text":"<p>docker config: geocodes-1 configs/onfigs/facet_search</p> <p>Production is a subset of the sources that have been vetted.</p> <p>In order to better handle the ability to point the 'client' at different endpoints and services The new pattern is that the main server has a basename  that is not only 'geocodes' 'geocodes-1' is the production service and it's services are affixed with geocodes-1.earthcube.org</p> <p>environment <pre><code>HOST=geocodes-1.earthcube.org\nGLEANER_PORTAINER_DOMAIN=portainer.geocodes-1.earthcube.org\nGLEANER_ADMIN_DOMAIN=admin.geocodes-1.earthcube.org\nGLEANER_OSS_DOMAIN=oss.geocodes-1.earthcube.org\nGLEANER_OSS_CONSOLE_DOMAIN=minioadmin.geocodes-1.earthcube.org\nGLEANER_GRAPH_DOMAIN=graph.geocodes-1.earthcube.org\nGLEANER_WEB_DOMAIN=web.geocodes-1.earthcube.org\nGLEANER_SPARQLGUI_DOMAIN=sparqlui.geocodes.earthcube.org\nGLEANER_GRAPH2_DOMAIN=graph2.geocodes-1.earthcube.org\nGC_CLIENT_DOMAIN=geocodes.earthcube.org\nGEODEX_BASE_DOMAIN=geodex.org\nMINIO_ROOT_ACCESS_KEY={snip}\nMINIO_ROOT_SECRET_KEY={snip}\nMINIO_SERVICE_ACCESS_KEY=gleaner\nMINIO_SERVICE_SECRET_KEY=addtoearthcube\nFUSEKI_ADMIN_PASSWORD=earthcubeAdmin1!\nGLEANER_TRAEFIK_YML=traefik_data\nGLEANER_TRAEFIK=traefik_data\nGLEANER_OBJECTS=minio\nGLEANER_GRAPH=graph\nSPARQL_DEFAULT_SPARQL_ENDPOINT_PATH=/blazegraph/namespace/earthcube/sparql\nTRAEFIK_AUTH={snip}\nGC_GITHUB_SECRET={snip}\nGC_GITHUB_CLIENTID={snip}\nGC_NB_AUTH_MODE=service\nS3ADDRESS=oss.geocodes-1.earthcube.org\nS3KEY=gleaner\nS3SECRET=adtoearthcube\nS3SSL=true\nS3PORT=443\nBUCKET=gleaner\nBUCKETPATH=summoned\nPATHTEMPLATE='${bucketpath}/${reponame}/${sha}.jsonld'\nTOOLTEMPLATE='${bucketpath}/${reponame}/${ref}.json'\nTOOLBUCKET=ecrr\nTOOLPATH=summoned</code></pre></p> <p>localConfig.yaml <pre><code>---\nminio:\n  address: oss.geocodes-1.earthcube.org\n  port: 443\n  accessKey: {snip}\n  secretKey: {snip}\n  ssl: true\n  bucket: gleaner # can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: https://graph.geocodes-1.earthcube.org/blazegraph/namespace/earthcube/sparql\ns3:\n  bucket: gleaner # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n#  location: sources.csv\n# this can be a remote csv\n#  type: csv\n  location: https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=TestSources202210\n#  location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&amp;sheet={sheet_name}\n# TBD -- Just use the sources in the gleaner file.\n#  type: yaml\n#  location: gleaner.yaml</code></pre></p> <p>config/facets_search <pre><code>---\n#API_URL: http://localhost:3000\nAPI_URL: https://geocodes.earthcube.org/ec/api\nTRIPLESTORE_URL: https://graph.geocodes-1.earthcube.org/blazegraph/namespace/earthcube/sparql\nSUMMARYSTORE_URL: https://graph.geocodes-1.earthcube.org/blazegraph/namespace/summary2/sparql\n#SUMMARYSTORE_URL: https://graph.geodex.org/blazegraph/namespace/summary/sparql\nECRR_TRIPLESTORE_URL: http://132.249.238.169:8080/fuseki/ecrr/query\nECRR_GRAPH: http://earthcube.org/gleaner-summoned\nTHROUGHPUTDB_URL: https://throughputdb.com/api/ccdrs/annotations\nSPARQL_QUERY: queries/sparql_query.txt\nSPARQL_HASTOOLS: queries/sparql_hastools.txt\nSPARQL_TOOLS_WEBSERVICE: queries/sparql_gettools_webservice.txt\nSPARQL_TOOLS_DOWNLOAD: queries/sparql_gettools_download.txt\nJSONLD_PROXY: https://geocodes.geocodes-1.earthcube.org/ec/api/${o}\nSPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\nSPARQL_YASGUI: https://sparqlui.geocodes.earthcube.org/?</code></pre></p>"},{"location":"production/production_configs/#-","title":"----------","text":""},{"location":"production/production_configs/#geocodes-devstaging-geocodesgeocodes-devearthcubeorg","title":"Geocodes-dev/staging  geocodes.geocodes-dev.earthcube.org","text":"<p>docker config: geocodes-dev configs/facet_search</p> <p>This would be a list of all sources and sitemaps.</p> <p>environment <pre><code>HOST=geocodes-1.earthcube.org\nFACET_SERVICES_FILE=./config/services.js\nGC_CLIENT_DOMAIN=alpha.geocodes.earthcube.org\nMINIO_ROOT_ACCESS_KEY={snip}\nMINIO_ROOT_SECRET_KEY={snip}\nMINIO_SERVICE_ACCESS_KEY={snip}\nMINIO_SERVICE_SECRET_KEY={snip}\nSPARQL_DEFAULT_SPARQL_ENDPOINT_PATH=/blazegraph/namespace/earthcube/sparql\nS3ADDRESS=oss.geocodes-1.earthcube.org\nS3KEY=worldsbestaccesskey\nS3SECRET=worldsbestsecretkey\nS3SSL=true\nS3PORT=443\nBUCKET=gleaner\nBUCKETPATH=summoned\nPATHTEMPLATE={{bucketpath}}/{{reponame}}/{{sha}}.jsonld\nTOOLTEMPLATE={{bucketpath}}/{{reponame}}/{{ref}}.json\nTOOLBUCKET=ecrr\nTOOLPATH=summoned\nGC_GITHUB_SECRET={snip}\nGC_GITHUB_CLIENTID={snip}\nGC_NB_AUTH_MODE=service\nGC_BASE=gcalpha</code></pre></p> <p>localConfig.yaml <pre><code>---\nminio:\n  address: oss.geocodes-dev.earthcube.org\n  port: 443\n  accessKey: {snip}\n  secretKey: {snip}\n  ssl: true\n  bucket: gleaner # can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/sparql\ns3:\n  bucket: gleaner # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n  #  location: sources.csv\n  location: https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=sources\n# this can be a remote csv\n#  type: csv\n#  location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&amp;sheet={sheet_name}\n# TBD -- Just use the sources in the gleaner file.\n#  type: yaml\n#  location: gleaner.yaml\n</code></pre></p> <p>facets_search.yaml <pre><code>---\n#API_URL: http://localhost:3000\nAPI_URL: https://geocodes.geocodes-dev.earthcube.org/ec/api\nTRIPLESTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/sparql\nSUMMARYSTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/summary/sparql\nECRR_TRIPLESTORE_URL: http://132.249.238.169:8080/fuseki/ecrr/query\nECRR_GRAPH: http://earthcube.org/gleaner-summoned\nTHROUGHPUTDB_URL: https://throughputdb.com/api/ccdrs/annotations\nSPARQL_QUERY: queries/sparql_query.txt\nSPARQL_HASTOOLS: queries/sparql_hastools.txt\nSPARQL_TOOLS_WEBSERVICE: queries/sparql_gettools_webservice.txt\nSPARQL_TOOLS_DOWNLOAD: queries/sparql_gettools_download.txt\nJSONLD_PROXY: https://geocodes.geocodes-dev.earthcube.org/ec/api/${o}\n# oauth issues. need to add another auth app for additional 'proxies'\nSPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\nSPARQL_YASGUI: https://sparqlui.geocodes-dev.earthcube.org/?</code></pre></p>"},{"location":"production/production_configs/#wifire","title":"wifire","text":"<p>docker config: geocodes-dev   configs/wifire</p> <p>environment <pre><code>HOST=geocodes-dev.earthcube,org\nFACET_SERVICES_FILE=./config/services.js\nGC_CLIENT_DOMAIN=geocodes.wifire-data.sdsc.edu\nMINIO_ROOT_ACCESS_KEY={snip}\nMINIO_ROOT_SECRET_KEY={snip}\nMINIO_SERVICE_ACCESS_KEY={snip}\nMINIO_SERVICE_SECRET_KEY={snip}\nSPARQL_DEFAULT_SPARQL_ENDPOINT_PATH=/blazegraph/namespace/wifire/sparql\nS3ADDRESS=oss.geocodes-dev.earthcube.org\nS3KEY={snip}\nS3SECRET={snip}\nS3SSL=true\nS3PORT=443\nBUCKET=wifire\nBUCKETPATH=summoned\nPATHTEMPLATE={{bucketpath}}/{{reponame}}/{{sha}}.jsonld\nTOOLTEMPLATE={{bucketpath}}/{reponame}}/{{ref}}.json\nTOOLBUCKET=ecrr\nTOOLPATH=summoned\nGC_GITHUB_SECRET=OAUTH SECRET\nGC_GITHUB_CLIENTID=OAUTH APP ID\nGC_NB_AUTH_MODE=service\nGC_BASE=wifire</code></pre></p> <p>localConfig.yaml <pre><code>---\nminio:\n  address: oss.geocodes-dev.earthcube.org\n  port: 443\n  accessKey: {snip}\n  secretKey: {snip}\n  ssl: true\n  bucket: wifire # can be overridden with MINIO_BUCKET\nsparql:\n#  endpoint: http://localhost/blazegraph/namespace/wifire/sparql\n  endpoint: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/wifire/sparql\ns3:\n  bucket: wifire # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n  location:  https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=wifire\n# this can be a remote csv\n#  type: csv\n#  location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&amp;sheet={sheet_name}\n# TBD -- Just use the sources in the gleaner file.\n#  type: yaml\n#  location: gleaner.yaml\n</code></pre></p> <p>config/facets_config_wifire  <pre><code>---\n#API_URL: http://localhost:3000\nAPI_URL: https://geocodes.wifire-data.sdsc.edu/ec/api\n#TRIPLESTORE_URL: https://graph.geodex.org/blazegraph/namespace/earthcube/sparql\nTRIPLESTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/wifire/sparql\nSUMMARYSTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/wifire_summary/sparql\n#SUMMARYSTORE_URL: https://graph.geodex.org/blazegraph/namespace/summary/sparql\nECRR_TRIPLESTORE_URL: http://132.249.238.169:8080/fuseki/ecrr/query\nECRR_GRAPH: http://earthcube.org/gleaner-summoned\nTHROUGHPUTDB_URL: https://throughputdb.com/api/ccdrs/annotations\nSPARQL_QUERY: queries/sparql_query.txt\nSPARQL_HASTOOLS: queries/sparql_hastools.txt\nSPARQL_TOOLS_WEBSERVICE: queries/sparql_gettools_webservice.txt\nSPARQL_TOOLS_DOWNLOAD: queries/sparql_gettools_download.txt\nJSONLD_PROXY: \"${window.location.origin}/ec/api/${o}\"\n# oauth issues. need to add another auth app for additional 'proxies'\n# This is the one that will work: SPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\nSPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\n####\nSPARQL_YASGUI: https://sparqlui.geocodes-dev.earthcube.org/?</code></pre></p>"},{"location":"production/production_configs/#alpha-needs-to-be-updated-alphageocodesearthcubeorg","title":"Alpha needs to be updated - alpha.geocodes.earthcube.org","text":"<p>This would be a list of all sources and sitemaps.</p> <p>environment <pre><code>HOST=geocodes-1.earthcube.org\nFACET_SERVICES_FILE=./config/services.js\nGC_CLIENT_DOMAIN=alpha.geocodes.earthcube.org\nMINIO_ROOT_ACCESS_KEY={snip}\nMINIO_ROOT_SECRET_KEY={snip}\nMINIO_SERVICE_ACCESS_KEY={snip}\nMINIO_SERVICE_SECRET_KEY={snip}\nSPARQL_DEFAULT_SPARQL_ENDPOINT_PATH=/blazegraph/namespace/earthcube/sparql\nS3ADDRESS=oss.geocodes-1.earthcube.org\nS3KEY=worldsbestaccesskey\nS3SECRET=worldsbestsecretkey\nS3SSL=true\nS3PORT=443\nBUCKET=gleaner\nBUCKETPATH=summoned\nPATHTEMPLATE={{bucketpath}}/{{reponame}}/{{sha}}.jsonld\nTOOLTEMPLATE={{bucketpath}}/{{reponame}}/{{ref}}.json\nTOOLBUCKET=ecrr\nTOOLPATH=summoned\nGC_GITHUB_SECRET={snip}\nGC_GITHUB_CLIENTID={snip}\nGC_NB_AUTH_MODE=service\nGC_BASE=gcalpha</code></pre></p> <p>localConfig.yaml <pre><code>---\nminio:\n  address: oss.geocodes-1.earthcube.org\n  port: 443\n  accessKey: {snip}\n  secretKey: {snip}\n  ssl: true\n  bucket: gleaner # can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: https://graph.geocodes-1.earthcube.org/blazegraph/namespace/earthcube/sparql\ns3:\n  bucket: gleaner # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n#  location: sources.csv\n# this can be a remote csv\n#  type: csv\n  location: https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=sources\n#  location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&amp;sheet={sheet_name}\n# TBD -- Just use the sources in the gleaner file.\n#  type: yaml\n#  location: gleaner.yaml\n</code></pre></p> <p>facets_search.yaml <pre><code></code></pre></p>"},{"location":"production/production_ui_deployment/","title":"Production UI Deployment and Server Switches","text":""},{"location":"production/production_ui_deployment/#requirements","title":"Requirements:","text":"<ul> <li>DNS Pointing at geocodes.earthcube.org  a</li> <li>Docker<ul> <li>secrets need to be set</li> <li>deploy configs facet_config_production</li> <li>create file env variables</li> <li>deploy deployment/geocodes-compose-production.yaml to portainer</li> </ul> </li> </ul>"},{"location":"production/production_ui_deployment/#gotchas","title":"GOTCHAS","text":"<p>Let's Encrypt has limits on production changes, so if you deploy the stack before the DNS, then you can lock y out changes for 7 days.</p>"},{"location":"production/production_ui_deployment/#proposed-steps","title":"Proposed Steps","text":"<ul> <li>ask for NDS TIL (time to live) to be lowered.</li> <li>Create docker config files, secrets, and configs</li> <li>change DNS.</li> <li>create a production stack</li> <li>STOP PREVIOUS PRODUCTION STACK</li> </ul>"},{"location":"production/production_ui_deployment/#notes","title":"Notes:","text":""},{"location":"production/production_ui_deployment/#lets-encypt","title":"Let's encypt","text":""},{"location":"production/production_ui_deployment/#dns","title":"DNS","text":"<p>We will now use a single DNS record for just the UI: geocodes.earthcube.org. This is fixed in the deployment/geocodes-compose-production.yaml</p> <p>Previously, before external file configuration (facet_config_production),  we had services at (graph|oss|minioadmin, etc).geocodes.earthcube.org.  It is easier to configure the client to point at server services (graph|oss|minioadmin, etc).host eg. (graph|oss|minioadmin, etc).geocodes-aws.earthcube.org where a wildcard DNS is setup.</p>"},{"location":"production/production_ui_deployment/#docker-secrets","title":"Docker Secrets","text":"<p>We are begining to use docker secrets, so these need to be configured</p>"},{"location":"production/reconfigure_geocodes_ui_containers/","title":"Reconfigure Geocodes Services Containers for Production:","text":""},{"location":"production/reconfigure_geocodes_ui_containers/#setup-and-start-geocodes-client-using-portainer-ui","title":"Setup and start GeoCodes Client using portainer ui","text":"<p>Steps:</p> <ul> <li>Stop the geocodes stack</li> <li>copy, edit, delete, recreate the configuration file</li> <li>test</li> <li>instructions for Updating a GEOCODES CLIENT Configuration if things do not work<ul> <li>or delete stack and reload</li> </ul> </li> </ul>"},{"location":"production/reconfigure_geocodes_ui_containers/#modify-the-facet-search-configuration","title":"Modify the Facet Search Configuration","text":"<ul> <li>edit in deployment/facets/config.yaml</li> <li>this file is mounted on the container as a docker config file<ul> <li>run the run_add_configs.sh</li> </ul> </li> </ul> <p>Portions of deployment/facets/config.yaml that might be changed.</p> portainer configs/facets_config.yaml <pre><code>API_URL: https://geocodes.{your host}/ec/api/\nSPARQL_NB: https:/geocodes.{your host}/notebook/mkQ?q=${q}\nSPARQL_YASGUI: https://geocodes.{your host}/sparqlgui?\n#API_URL: \"${window_location_origin}/ec/api\"\n#TRIPLESTORE_URL: https://graph.geocodes-1.earthcube.org/blazegraph/namespace/gctest/sparql\nTRIPLESTORE_URL: https://graph.{your host}/blazegraph/namespace/gctest/sparql\nBLAZEGRAPH_TIMEOUT: 20\n## ECRR need to use fuseki source, for now.\nECRR_TRIPLESTORE_URL: http://132.249.238.169:8080/fuseki/ecrr/query \n# ECRR_TRIPLESTORE_URL:   http://{your host}/blazegraph/namespace/ecrr/sparql \nECRR_GRAPH: http://earthcube.org/gleaner-summoned\nTHROUGHPUTDB_URL: https://throughputdb.com/api/ccdrs/annotations\nSPARQL_QUERY: queries/sparql_query.txt\nSPARQL_HASTOOLS: queries/sparql_hastools.txt\nSPARQL_TOOLS_WEBSERVICE: queries/sparql_gettools_webservice.txt\nSPARQL_TOOLS_DOWNLOAD: queries/sparql_gettools_download.txt\n# JSONLD_PROXY needs qoutes... since it has a $\nJSONLD_PROXY: \"https://geocodes.{your host}/ec/api/${o}\" \nSPARQL_YASGUI: https://sparqlui.{your host}/?</code></pre>"},{"location":"production/reconfigure_geocodes_ui_containers/#create-geocodes-stack","title":"Create Geocodes Stack","text":"<ul> <li>log into portainer<ul> <li>if this is a first login, it will ask you for a password.</li> <li>click add stack button <pre><code>Name: geocodes\nBuild method: git repository\nRepository URL: https://github.com/earthcube/geocodes\nreference: refs/heads/main\nCompose path: deployment/geocodes-compose.yaml</code></pre></li> <li>Environment variables: click 'load variables from .env file'<ul> <li>load {myhost}.geocodes.env</li> </ul> </li> <li>Actions:<ul> <li>Click: Deploy This Stack</li> </ul> </li> </ul> </li> </ul> Geocodes Stack"},{"location":"production/reconfigure_geocodes_ui_containers/#test-geocodes-client","title":"Test Geocodes Client","text":"<p>Issues</p> <p>IF things are not working in the UI, it is probably the facet search configuration You can take down the geocodes stack, and delete the config/facets_search or you can possibly just stop the gecodes_vue_ui service, and edit the facets_search config as noted here: See Managing Geocodes UI Containers</p> <ol> <li>Got to https://geocodes.{your host}/</li> <li>Got to configuration: https://geocodes.{your host}/#/config</li> <li>Two sections, one is the facests/config.yaml and the second is the API configuration (sanitized, we hope)</li> </ol> <p>Done</p> <p>This is the end of the deployment steps.</p>"},{"location":"production/reconfigure_geocodes_ui_containers/#if-the-deployment-is-working-you-can-now","title":"If the deployment is working, you can now","text":"<ul> <li>(Setup a Production Configuration)[./production/creatingProductionConfigs.md]</li> <li>(Reconfigure Geocodes UI)[./production/reconfigure_geocodes_ui_containers.md]</li> </ul>"},{"location":"production/sparql/","title":"Some SPARQL Queries","text":""},{"location":"production/sparql/#is-there-data-in-the-namespace","title":"Is there data in the namespace:","text":""},{"location":"production/sparql/#count","title":"count","text":"<pre><code>SELECT (count(*) as ?count) \nWHERE     { ?s ?p ?o}\n</code></pre>"},{"location":"production/sparql/#triples","title":"triples","text":"<p>select all from all graphs (?g) <pre><code>SELECT *\nWHERE     {     GRAPH ?g {?s ?p ?o}}\nLIMIT 100</code></pre></p> <pre><code>SELECT *\nWHERE     {     GRAPH ?g {?s ?p ?o}}\nLIMIT 100</code></pre>"},{"location":"production/sparql/#what-types-are-in-the-system","title":"what types are in the system","text":"<pre><code>prefix schema: &lt;https://schema.org/&gt;\nSELECT  ?type  (count(distinct ?s ) as ?scount)\nWHERE {\n{\n\n       ?s a ?type .\n\n       }\n}\n\nGROUP By ?type\nORDER By DESC(?scount)```\n\n## Dataset \n### count\nsubject is a rdf:type schema.org/Dataset\n```sparql\nSELECT (count(?g ) as ?count) \nWHERE     {     GRAPH ?g {?s a &lt;https://schema.org/Dataset&gt;}}\n</code></pre>"},{"location":"production/sparql/#triples_1","title":"triples","text":"<pre><code>SELECT *  \nWHERE     {     GRAPH ?g {?s a &lt;https://schema.org/Dataset&gt;}}\nLIMIT 100</code></pre>"},{"location":"production/sparql/#keyword-count","title":"Keyword count","text":""},{"location":"production/sparql/#count-of-keywords","title":"count of keywords","text":"<pre><code>prefix schema: &lt;https://schema.org/&gt;\nSELECT  (count(distinct ?keyword ) as ?scount)\nWHERE {\n  {\n\n       ?s schema:keywords ?keyword .\n\n       }\n}\n\nORDER By DESC(?scount)</code></pre>"},{"location":"production/sparql/#keyword-counts","title":"Keyword counts","text":"<pre><code># needs work... keywords can be an array.\nprefix schema: &lt;https://schema.org/&gt;\nSELECT  ?keyword (count(distinct ?s) as ?scount)\nWHERE {\n  {\n\n       ?s schema:keywords ?keyword .\n\n       }\n}\nGROUP By ?keyword\nORDER By DESC(?scount)</code></pre>"},{"location":"production/sparql/#publisher","title":"Publisher","text":"<pre><code>prefix schema: &lt;https://schema.org/&gt;\nSELECT  ?pubname (count(distinct ?s) as ?scount)\nWHERE {\n  {\n\n       ?s schema:publisher/schema:name|schema:sdPublisher ?pubname .\n       }\n}\nGROUP By ?pubname\nORDER By DESC(?scount)</code></pre>"},{"location":"production/sparql/#variable-name","title":"Variable Name","text":"<pre><code>prefix schema: &lt;https://schema.org/&gt;\nSELECT  ?variableName (count(distinct ?s) as ?scount)\nWHERE {\n  {\n\n       ?s schema:variableMeasured ?variableMeasured .\n    ?variableMeasured schema:name ?variableName\n\n       }\n}\nGROUP By ?variableName\nORDER By DESC(?scount)</code></pre>"},{"location":"production/sparql/#dataset-with-versions","title":"Dataset with versions","text":""},{"location":"production/sparql/#list-of-version-numbers","title":"list of version numbers","text":"<pre><code>prefix schema: &lt;https://schema.org/&gt;\nSELECT  ?version (count(distinct ?s) as ?scount)\nWHERE {\n  {\n\n       ?s schema:version ?version .\n\n       }\n}\nGROUP By ?version\nORDER By DESC(?scount)</code></pre>"},{"location":"production/sparql/#datasets-with-a-version-number","title":"datasets with a version number","text":"<pre><code>prefix schema: &lt;http://schema.org/&gt;\nprefix sschema: &lt;https://schema.org/&gt;\nSELECT distinct ?subj ?sameAs ?version ?url where {\n    {SELECT distinct  ?sameAs (MAX(?version2) as ?version  )\n    where {\n       ?subj schema:sameAs|sschema:sameAs ?sameAs .\n        ?subj schema:version|sschema:version ?version2 .\n\n    }\n        GROUP BY ?sameAs\n}\n        ?subj schema:identifier|sschema:identifier ?url .\n        ?subj schema:version|sschema:version ?version .\n        ?subj schema:sameAs|sschema:sameAs ?sameAs .\n    }\n    GROUP BY ?sameAs ?version  ?subj  ?url\norder by ?sameAs ?version\nlimit 1000</code></pre>"},{"location":"production/sparql/#datasets-with-multiple-versions","title":"Datasets with multiple versions","text":"<pre><code>prefix schema: &lt;http://schema.org/&gt;\nprefix sschema: &lt;https://schema.org/&gt;\nSELECT distinct ?subj ?sameAs ?version ?url where {\n    {SELECT distinct  ?sameAs (MAX(?version2) as ?version  )\n    where {\n       ?subj schema:sameAs|sschema:sameAs ?sameAs .\n        ?subj schema:version|sschema:version ?version2 .\n    filter (?version2 &gt;1)\n    }\n        GROUP BY ?sameAs\n}\n        ?subj schema:identifier|sschema:identifier ?url .\n        ?subj schema:version|sschema:version ?version .\n        ?subj schema:sameAs|sschema:sameAs ?sameAs .\n    }\n    GROUP BY ?sameAs ?version  ?subj  ?url\norder by ?sameAs ?version</code></pre>"},{"location":"production/sparql/#versions-for-earthreforg","title":"Versions for Earthref.org","text":"<pre><code>prefix schema: &lt;http://schema.org/&gt;\nprefix sschema: &lt;https://schema.org/&gt;\nSELECT distinct ?subj ?sameAs ?version ?url ?g where {\n   graph ?g {\n        ?subj schema:identifier|sschema:identifier ?url .\n        ?subj schema:version|sschema:version ?version .\n        ?subj schema:sameAs|sschema:sameAs ?sameAs .\n        ?subj schema:sdPublisher|sschema:sdPublisher \"EarthRef.org\".\n    }\n        ?subj2 schema:sameAs|sschema:sameAs ?sameAs .\n        ?subj2 schema:version|sschema:version ?version2 .\n\n    FILTER (?version &lt; ?version2).\n    }\n    GROUP BY ?sameAs ?version  ?subj  ?url ?g\n</code></pre>"},{"location":"production/sparql/#latest-version-for-a-steens-query","title":"Latest version for a steens query","text":"<p>this includes a max version  <pre><code>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nprefix schema: &lt;http://schema.org/&gt;\nprefix sschema: &lt;https://schema.org/&gt;\n# no longer works as expected. Identifier changed. Try to use sameAs as in other version examples.\nSELECT distinct ?subj  ?citation ?version ?pubname ?resourceType ?name  (GROUP_CONCAT(DISTINCT ?placename; SEPARATOR=\", \") AS ?placenames)\n        (GROUP_CONCAT(DISTINCT ?kwu; SEPARATOR=\", \") AS ?kw) (MAX(?version) as ?latestVersion)\n        ?datep  (GROUP_CONCAT(DISTINCT ?url; SEPARATOR=\", \") AS ?disurl) (MAX(?score1) as ?score) ?description ?g\n        WHERE {\n            ?lit bds:search \"steens\" .\n            ?lit bds:matchAllTerms false .\n            ?lit bds:relevance ?score1 .\n            ?subj ?p ?lit .\n            BIND (IF (exists {?subj a schema:Dataset .} ||exists{?subj a sschema:Dataset .} , \"data\", \"tool\") AS ?resourceType).\n            filter( ?score1 &gt; 0.04).\n          graph ?g {\n            Minus {?subj a sschema:ResearchProject } .\n            Minus {?subj a schema:ResearchProject } .\n            Minus {?subj a schema:Person } .\n            Minus {?subj a sschema:Person } .\n             ?subj schema:name|sschema:name ?name .\n             ?subj schema:description|sschema:description ?description .\n             }\n             ?subj schema:citation|sschema:citation ?citation .\n             ?subj schema:version|sschema:version ?version .\n            optional {?subj schema:distribution/schema:url|schema:subjectOf/schema:url ?url .}\n            OPTIONAL {?subj schema:datePublished|sschema:datePublished ?date_p .}\n            OPTIONAL {?subj schema:publisher/schema:name|sschema:publisher/sschema:name|sschema:sdPublisher|schema:provider/schema:name ?pub_name .}\n            OPTIONAL {?subj schema:spatialCoverage/schema:name|sschema:spatialCoverage/sschema:name ?place_name .}\n            OPTIONAL {?subj schema:keywords|sschema:keywords ?kwu .}\n            BIND ( IF ( BOUND(?date_p), ?date_p, \"No datePublished\") as ?datep ) .\n            BIND ( IF ( BOUND(?pub_name), ?pub_name, \"No Publisher\") as ?pubname ) .\n            BIND ( IF ( BOUND(?place_name), ?place_name, \"No spatialCoverage\") as ?placename ) .\n            ?subj schema:version|sschema:version ?version .\n        }\n        GROUP BY ?subj ?pubname ?placenames ?kw ?datep ?disurl ?score ?name ?description  ?resourceType ?g  ?citation ?version\n        ORDER BY DESC(?score)\n ```\n\n## Resource Registry\n\n(lot of blank nodes)\n### count\n```sparql\nSELECT (count(?g ) as ?count)\nWHERE     {     GRAPH ?g {?s a &lt;https://schema.org/CreativeWork&gt;}}\n</code></pre></p>"},{"location":"production/sparql/#triples_2","title":"Triples","text":"<pre><code>SELECT * \nWHERE     {     GRAPH ?g {?s a &lt;https://schema.org/CreativeWork&gt;}}\nLIMIT 100</code></pre>"},{"location":"setting_up_services/setting_up_aws/","title":"Setting up Geocodes Services on AWS","text":"Note <p>we decided against using both s3 and Neptune. each neptune instnace is one graph namespace, so at $95/month for an instance, that is a bit high. s3 works, but for testing we take buckets up and down, easier to manage in minio that AWS.</p> <p>To do this, we are going to utilize four pieces: * s3, replaces Minio Container * neptune, replaces Graph Container * single virutal machine setup as a docker storm. * Portainer setup on in Our Openstack.</p> <p>Major differences in the docker: * uses config to store traefik config * use a volume to store the trafik * using secrets</p>"},{"location":"setting_up_services/setting_up_aws/#notes-on-setting-up-s3-and-neptune","title":"Notes on Setting up s3 and Neptune","text":""},{"location":"setting_up_services/setting_up_aws/#docker-stack","title":"Docker stack","text":"<p>Install docker stack setup as the base machine install where  <code>docker stack init --address--</code></p>"},{"location":"setting_up_services/setting_up_aws/#from-console","title":"From Console","text":"<ol> <li>setup netowrk</li> <li>setup volumes</li> <li>setup config</li> <li>setup secrets</li> <li></li> </ol> <p><code>volume create traefik_data</code> <code>volume create logs</code></p> <p><code>docker network create -d overlay --attachable traefik_proxy</code></p> <p><code>docker config create   traefik_yml ./traefik-aws/traefik.yml</code></p> <p>docker secret create </p> <p>S3SSL=true S3PORT=443</p> <p>MINIO_ROOT_ACCESS_KEY=worldsbestaccesskey MINIO_ROOT_SECRET_KEY=worldsbestsecretkey</p>"},{"location":"setting_up_services/setting_up_aws/#minio_service_access_keyworldsbestaccesskey","title":"MINIO_SERVICE_ACCESS_KEY=worldsbestaccesskey","text":""},{"location":"setting_up_services/setting_up_aws/#minio_service_secret_keyworldsbestsecretkey","title":"MINIO_SERVICE_SECRET_KEY=worldsbestsecretkey","text":"<p>GC_GITHUB_CLIENTID_OR_USER_GITHUB_TOKEN=OAUTH SECRET GC_GITHUB_SECRET_OR_USER=OAUTH APP ID</p>"},{"location":"setting_up_services/setting_up_aws/#setup-stack-in-portainer","title":"setup stack in portainer","text":"<ol> <li>edit the portainer_aws.env</li> <li> <p>copy over the base-aws-compose.yaml</p> </li> <li> <p>or use the git version <pre><code>Name: base\nBuild method: git repository\nRepository URL: https://github.com/earthcube/geocodes\nReference: refs/heads/main\nCompose path: dbase-aws-compose.yaml</code></pre></p> </li> </ol>"},{"location":"setting_up_services/setting_up_aws/#notes","title":"Notes:","text":"<p>https://enginaltay.medium.com/how-to-use-traefik-as-ingress-router-on-aws-fc559f87f4d8</p>"},{"location":"setting_up_services/setup_aws_neptune/","title":"Setup aws neptune","text":""},{"location":"setting_up_services/setup_aws_neptune/#setting-up-aws-services","title":"setting up AWS services","text":""},{"location":"setting_up_services/setup_aws_neptune/#setup-up-aws-neptune","title":"setup up AWS-Neptune","text":"<p>Start with cloudbank login</p> <p>cloudbank.org/billing-account-access</p> <p>click on login</p> <p>go to upper left services, and star/favorite  Neptune and s3, iam, ec2</p> <p>click on Neptune to provision</p> <p>you can take all the defaults</p> <p>including making a decoder notebook to look at it</p> <p>more to come as we sort out access</p> <p>For now getting crawl from radiant to s3, for later Neptune load</p>"},{"location":"setting_up_services/setup_base_machine_configuration/","title":"Setup Machine:","text":"<p>This is step 1 of 5 major steps:</p> <ol> <li>Install base containers on a server</li> <li>Setup services containers</li> <li>Setup Gleaner containers</li> <li>Initial setup of services and loading of data</li> <li>Setup Geocodes UI using datastores defined in Initial Setup</li> </ol>"},{"location":"setting_up_services/setup_base_machine_configuration/#base-machine-to-run-docker-containers-treafik-and-portainer","title":"Base Machine to run Docker Containers Treafik and Portainer:","text":"<p>This is what will be needed to create a production server</p> <ul> <li>base virtual machine for containers</li> <li>ability to request DNS,</li> </ul> <p>SUMMARY</p> <p>These are a summary of the steps, The Step Details are below.</p> <p>DOCKER REQUIREMENT</p> <p>If you are running on Ubuntu, you need to remove the provided docker.com version. Official docker package We suggest that for others, confirm that you can run </p> <pre><code>docker compose version\nDocker Compose version v2.13.0</code></pre> <p>If you cannot run <code>docker compose</code> then update to the docker.com version This is the version we are presently running.</p> <pre><code>Client: Docker Engine - Community\n     Version:           20.10.21\n     API version:       1.41</code></pre> <p>DOCKER SWARM</p> <p>Docker swarm needs to be init'd with the public ip address.</p>"},{"location":"setting_up_services/setup_base_machine_configuration/#step-overview","title":"Step Overview:","text":"<ul> <li>create a machine in openstack (if production)<ul> <li>select size</li> <li>associate floating IP<ul> <li>ask for DNS for that ip to be configured with needed names</li> </ul> </li> </ul> </li> <li> <p>ssh to machine. You do not need to have the DNS's to install the software. But it will be needed.</p> <ul> <li>update apt<ul> <li><code>sudo apt update</code></li> </ul> </li> <li> <p>update base software</p> <ul> <li><code>sudo apt upgrade</code></li> </ul> </li> <li> <p>install docker</p> </li> </ul> </li> </ul> Use Official Docker for Ubuntu <ul> <li> <p>use these docker install instructions</p> </li> <li> <p>add ubuntu (or other users) to docker group</p> <ul> <li><code>sudo groupadd docker</code></li> <li><code>sudo usermod -aG docker ubuntu</code></li> </ul> </li> <li>reboot</li> <li><code>sudo reboot now</code></li> </ul> <ul> <li>create a directory for geocodes, set up permissions and groups<ul> <li><code>sudo mkdir /data/decoder</code></li> <li><code>ln -s /data/decoder/ decoder</code></li> <li><code>ln -s /data/decoder/ geocodes</code></li> <li><code>sudo addgroup geocodes</code></li> <li><code>usermod -a -G geocodes {user}</code></li> <li><code>sudo chgrp geocodes /data/decoder</code></li> <li><code>sudo chmod g+rwx /data/decoder</code></li> </ul> </li> <li>init docker swarm<ul> <li> <p>DOCKER SWARM</p>   Docker swarm needs to be init'd with the public ip address. </li> <li><code>nslookup {HOSTNAME}</code></li> <li><code>sudo docker swarm init --advertise-addr {PUBLIC_IP}</code></li> <li>save the token to a file (I use NOTES)</li> </ul> </li> <li>verify proper base configuration<ul> <li><code>docker compose --help</code> shows a -p flag</li> </ul> </li> <li>SNAPSHOT and creaate an image<ul> <li></li> </ul> </li> <li>clone geocodes<ul> <li><code>cd decoder</code> or <code>cd /data/decoder</code></li> <li><code>git clone https://github.com/earthcube/geocodes.git</code></li> </ul> </li> <li>configure a base server</li> <li>base-machine-compose.yaml is the full stack with a portainer, treafik</li> <li>base-swarm-compose.yaml is just a treakfit. connect with your existing portainer.</li> <li>take a break and wait for the DNS entries.<ul> <li>if you cannot wait for the DNS, you can go to the no cert port <ul> <li>https://{HOST}}:9443/</li> <li>use chrome, click advanced, and go to the port.</li> </ul> </li> </ul> </li> </ul>"},{"location":"setting_up_services/setup_base_machine_configuration/#step-details","title":"Step Details:","text":""},{"location":"setting_up_services/setup_base_machine_configuration/#create-a-machine-in-openstack","title":"create a machine in openstack","text":"<p>Suggested size:</p> <p>SDSC Openstack:</p> <ul> <li>ubuntu 22</li> <li>100 gig<ul> <li>m1.2xlarge (8 CPU, 32 gig)</li> <li>network: earthcube</li> </ul> </li> <li>Security groups:<ul> <li>remote ssh (22)</li> <li>geocodes (http/https; 80:443)</li> <li>portainer (temporary need: 9443)</li> <li>minio (optional: 9000/9001)</li> </ul> </li> <li>Keypair: earthcube (or any)</li> </ul> <p>Ports Pre-DNS</p> <p>minio ports do not need to be open, we are proxying on 80 and 443 Portainer port (9443)  can be opended temporarily if you want to play a bit pre-DNS.</p> <p>Associate a Public IP</p> <p>After the machine is created, we can change the IP to the one associated with geocodes.earthcube.org</p>"},{"location":"setting_up_services/setup_base_machine_configuration/#setup-domain-names","title":"setup domain names","text":"<ul> <li>Machines</li> <li>Name for remote DNS</li> </ul> <p>ESSENTIAL for PRODUCTION</p> <p>It is ESSENTIAL for PRODUCTION that the names are defined in a DNS. This allows for https for all services and some services (aka s3/minio) do not play well with a proxy.</p> <p>You might be able to run production stack using localhost, with these DNS... but that mucks with the lets encrypt HTTPS certs... if you control your own DNS, these are the  entries needed.  Name for local DNS</p> <p>Local testing and development can be using  the local compose configuration. This use http, and  local ports for services that cannot be proxied</p>"},{"location":"setting_up_services/setup_base_machine_configuration/#ssh-to-machine-and-verify","title":"ssh to machine and verify","text":"<p><code>ssh -i ~/.ssh/earthcube.pem ubuntu@{public IP}</code></p> add your ssh key so you can log in as main user (eg. ubuntu) <p>SSH Keys</p> <p>for production, we recommend that you use a group account/main account</p> <p>to do this you will need to create and copy a public/private key</p> <p>Generate an ssh-key:</p> <pre><code>ssh-keygen -t rsa -b 4096 -C \"comment\"</code></pre> <p>copy it to your remote server:</p> <pre><code>ssh-copy-id user@ip</code></pre> <p>or you can manually copy the <pre><code>~/.ssh/id_rsa.pub to ~/.ssh/authorized_keys.</code></pre></p> <p>Edit</p> <p>It can be done through ssh command as mentioned @chepner:</p> <p><pre><code>ssh user@ip 'mkdir ~/.ssh'\nssh user@ip 'cat &gt;&gt; ~/.ssh/authorized_keys' &lt; ~/.ssh/id_rsa.pub</code></pre> (Above based on: stackexchange)</p>"},{"location":"setting_up_services/setup_base_machine_configuration/#configure-a-base-server","title":"configure a base server","text":""},{"location":"setting_up_services/setup_base_machine_configuration/#update-os","title":"update OS","text":"<ul> <li>update apt<ul> <li><code>sudo apt update</code></li> </ul> </li> <li>update base software<ul> <li>`sudo apt upgrade</li> </ul> </li> </ul>"},{"location":"setting_up_services/setup_base_machine_configuration/#add-docker-git","title":"add docker, git","text":"Offical Docker for Ubuntu <p>use these docker install instruction</p> <ul> <li>add ubuntu (or other users) to docker group</li> <li><code>sudo groupadd docker</code></li> <li><code>sudo usermod -aG docker ubuntu</code></li> <li>reboot</li> <li><code>sudo reboot now</code></li> </ul>"},{"location":"setting_up_services/setup_base_machine_configuration/#create-a-directory-for-geocodes-set-up-permissions-and-groups","title":"create a directory for geocodes, set up permissions and groups","text":"<pre><code>* `sudo mkdir /data/decoder`\n* `ln -s /data/decoder/ decoder`\n* `sudo addgroup geocodes`\n* `usermod -a -G geocodes {user}`\n* `sudo chgrp geocodes /data/decoder`\n* `sudo chmod g+rwx /data/decoder`</code></pre>"},{"location":"setting_up_services/setup_base_machine_configuration/#clone-geocodes-stack","title":"clone geocodes stack","text":"<ul> <li><code>cd decoder</code> or <code>cd /data/decoder</code></li> <li><code>git clone https://github.com/earthcube/geocodes.git</code></li> <li><code>cd geocodes/deployment</code></li> </ul>"},{"location":"setting_up_services/setup_base_machine_configuration/#copy-base_machineexampleenv-to-env","title":"copy  base_machine.example.env, to .env","text":""},{"location":"setting_up_services/setup_base_machine_configuration/#option-1-production-server-use-env","title":"Option 1. production server use .env","text":"<ul> <li><code>cp base_machine.example.env .env</code></li> <li>modify the file<ul> <li>note: you can also copy the full portainer.env. </li> </ul> </li> </ul>"},{"location":"setting_up_services/setup_base_machine_configuration/#option-2-testing-playing-developer","title":"Option 2. testing, playing, developer","text":"<ul> <li><code>cp base_machine.example.env {myproject}.env</code></li> <li>modify the file<ul> <li>note: you can also copy the full portainer.env.</li> </ul> </li> </ul>"},{"location":"setting_up_services/setup_base_machine_configuration/#modify-the-treafik-datatraefikyml","title":"modify the treafik-data/traefik.yml","text":"treafik-data/traefik.yml <p><pre><code>acme:\n# using staging for testing/development\n#     caServer: https://acme-staging-v02.api.letsencrypt.org/directory\n    email: example@earthcube.org\n    storage: acme.json\n    httpChallenge:\n        entryPoint: http</code></pre> If production, comment the line as shown. Developers see Lets Encypt Notes </p> Let Encrypt Notes <p>lets encrypt, </p> <p>(developers) set to use staging environment server while testing If you are doing development, then leave the caServer uncommented.</p>"},{"location":"setting_up_services/setup_base_machine_configuration/#start-the-base-containers","title":"start the base containers","text":"<ul> <li>new machine or developer</li> <li> <p><code>./run_base.sh -e  {myproject}.env</code></p> </li> <li> <p>production: this uses the default .env (cp  portainer.env .env)</p> </li> </ul> <code>./run_base.sh</code> <pre><code>      ubuntu@geocodes-dev:~/geocodes/deployment$ ./run_base.sh -e geocodes-1.env\n      Error response from daemon: network with name traefik_proxy already exists\n      NETWORK ID     NAME              DRIVER    SCOPE\n      ad6cbce4ec60   bridge            bridge    local\n      2f618fa7da6d   docker_gwbridge   bridge    local\n      f8048bc7a3d9   host              host      local\n      kibdi510bt0x   ingress           overlay   swarm\n      12c01a2186b0   none              null      local\n      u4d4oxfy7olc   traefik_proxy     overlay   swarm\n      Verify that the traefik_proxy network SCOPE is swarm\n      traefik_data\n      portainer_data\n      true\n      [+] Running 2/2\n      \u283f Container portainer  Started                                           13.7s\n      \u283f Container traefik    Started</code></pre>"},{"location":"setting_up_services/setup_base_machine_configuration/#testing-setup","title":"Testing Setup","text":"<code>Are containers running</code> <p><code>docker ps</code> <pre><code>    * ubuntu@geocodes-dev:~/geocodes/deployment$ docker ps\n      CONTAINER ID   IMAGE                           COMMAND                  CREATED         STATUS         PORTS                                                                      NAMES\n      09a5d8683cce   traefik:v2.4                    \"/entrypoint.sh trae\u2026\"   2 minutes ago   Up 2 minutes   0.0.0.0:80-&gt;80/tcp, :::80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp, :::443-&gt;443/tcp   traefik\n      d3e2333ade6f   portainer/portainer-ce:latest   \"/portainer\"             2 minutes ago   Up 2 minutes   8000/tcp, 9000/tcp, 9443/tcp                                               portainer</code></pre></p> Is network setup correctly? <p><code>docker network ls</code> <pre><code>docker network ls\n      NETWORK ID     NAME              DRIVER    SCOPE\n      ad6cbce4ec60   bridge            bridge    local\n      2f618fa7da6d   docker_gwbridge   bridge    local\n      f8048bc7a3d9   host              host      local\n      kibdi510bt0x   ingress           overlay   swarm\n      12c01a2186b0   none              null      local\n      u4d4oxfy7olc   traefik_proxy     overlay   swarm</code></pre></p> Note <p>NAME:traefik_proxy needs to exist, and be DRIVER:overlay, SCOPE:swarm</p> Are volumes available <p><code>docker volumes</code> <pre><code>ubuntu@geocodes-dev:~$ docker volume ls\n      DRIVER    VOLUME NAME\n      local     graph\n      local     minio\n      local     portainer_data\n      local     traefik_data</code></pre></p> <p>are Traefik and Portainer available via the web?</p> <ul> <li>Treafik https://admin.{host}<ul> <li>login is admin:iforget</li> </ul> </li> </ul> image <p></p> <ul> <li>Portainer https://portainer.{host}/<ul> <li>this will ask you to setup and admin password</li> </ul> </li> </ul> image <p></p>"},{"location":"setting_up_services/setup_base_machine_configuration/#go-to-step-2","title":"Go to step 2.","text":"<ol> <li>Install base containers on a server</li> <li>Setup services containers</li> <li>Setup Gleaner containers</li> <li>Initial setup of services and loading of data</li> <li>Setup Geocodes UI using datastores defined in Initial Setup</li> </ol>"},{"location":"setting_up_services/setup_base_machine_configuration/#how-totroubleshooting","title":"How to/Troubleshooting","text":""},{"location":"setting_up_services/setup_base_machine_configuration/#updating-portainer-or-treafik","title":"updating Portainer, or treafik","text":"<p>the latest image needs to be pulled</p> <p><code>docker pull portainer/portainer-ce:latest</code></p> <p>then <code>./run_base.sh</code></p>"},{"location":"setting_up_services/setup_base_machine_configuration/#how-tos-needed","title":"How tos needed:","text":"<ul> <li>LOCAL DNS SETUP</li> <li>editing your local machine /etc/hosts file does not work with letsencrypt. </li> <li>If user has a local name server they control, that might work.</li> <li>setup a new password for traefik</li> <li>lets encrypt</li> </ul>"},{"location":"setting_up_services/setup_docker_swarm/","title":"intial notes on setting up a docker swarm on aws","text":""},{"location":"setting_up_services/setup_docker_swarm/#create-virutal-machine","title":"create virutal machine","text":"<ul> <li>Kevin did this ** user earthcube: sent key</li> <li></li> </ul>"},{"location":"setting_up_services/setup_docker_swarm/#login-install-docker","title":"login, install docker","text":"<p>earthcube@ip-172-31-9-0:~$ docker <pre><code>Command 'docker' not found, but can be installed with:\nsnap install docker         # version 20.10.17, or\napt  install docker.io      # version 20.10.21-0ubuntu1~22.04.2\napt  install podman-docker  # version 3.4.4+ds1-1ubuntu1`\nSee 'snap info docker' for additional versions.</code></pre></p> <p>This is the wrong command</p> <p>If you are running on Ubuntu, you need to remove the provided docker.com version. Official docker package We suggest that for others, confirm that you can run</p> <p>Follow the official docker as noted in the docs and the linux post isntall</p> <pre><code>sudo groupadd docker\nsudo usermod -aG docker earthcube\n</code></pre> <p>Log in and out</p> <pre><code>docker info </code></pre> <p>sudo systemctl enable docker.service sudo systemctl enable containerd.service</p> <p>docker swarm init --advertise-addr 54.244.44.10</p> <p>from the portainter.geocodes-dev.earthcube.org/ add environment Docker Swarm</p> <p>USE THE INTERNAL AMAZON IP ADDRESS, else you will need to open port 9001... </p> <p>docker network create \\ --driver overlay \\ portainer_agent_network</p> <p>docker service create \\ --name portainer_agent \\ --network portainer_agent_network \\ -p 9001:9001/tcp \\ --mode global \\ --constraint 'node.platform.os == linux' \\ --mount type=bind,src=//var/run/docker.sock,dst=/var/run/docker.sock \\ --mount type=bind,src=//var/lib/docker/volumes,dst=/var/lib/docker/volumes \\ portainer/agent:2.18.4</p>"},{"location":"setting_up_services/setup_docker_swarm/#configure-portainer","title":"configure portainer","text":""},{"location":"setting_up_services/setup_docker_swarm/#test","title":"Test...","text":"<p>do you have a the DNS setup properly. try</p> <p><code>nslookup admin.{HOST}</code></p>"},{"location":"setting_up_services/setup_docker_swarm/#add-in-portainer-interface","title":"Add in portainer interface","text":"<ul> <li>network :  </li> <li>traefik_proxy (overlay)</li> <li>headless_gleanerio(overlay)</li> <li>volumes: log,   traefik_data</li> <li>configuration: </li> <li>gleaner</li> <li>nabu</li> <li>treafik.yml    please change the email address </li> </ul> <pre><code>api:\n  dashboard: true\n\nentryPoints:\n  http:\n    address: \":80\"\n  https:\n    address: \":443\"\n\nproviders:\n  docker:\n    endpoint: \"unix:///var/run/docker.sock\"\n    exposedByDefault: false\n\ncertificatesResolvers:\n  httpresolver:\n    acme:\n      # using staging for testing/development\n      #caServer: https://acme-staging-v02.api.letsencrypt.org/directory\n      email: dwvalentine@ucsd.edu\n      storage: /etc/traefik/acme.json\n      httpChallenge:\n        entryPoint: http\n  httpresolver_staging:\n    acme:\n      # using staging for testing/development\n      caServer: https://acme-staging-v02.api.letsencrypt.org/directory\n      email: dwvalentine@ucsd.edu\n      storage: /etc/traefik/acme.json\n      httpChallenge:\n        entryPoint: http\n  httpresolver_production:\n    acme:\n      # using staging for testing/development\n      #caServer: https://acme-staging-v02.api.letsencrypt.org/directory\n      email: dwvalentine@ucsd.edu\n      storage: /etc/traefik/acme.json\n      httpChallenge:\n        entryPoint: http\n</code></pre> <p>Warn</p> <p>this needs to deploy a headleass, and the configs for gleaner and nabu, .</p> <ul> <li> <p>stack: headless</p> <ul> <li>https://github.com/earthcube/geocodes.git</li> <li>deployment/gleaner-compose.yaml</li> <li>env variables <pre><code>     HOST=</code></pre></li> </ul> </li> <li> <p>Then configure dagit in portainer hosting dagster. get the porteiner endpoint correct <pre><code>PORTAINER_URL=https://portainer.geocodes-aws-dev.earthcube.org:443/api/endpoints/9/docker/</code></pre></p> </li> </ul> <p>will need notes on securing it to a user, with that users API key</p>"},{"location":"setting_up_services/setup_geocodes_services_containers/","title":"Setup Geocodes Services  Containers:","text":"<p>This is step 2 of 4 major steps:</p> <ol> <li>Install base containers on a server</li> <li>Setup services containers</li> <li>Initial setup of services and loading of data</li> <li>Setup Geocodes UI using datastores defined in Initial Setup</li> </ol>"},{"location":"setting_up_services/setup_geocodes_services_containers/#services-stack","title":"Services Stack","text":"<p>The services stack includes the graph, storage (s3) and sparql gui containers. JSON-LD files are 'summoned' by gleaner to the s3 storage, and nabu convert jsonld to rdf quads and pushes the results to the graph.  After uploading, a step to produce a materialized view is required to improve performance The summarize step is undocoumented at present.</p>"},{"location":"setting_up_services/setup_geocodes_services_containers/#create-a-new-env-file","title":"create a new env file","text":"<ul> <li>cd deployment</li> <li>Edit files containing env variables</li> <li>copy portainer.services.env to new file.<code>cp portainer.env {myhost}.services.env</code></li> <li>copy portainer.geocodes.env to new file.<code>cp portainer.geocodes.env {myhost}.geocodes.env</code></li> <li>(option) use single file copy portainer.env to new file.<code>cp portainer.env {myhost}.env</code></li> <li>edit {myhost}.{geocodes|services}.env<ul> <li>change</li> </ul> </li> </ul> env <pre><code>HOST=geocodes-dev.mydomain.org\nPRODUCTION=geocodes.mydomain.org\nGC_CLIENT_DOMAIN=geoccodes.geocodes-dev.mydomain.org\nS3ADDRESS=oss.geocodes-dev.mydomain.org</code></pre>"},{"location":"setting_up_services/setup_geocodes_services_containers/#setup-and-start-services-using-portainer-ui","title":"Setup and start services using portainer ui","text":""},{"location":"setting_up_services/setup_geocodes_services_containers/#create-services-stack","title":"Create Services Stack","text":"<ul> <li>log into portainer<ul> <li>if this is a first login, it will ask you for a password.</li> <li>Select stack tab</li> <li>click add stack button <pre><code>Name: services\nBuild method: git repository\nRepository URL: https://github.com/earthcube/geocodes\nReference: refs/heads/main\nCompose path: deployment/services-compose.yaml</code></pre></li> <li>Environment variables: click 'load variables from .env file'<ul> <li>load {myhost}.services.env</li> </ul> </li> <li>Actions: <ul> <li>Click: Deploy This Stack </li> </ul> </li> </ul> </li> </ul> Services Stack"},{"location":"setting_up_services/setup_geocodes_services_containers/#go-to-step-3","title":"Go to step 3.","text":"<ol> <li>Install base containers on a server</li> <li>Setup services containers</li> <li>Initial setup of services and loading of data</li> <li>Setup Geocodes UI using datastores defined in Initial Setup</li> <li></li> </ol>"},{"location":"setting_up_services/setup_geocodes_services_containers/#testing-services-stack","title":"Testing Services Stack","text":""},{"location":"setting_up_services/setup_gleaner_container/","title":"Setup Gleaner   Containers:","text":"<p>This is step 3 of 5 major steps:</p> <ol> <li>Install base containers on a server</li> <li>Setup services containers</li> <li>Setup Gleaner containers</li> <li>Initial setup of services and loading of data</li> <li>Setup Geocodes UI using datastores defined in Initial Setup</li> </ol>"},{"location":"setting_up_services/setup_gleaner_container/#gleaner-container-stack","title":"Gleaner Container Stack","text":"<p>The gleaner-compose.yaml contains a single headless chrome container. Headless chrome needs to be run with extra shared memory    <code>` shm_size: \"2gb\"</code>, which was not fully supported by portainer prior to v 2.16. A script, run_gleaner.sh, that is run outside of portainer allows for this setting to be configured.</p> <p>Future</p> <p>in the future, this stack will most likely be a stack with a workflow system based on dagster</p>"},{"location":"setting_up_services/setup_gleaner_container/#gleaner-ingest-containers","title":"Gleaner Ingest Containers","text":""},{"location":"setting_up_services/setup_gleaner_container/#create-gleaner-via","title":"Create Gleaner (via)","text":"<code>./run_gleaner.sh</code> <p>this will run a headless chrome container  for gleaner summoning.  It will only be available locally via http://localhost:9222/</p>"},{"location":"setting_up_services/setup_gleaner_container/#go-to-step-4","title":"Go to step 4.","text":"<ol> <li>Install base containers on a server</li> <li>Setup services containers</li> <li>Setup Gleaner containers</li> <li>Initial setup of services and loading of data</li> <li>Setup Geocodes UI using datastores defined in Initial Setup </li> </ol>"},{"location":"setting_up_user_interface/setup_geocodes_ui_containers/","title":"Setup Geocodes Services Containers for Test Data load GCTEST:","text":"<p>This is step 5 of 5 major steps:</p> <ol> <li>Install base containers on a server</li> <li>Setup services containers</li> <li>Setup Gleaner containers</li> <li>Initial setup of services and loading of data</li> <li>Setup Geocodes UI using datastores defined in Initial Setup</li> </ol>"},{"location":"setting_up_user_interface/setup_geocodes_ui_containers/#step-overview-setup-and-start-geocodes-client-using-portainer-ui","title":"Step Overview: Setup and start GeoCodes Client using portainer ui:","text":"<ul> <li>modify the configuration file</li> <li>create stack in portainer</li> <li>test</li> <li>instructions for Updating a GEOCODES CLIENT Configuration if things do not work<ul> <li>or delete stack and reload</li> </ul> </li> </ul>"},{"location":"setting_up_user_interface/setup_geocodes_ui_containers/#step-details","title":"Step Details:","text":""},{"location":"setting_up_user_interface/setup_geocodes_ui_containers/#modify-the-facet-search-configuration","title":"Modify the Facet Search Configuration","text":"<ul> <li>edit in deployment/facets/config.yaml</li> <li>this file is mounted on the container as a docker config file<ul> <li>run the run_add_configs.sh</li> </ul> </li> </ul> <p>Portions of deployment/facets/config.yaml that might be changed.</p> section of deployment/facets/config.yaml <pre><code>API_URL: https://geocodes.{your host}/ec/api/\nSPARQL_NB: https:/geocodes.{your host}/notebook/mkQ?q=${q}\nSPARQL_YASGUI: https://geocodes.{your host}/sparqlgui?\n#API_URL: \"${window_location_origin}/ec/api\"\n#TRIPLESTORE_URL: https://graph.geocodes-1.earthcube.org/blazegraph/namespace/gctest/sparql\nTRIPLESTORE_URL: https://graph.{your host}/blazegraph/namespace/gctest/sparql\nBLAZEGRAPH_TIMEOUT: 20\n## ECRR need to use fuseki source, for now.\nECRR_TRIPLESTORE_URL: http://132.249.238.169:8080/fuseki/ecrr/query \n# ECRR_TRIPLESTORE_URL:   http://{your host}/blazegraph/namespace/ecrr/sparql \nECRR_GRAPH: http://earthcube.org/gleaner-summoned\nTHROUGHPUTDB_URL: https://throughputdb.com/api/ccdrs/annotations\nSPARQL_QUERY: queries/sparql_query.txt\nSPARQL_HASTOOLS: queries/sparql_hastools.txt\nSPARQL_TOOLS_WEBSERVICE: queries/sparql_gettools_webservice.txt\nSPARQL_TOOLS_DOWNLOAD: queries/sparql_gettools_download.txt\n# JSONLD_PROXY needs qoutes... since it has a $\nJSONLD_PROXY: \"https://geocodes.{your host}/ec/api/${o}\" \nSPARQL_YASGUI: https://sparqlui.{your host}/?</code></pre>"},{"location":"setting_up_user_interface/setup_geocodes_ui_containers/#create-geocodes-stack","title":"Create Geocodes Stack","text":"<ul> <li>log into portainer<ul> <li>if this is a first login, it will ask you for a password.</li> <li>click add stack button <pre><code>Name: geocodes\nBuild method: git repository\nRepository URL: https://github.com/earthcube/geocodes\nreference: refs/heads/main\nCompose path: deployment/geocodes-compose.yaml</code></pre></li> <li>Environment variables: click 'load variables from .env file'<ul> <li>load {myhost}.geocodes.env</li> </ul> </li> <li>Actions:<ul> <li>Click: Deploy This Stack</li> </ul> </li> </ul> </li> </ul> Geocodes Stack"},{"location":"setting_up_user_interface/setup_geocodes_ui_containers/#test-geocodes-client","title":"Test Geocodes Client","text":"<p>Issues</p> <p>IF things are not working in the UI, it is probably the facet search configuration You can take down the geocodes stack, and delete the config/facets_search or you can possibly just stop the gecodes_vue_ui service, and edit the facets_search config as noted here: See Managing Geocodes UI Containers</p> <ol> <li>Got to https://geocodes.{your host}/</li> <li>Got to configuration: https://geocodes.{your host}/#/config</li> <li>Two sections, one is the facests/config.yaml and the second is the API configuration (sanitized, we hope)</li> </ol> <p>Done</p> <p>This is the end of the deployment steps.</p> <p>If the deployment is working, you can now</p> <ul> <li>(Setup a Production Configuration)[./production/creatingProductionConfigs.md]</li> <li>(Reconfigure Geocodes UI)[./production/reconfigure_geocodes_ui_containers.md]</li> </ul>"},{"location":"setting_up_user_interface/tenant/","title":"Setting up A Tenant","text":"<p>A 'tenant' will allow us to host a project with a separate UI, using common services</p> <p>Assumptions</p> <p>this assumes that the user is familiar with how the stack works</p> <ul> <li>how to get a DNS name for the tenant</li> <li>how to create datasource for minio and graph,</li> <li>how to create a gleaner config file using glcon</li> <li>how to  load data using glcon</li> <li>how to create a facet_search.yaml  and load to docker/portainer config at config/facet_search+{project}</li> <li>how to add a stack \"geocodes_{project} in  portainer/docker</li> </ul> <ul> <li>a namepace for the graph and oss in the geocodes.services</li> <li>a client at thier DNS, eg. geoocdes.project.org</li> </ul>"},{"location":"setting_up_user_interface/tenant/#outline-of-the-steps","title":"Outline of the Steps:","text":"<ul> <li>Setup</li> <li>Load Data</li> <li>Summarize</li> <li>Create Geocodes Client UI Stack</li> </ul>"},{"location":"setting_up_user_interface/tenant/#setup","title":"Setup","text":"<ul> <li>Choose a 'project' identifier, This will be an ENV variable ${GC_BASE} set in local environment, or portainer</li> <li>setup two namespaces in graph (see table below)</li> <li><code>project</code> - a quad store</li> <li><code>project_summary</code> - a triple store, full text index</li> <li>setup bucket in minio</li> <li><code>project</code></li> <li>ask project to setup a DNS name for the client:</li> <li><code>geocodes.project.org CNAME geocodes-dev.earthcube.org</code></li> </ul> Item Name This instance project GC_BASE if you are testing locally you can use an env variable, export GC_BASE={project} DNS DNS HOST geocodes.{dns} project graph graph.XXXX.XXX {project}        (quad with full text) project summary graph.XXXX.XXX {project}_summary   (triples with full text) project bucket oss.xxxx.xxx {project} config {on docker/portainer server xxx} facets_config_{project}"},{"location":"setting_up_user_interface/tenant/#load-data","title":"Load data","text":"<p>Load data Steps Overview:</p> <ul> <li>setup source</li> <li>glcon/gleaer/nabu</li> <li>init config</li> <li>edit localconfig.yaml</li> <li>generate config</li> <li>run gleaner</li> <li>run nabu</li> <li>run summary</li> </ul>"},{"location":"setting_up_user_interface/tenant/#create-a-config-for-the-project-run-and-load-data-to-the-namespace-and-graph","title":"create a config for the project, run and load data to the namespace and graph","text":"<ul> <li>data<ul> <li>add tab to the sources spreadsheet, use that location url</li> <li>OR just use a local csv</li> </ul> </li> <li><code>glcon config init --cfgName {project}</code></li> <li>edit localConfig.yaml<ul> <li><code>nano configs/{project}/localConfig.yaml</code></li> </ul> </li> <li><code>glcon config generate --cfgName {project}</code></li> </ul> localConfig.yaml <pre><code>minio:\n  address: oss.geocodes-dev.earthcube.org\n  port: 443\n  accessKey: {snip}\n  secretKey: {snip}\n  ssl: true\n  bucket: {PROJECT} # can be overridden with MINIO_BUCKET\nsparql:\n#  endpoint: http://localhost/blazegraph/namespace/wifire/sparql\n  endpoint: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/{PROJECT}/sparql\ns3:\n  bucket: {PROJECT}  # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n  location:  https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=wifire\n# this can be a remote csv\n#  type: csv\n#  location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&amp;sheet={sheet_name}\n# TBD -- Just use the sources in the gleaner file.\n#  type: yaml\n#  location: gleaner.yaml</code></pre>"},{"location":"setting_up_user_interface/tenant/#run-glcon","title":"Run Glcon","text":"<ul> <li><code>glcon gleaner batch --cfgName {project}</code></li> <li><code>glcon nabu prefix --cfgName {project}</code></li> </ul>"},{"location":"setting_up_user_interface/tenant/#run-summarize","title":"Run Summarize","text":"<p>summarize materializes a flattend the graph  *  * Note this is new, undetested... take a look at the source if something breaks</p> <ul> <li>install earthcube summarize</li> <li><code>pip3 install earthcube_summarize</code></li> <li>run summarize (if installed via package, there should be a command line)</li> <li><code>summarize_from_graph--repo {repo} --graphendpoint {endppiont} --summary_namespace {earthcube_summary}</code></li> </ul>"},{"location":"setting_up_user_interface/tenant/#configure-client","title":"Configure Client","text":"<p>If you may want to initially test with a local instance in an IDE. After that this is the possible instructions for creating a tennant.</p> FACETS_CONFIG_CONFIG <p>This variable now tells the client which configuraiton to utilize</p>"},{"location":"setting_up_user_interface/tenant/#add-a-config-in-portainer-facets_config_project","title":"add a config in portainer (facets_config_{project})","text":"<pre><code>* using namespaces, minio and dns from above</code></pre> config/$FACETS_CONFIG_CONFIG <pre><code>---\n#API_URL: http://localhost:3000\nAPI_URL: https://geocodes.{HOST}/ec/api\nTRIPLESTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/{PROJECT}/sparql\nSUMMARYSTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/{PROJECT}_summary/sparql\nECRR_TRIPLESTORE_URL: http://132.249.238.169:8080/fuseki/ecrr/query\nECRR_GRAPH: http://earthcube.org/gleaner-summoned\nTHROUGHPUTDB_URL: https://throughputdb.com/api/ccdrs/annotations\nSPARQL_QUERY: queries/sparql_query.txt\nSPARQL_HASTOOLS: queries/sparql_hastools.txt\nSPARQL_TOOLS_WEBSERVICE: queries/sparql_gettools_webservice.txt\nSPARQL_TOOLS_DOWNLOAD: queries/sparql_gettools_download.txt\nJSONLD_PROXY: \"${window.location.origin}/ec/api/${o}\"\n# oauth issues. need to add another auth app for additional 'proxies'\n# This is the one that will work: SPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\nSPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\n####\nSPARQL_YASGUI: https://sparqlui.geocodes-dev.earthcube.org/?</code></pre>"},{"location":"setting_up_user_interface/tenant/#setup-tenant-stack","title":"setup tenant stack","text":"<pre><code>* create a new configutatio with a name\n* add a stack with project name  using  geocodes-compose_named.yaml\n* Before saving,  env var GC_BASE with project name</code></pre> <pre><code>HOST=geocodes-dev.earthcube.org\nFACETS_CONFIG_CONFIG=facets_config_name_you_created\n\nGC_CLIENT_DOMAIN=geocodes.{dns}\nMINIO_ROOT_ACCESS_KEY={snip}\nMINIO_ROOT_SECRET_KEY={snip}\nMINIO_SERVICE_ACCESS_KEY={snip}\nMINIO_SERVICE_SECRET_KEY={snip}\nSPARQL_DEFAULT_SPARQL_ENDPOINT_PATH=/blazegraph/namespace/{PROJECT}/sparql\nS3ADDRESS=oss.geocodes-dev.earthcube.org\nS3KEY={snip}\nS3SECRET={snip}\nS3SSL=true\nS3PORT=443\nBUCKET={PROJECT}\nBUCKETPATH=summoned\nPATHTEMPLATE={{bucketpath}}/{{reponame}}/{{sha}}.jsonld\nTOOLTEMPLATE={{bucketpath}}/{reponame}}/{{ref}}.json\nTOOLBUCKET=ecrr\nTOOLPATH=summoned\nGC_GITHUB_SECRET={snip}\nGC_GITHUB_CLIENTID={snip}\nGC_NB_AUTH_MODE=service\nGC_BASE=wifire</code></pre> <ul> <li>add a config in portainer (facets_config_{project}) (go to Configs on portainer)</li> <li>using namespaces, minio and dns from above</li> <li>example for \"config/facets_config_PROJECT\" <pre><code>    #API_URL: http://localhost:3000\n    API_URL: https://geocodes.{HOST}/ec/api\n    TRIPLESTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/{PROJECT}/sparql\n    SUMMARYSTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/{PROJECT}_summary/sparql\n    ECRR_TRIPLESTORE_URL: http://132.249.238.169:8080/fuseki/ecrr/query\n    ECRR_GRAPH: http://earthcube.org/gleaner-summoned\n    THROUGHPUTDB_URL: https://throughputdb.com/api/ccdrs/annotations\n    SPARQL_QUERY: queries/sparql_query.txt\n    SPARQL_HASTOOLS: queries/sparql_hastools.txt\n    SPARQL_TOOLS_WEBSERVICE: queries/sparql_gettools_webservice.txt\n    SPARQL_TOOLS_DOWNLOAD: queries/sparql_gettools_download.txt\n    JSONLD_PROXY: \"${window.location.origin}/ec/api/${o}\"\n    # oauth issues. need to add another auth app for additional 'proxies'\n    # This is the one that will work: SPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\n    SPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\n    ####\n    SPARQL_YASGUI: https://sparqlui.geocodes-dev.earthcube.org/?</code></pre></li> </ul>"},{"location":"setting_up_user_interface/tenant/#important-changes","title":"important changes","text":"<ul> <li>host is machine host</li> <li>GC_CLIENT_DOMAIN</li> <li>TRIPLESTORE_URL</li> <li>SUMMARYSTORE_URL</li> <li>jsonLD_proxy</li> </ul>"},{"location":"setting_up_user_interface/tenant/#issues","title":"issues:","text":"<p>traefik admin</p>"}]}